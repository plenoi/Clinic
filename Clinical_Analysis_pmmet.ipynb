{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Clinical_Analysis_pmmet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "nkhW04wi8JQW"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plenoi/Clinic/blob/master/Clinical_Analysis_pmmet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWHnvx9H8JPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jntbWEiv8JPM",
        "colab_type": "text"
      },
      "source": [
        "# Read all data and set hn as index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCtTLpCL8JPM",
        "colab_type": "code",
        "outputId": "6c1c2cc1-2486-4012-eb65-ee0d5a16c11a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/plenoi/Clinic/master/ultima_all_clean.csv')\n",
        "df = df.set_index('hn')\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>parity</th>\n",
              "      <th>hiv</th>\n",
              "      <th>menopaus</th>\n",
              "      <th>disease</th>\n",
              "      <th>surgery</th>\n",
              "      <th>conization</th>\n",
              "      <th>OPDsize</th>\n",
              "      <th>appearance</th>\n",
              "      <th>stage</th>\n",
              "      <th>pchemo</th>\n",
              "      <th>Wardsize</th>\n",
              "      <th>finalhisto</th>\n",
              "      <th>nodeyiel</th>\n",
              "      <th>RHlvsi</th>\n",
              "      <th>depth</th>\n",
              "      <th>size</th>\n",
              "      <th>utmet</th>\n",
              "      <th>vgmargin</th>\n",
              "      <th>vgmet</th>\n",
              "      <th>pelvicme</th>\n",
              "      <th>pmmet</th>\n",
              "      <th>adnmet</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hn</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2631840</th>\n",
              "      <td>52</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2633481</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2634477</th>\n",
              "      <td>52</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2633633</th>\n",
              "      <td>38</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2630496</th>\n",
              "      <td>55</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         age  parity  hiv  menopaus  ...  vgmet  pelvicme  pmmet  adnmet\n",
              "hn                                   ...                                \n",
              "2631840   52       3  0.0       0.0  ...    0.0       0.0    0.0     0.0\n",
              "2633481   32       2  0.0       0.0  ...    0.0       1.0    0.0     2.0\n",
              "2634477   52       2  0.0       0.0  ...    0.0       0.0    0.0     0.0\n",
              "2633633   38       2  0.0       0.0  ...    0.0       0.0    0.0     2.0\n",
              "2630496   55       3  0.0       1.0  ...    0.0       1.0    0.0     0.0\n",
              "\n",
              "[5 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw4lB2Yj8JPP",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iO_k48uw8JPQ",
        "colab_type": "text"
      },
      "source": [
        "Check number of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1gVBPp48JPQ",
        "colab_type": "code",
        "outputId": "b95dfe3a-7245-4cde-d889-6863c9d35ee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1723, 23)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKStqK_68JPT",
        "colab_type": "text"
      },
      "source": [
        "Check any missing data in each column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q77h1QHI8JPT",
        "colab_type": "code",
        "outputId": "9c4252e1-7190-4ee8-9124-926a858dacfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "df.isnull().sum(axis=0)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age             0\n",
              "parity          0\n",
              "hiv             4\n",
              "menopaus        1\n",
              "disease         0\n",
              "surgery         0\n",
              "conization      5\n",
              "OPDsize        17\n",
              "appearance    101\n",
              "stage          24\n",
              "pchemo          1\n",
              "Wardsize      145\n",
              "finalhisto     10\n",
              "nodeyiel       12\n",
              "RHlvsi        366\n",
              "depth         489\n",
              "size          114\n",
              "utmet          98\n",
              "vgmargin       96\n",
              "vgmet          97\n",
              "pelvicme        1\n",
              "pmmet          94\n",
              "adnmet          7\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlAFFf418JPV",
        "colab_type": "text"
      },
      "source": [
        "Delete column with missing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5YSx3NY8JPW",
        "colab_type": "code",
        "outputId": "7b565e11-1c60-4829-8901-f4e7d10dd241",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "df_clean_column = df.drop(['appearance','Wardsize','RHlvsi','depth','nodeyiel','vgmargin','pelvicme','adnmet'],axis = 1)\n",
        "df_clean_column.isnull().sum(axis=0)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age             0\n",
              "parity          0\n",
              "hiv             4\n",
              "menopaus        1\n",
              "disease         0\n",
              "surgery         0\n",
              "conization      5\n",
              "OPDsize        17\n",
              "stage          24\n",
              "pchemo          1\n",
              "finalhisto     10\n",
              "size          114\n",
              "utmet          98\n",
              "vgmet          97\n",
              "pmmet          94\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyNnK21u8JPX",
        "colab_type": "text"
      },
      "source": [
        "Delete row with at least 1 missing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03MGmT4a8JPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_clean = df_clean_column.dropna(axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kUDENLG8JPZ",
        "colab_type": "text"
      },
      "source": [
        "Total Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brL_4adg8JPb",
        "colab_type": "code",
        "outputId": "2267847d-e52d-44b2-deef-98adb8afb1f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df_clean.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1555, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHcRmIb78JPc",
        "colab_type": "text"
      },
      "source": [
        "Check number of sample in pelvicme class "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QQbuaAY8JPd",
        "colab_type": "code",
        "outputId": "5e477d1a-888c-4edd-a8bb-8160f79d91a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pel_class = np.unique(df_clean['pmmet'])\n",
        "pel_class"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LctFWu_H8JPf",
        "colab_type": "code",
        "outputId": "fa731b2c-f8ff-43ac-dd0d-f52d4105a31b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pel_value = [sum(df_clean['pmmet']==pel_class[0]),\n",
        "             sum(df_clean['pmmet']==pel_class[1])]\n",
        "pel_value"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1278, 277]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VJ6Ejg48JPk",
        "colab_type": "text"
      },
      "source": [
        "Separate pelviceme dataset into data (X) and label (y)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Gdtafhe8JPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = df_clean['pmmet'].values\n",
        "X = df_clean.drop(['pmmet'],axis = 1).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFhEkk8X8JPm",
        "colab_type": "text"
      },
      "source": [
        "Randomly choose 200 samples of class 1 (positive) as training data and the rest as test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnFz3mek8JPn",
        "colab_type": "code",
        "outputId": "d2592362-1ae5-45cb-af55-b5bc09063728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import random\n",
        "random.seed(1)\n",
        "positive_index = np.where(y==1)[0]\n",
        "negative_index = np.where(y==0)[0]\n",
        "pos_train_index = random.sample(list(positive_index),200)\n",
        "pos_test_index = list(set(positive_index) - set(pos_train_index))\n",
        "\n",
        "print(\"All dataset: \"+str(len(positive_index))+\" \"+str(len(negative_index)))\n",
        "print(\"Positive test dataset: \"+str(len(pos_test_index)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All dataset: 277 1278\n",
            "Positive test dataset: 77\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb3---1e8JPo",
        "colab_type": "text"
      },
      "source": [
        "Randomly separate negative dataset into 5 parts to create 5 training datasets consisted of 200 samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpCeL8Jz8JPp",
        "colab_type": "code",
        "outputId": "fd5750e5-44de-43df-9487-8e2adbedd956",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "neg_train_index1 = random.sample(list(negative_index),200)\n",
        "neg_tmp_index = list(set(negative_index) - set(neg_train_index1))\n",
        "\n",
        "neg_train_index2 = random.sample(list(neg_tmp_index),200)\n",
        "neg_tmp_index = list(set(neg_tmp_index) - set(neg_train_index2))\n",
        "\n",
        "neg_train_index3 = random.sample(list(neg_tmp_index),200)\n",
        "neg_tmp_index = list(set(neg_tmp_index) - set(neg_train_index3))\n",
        "\n",
        "neg_train_index4 = random.sample(list(neg_tmp_index),200)\n",
        "neg_tmp_index = list(set(neg_tmp_index) - set(neg_train_index4))\n",
        "\n",
        "neg_train_index5 = random.sample(list(neg_tmp_index),200)\n",
        "neg_tmp_index = list(set(neg_tmp_index) - set(neg_train_index5))\n",
        "\n",
        "neg_train_index6 = random.sample(list(neg_tmp_index),200)\n",
        "neg_tmp_index = list(set(neg_tmp_index) - set(neg_train_index6))\n",
        "\n",
        "neg_test_index = neg_tmp_index\n",
        "print(\"Negative test dataset: \"+str(len(neg_test_index)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Negative test dataset: 78\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCzFlfc58JPq",
        "colab_type": "text"
      },
      "source": [
        "Create 5 training dataset and 1 test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDcP1OdWSYZM",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LR5IIJ58JPr",
        "colab_type": "code",
        "outputId": "9b267249-f05a-4cb2-e6bc-840595d186a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train1 = np.concatenate((X[neg_train_index1,:],X[pos_train_index,:]),axis=0)\n",
        "y_train1 = np.concatenate((y[neg_train_index1],y[pos_train_index]),axis=0)\n",
        "\n",
        "X_train2 = np.concatenate((X[neg_train_index2,:],X[pos_train_index,:]),axis=0)\n",
        "y_train2 = np.concatenate((y[neg_train_index2],y[pos_train_index]),axis=0)\n",
        "\n",
        "X_train3 = np.concatenate((X[neg_train_index3,:],X[pos_train_index,:]),axis=0)\n",
        "y_train3 = np.concatenate((y[neg_train_index3],y[pos_train_index]),axis=0)\n",
        "\n",
        "X_train4 = np.concatenate((X[neg_train_index4,:],X[pos_train_index,:]),axis=0)\n",
        "y_train4 = np.concatenate((y[neg_train_index4],y[pos_train_index]),axis=0)\n",
        "\n",
        "X_train5 = np.concatenate((X[neg_train_index5,:],X[pos_train_index,:]),axis=0)\n",
        "y_train5 = np.concatenate((y[neg_train_index5],y[pos_train_index]),axis=0)\n",
        "\n",
        "X_train6 = np.concatenate((X[neg_train_index6,:],X[pos_train_index,:]),axis=0)\n",
        "y_train6 = np.concatenate((y[neg_train_index6],y[pos_train_index]),axis=0)\n",
        "\n",
        "X_train6.shape, y_train6.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((400, 14), (400,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5hQ5N618JPt",
        "colab_type": "code",
        "outputId": "6af34a50-b15b-4fb8-8318-63b08d45d9be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_test = np.concatenate((X[neg_test_index,:],X[pos_test_index,:]),axis=0)\n",
        "y_test = np.concatenate((y[neg_test_index],y[pos_test_index]),axis=0)\n",
        "X_test.shape, y_test.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((155, 14), (155,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgKGb8HK8JPv",
        "colab_type": "text"
      },
      "source": [
        "Data normalization to range of (0 to 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmkv5Op98JPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "alltrain = np.concatenate((X_train1,X_train2,X_train3,X_train4,X_train5,X_train6),axis=0)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaler.fit(alltrain)\n",
        "X_train_norm1 = scaler.transform(X_train1)\n",
        "X_train_norm2 = scaler.transform(X_train2)\n",
        "X_train_norm3 = scaler.transform(X_train3)\n",
        "X_train_norm4 = scaler.transform(X_train4)\n",
        "X_train_norm5 = scaler.transform(X_train5)\n",
        "X_train_norm6 = scaler.transform(X_train6)\n",
        "X_test_norm = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIohrB1DCyQS",
        "colab_type": "text"
      },
      "source": [
        "# Load Blind Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeW-r6UEC03N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "97fadc74-81b5-4082-c4e0-c6da7aec6d19"
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/plenoi/Clinic/master/Blind.csv')\n",
        "df = df.set_index('hn')\n",
        "df_clean_column = df.drop(['appearance','Wardsize','RHlvsi','depth','nodeyiel','vgmargin','pelvicme','adnmet'],axis = 1)\n",
        "df_clean = df_clean_column.dropna(axis = 0)\n",
        "y_blind = df_clean['pmmet'].values\n",
        "X_blind = df_clean.drop(['pmmet'],axis = 1).values\n",
        "\n",
        "pel_class = np.unique(df_clean['pmmet'])\n",
        "pel_value = [sum(df_clean['pmmet']==pel_class[0]),\n",
        "             sum(df_clean['pmmet']==pel_class[1])]\n",
        "pel_value\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[96, 43]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_TJBQK9C2Cv",
        "colab_type": "text"
      },
      "source": [
        "Data normalization using same scale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1IiFfFLC1Fw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_blind_norm = scaler.transform(X_blind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpFA7Afx8JPy",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEC9r99f8JP0",
        "colab_type": "text"
      },
      "source": [
        "10-Folds Cross Validation Training Accuracy with Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtagYOLw8JP0",
        "colab_type": "code",
        "outputId": "26184273-c5f5-42d3-a278-f2b8de29d460",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {'C': [1, 2, 4, 8, 16]}\n",
        "clf = GridSearchCV(LogisticRegression(random_state=0, solver='liblinear'),params, cv=10)\n",
        "clf.fit(X_train_norm, y_train)\n",
        "print(\"Best params : \" + str(clf.best_params_))\n",
        "print(\"10CV accuracy : \"+str(clf.best_score_*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best params : {'C': 2}\n",
            "10CV accuracy : 71.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TzTClfO8JP3",
        "colab_type": "text"
      },
      "source": [
        "Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "HhOEcwrg8JP3",
        "colab_type": "code",
        "outputId": "5747ffcd-6a65-4721-e737-b9458eeda6ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_predict = clf.predict(X_test_norm)\n",
        "print(\"Test accuracy : \"+str(sum(y_test == y_predict)/len(y_test)*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy : 72.90322580645162\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1j5UvWw8JP5",
        "colab_type": "code",
        "outputId": "36a761db-f155-460c-95c5-e4503cc0fd16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "target_names = ['negative', 'positive']\n",
        "C = confusion_matrix(y_test,y_predict) \n",
        "#C = C / C.astype(np.float).sum(axis=1)*100\n",
        "sns.heatmap(C, annot=True, fmt=\"d\",cmap=\"GnBu\",xticklabels=target_names, yticklabels=target_names)\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAEKCAYAAAA/2c+EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icsS2l7yS3Ke",
        "colab_type": "text"
      },
      "source": [
        "The precision is the ratio tp / (tp + fp). The precision is the ability of the classifier not to label as positive a sample that is negative.\n",
        "\n",
        "The recall is the ratio tp / (tp + fn). The recall is the ability of the classifier to find all the positive samples.\n",
        "\n",
        "The F score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n",
        "\n",
        "The support is the number of occurrences of each class in y_true."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUQXjLDe8JP6",
        "colab_type": "code",
        "outputId": "176247ad-3c37-4427-a87c-fef2dc779f81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "print(classification_report(y_test, y_predict, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.80      0.62      0.70        78\n",
            "    positive       0.68      0.84      0.76        77\n",
            "\n",
            "    accuracy                           0.73       155\n",
            "   macro avg       0.74      0.73      0.73       155\n",
            "weighted avg       0.74      0.73      0.73       155\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPlixLR6yz00",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression with Validation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdBgopMPMyKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = list()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPTCh-pUy0Nb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e8db0a71-207b-4797-870b-c089c23506ed"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
        "params = {'C': [1, 2, 4, 8, 16]}\n",
        "clf = GridSearchCV(LogisticRegression(random_state=0, solver='liblinear'),params, cv=10 , scoring = 'accuracy')\n",
        "clf.fit(X_train_norm1, y_train1)\n",
        "train_acc = clf.best_score_*100\n",
        "print(\"Best params : \" + str(clf.best_params_))\n",
        "print(\"Training accuracy : \"+str(train_acc))"
      ],
      "execution_count": 309,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best params : {'C': 1}\n",
            "Training accuracy : 71.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYDcOWGq3j0V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "33c85c99-4464-4e5b-a4e0-aa634c57e21c"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "y_predict = clf.predict(X_test_norm)\n",
        "test_acc = sum(y_test == y_predict)/len(y_test)*100\n",
        "print(\"Test accuracy : \"+str(test_acc))"
      ],
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy : 71.61290322580646\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLqaewjDINfB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ec6c2fd-7c21-49fa-cd58-bbd219307d3a"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_predict = clf.predict(X_blind_norm)\n",
        "blind_acc = sum(y_blind == y_predict)/len(y_blind)*100\n",
        "print(\"Blind accuracy : \"+str(blind_acc))"
      ],
      "execution_count": 311,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Blind accuracy : 64.74820143884892\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhfI4i6TL5JJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result.append([train_acc, test_acc, blind_acc])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trr4zcA9Nuy1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6fef3f06-135d-450f-f5c0-9b793ad37e8e"
      },
      "source": [
        "result"
      ],
      "execution_count": 307,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[77.25, 69.6774193548387, 66.18705035971223],\n",
              " [71.0, 70.3225806451613, 69.06474820143885],\n",
              " [69.75, 70.96774193548387, 68.34532374100719]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 307
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqDI1JlmZat7",
        "colab_type": "text"
      },
      "source": [
        "#SGD Classifier\n",
        "10-Folds Cross Validation Training Accuracy with Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGvBrm6SZbGw",
        "colab_type": "code",
        "outputId": "21531334-5e19-456a-cb57-0c8c79856eff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {\n",
        "    'loss' : ['hinge', 'log','modified_huber','squared_hinge', 'perceptron'],\n",
        "    'penalty' : ['l2', 'l1', 'none', 'elasticnet'],\n",
        "    'alpha' : [0.0001, 0.001, 0.01, 0.1],\n",
        "    'eta0' : [0.001, 0.01, 0.1],\n",
        "    'l1_ratio' : [0.1, 0.2, 0.3,0.4,0.5,0.6,0.7,0.8, 0.9]\n",
        "}\n",
        "clf = GridSearchCV(SGDClassifier(random_state=0,max_iter=1000,learning_rate='constant', eta0 = 0.0001),params, cv=10)\n",
        "clf.fit(X_train_norm, y_train)\n",
        "print(\"Best params : \" + str(clf.best_params_))\n",
        "print(\"10CV accuracy : \"+str(clf.best_score_))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best params : {'alpha': 0.0001, 'eta0': 0.1, 'l1_ratio': 0.8, 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
            "10CV accuracy : 0.74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtJH6iOFDBDG",
        "colab_type": "text"
      },
      "source": [
        "Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlx7BzobDC4o",
        "colab_type": "code",
        "outputId": "b740379e-6bb8-4671-88f0-5400a2808ac3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_predict = clf.predict(X_test_norm)\n",
        "print(\"Test accuracy : \"+str(sum(y_test == y_predict)/len(y_test)*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy : 72.90322580645162\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "virBSM-lDUs2",
        "colab_type": "code",
        "outputId": "47b57532-badc-4f65-c274-ff106a61dd1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "target_names = ['negative', 'positive']\n",
        "C = confusion_matrix(y_test,y_predict) \n",
        "#C = C / C.astype(np.float).sum(axis=1)*100\n",
        "sns.heatmap(C, annot=True, fmt=\"d\",cmap=\"GnBu\",xticklabels=target_names, yticklabels=target_names)\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAEKCAYAAAA/2c+EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yZrHa4ZDWkX",
        "colab_type": "text"
      },
      "source": [
        "The precision is the ratio tp / (tp + fp). The precision is the ability of the classifier not to label as positive a sample that is negative.\n",
        "\n",
        "The recall is the ratio tp / (tp + fn). The recall is the ability of the classifier to find all the positive samples.\n",
        "\n",
        "The F score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n",
        "\n",
        "The support is the number of occurrences of each class in y_true."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sanoTnPqDW6G",
        "colab_type": "code",
        "outputId": "e330973d-cb99-4998-8814-95498ab3a623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(classification_report(y_test, y_predict, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.79      0.63      0.70        78\n",
            "    positive       0.69      0.83      0.75        77\n",
            "\n",
            "    accuracy                           0.73       155\n",
            "   macro avg       0.74      0.73      0.73       155\n",
            "weighted avg       0.74      0.73      0.73       155\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83TUQPbu8JP8",
        "colab_type": "text"
      },
      "source": [
        "# Support Vector Machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB-9M8468JP9",
        "colab_type": "text"
      },
      "source": [
        "10-Folds Cross Validation Training Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVtRIhLf8JP9",
        "colab_type": "code",
        "outputId": "72bd405f-3033-4af5-efdf-060facb56d3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "params = {\n",
        "    'C' : [1,2,4,8,16,32], # High C = Overfitting\n",
        "    'gamma' : [0.03125, 0.0625, 0.125, 0.25, 0.5, 1, 2, 4, 8, 16, 32] # High gamma = Overfitting\n",
        "}\n",
        "clf = GridSearchCV(SVC(),params, cv=10)\n",
        "clf.fit(X_train_norm, y_train)\n",
        "print(\"Best params : \" + str(clf.best_params_))\n",
        "print(\"10CV accuracy : \"+str(clf.best_score_*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best params : {'C': 16, 'gamma': 0.125}\n",
            "10CV accuracy : 74.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsybCHdK8JP_",
        "colab_type": "text"
      },
      "source": [
        "Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ0XGMYp8JP_",
        "colab_type": "code",
        "outputId": "a869883c-c930-41f8-e8a0-54906cb7d20b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_predict = clf.predict(X_test_norm)\n",
        "target_names = ['negative', 'positive']\n",
        "sum(y_test == y_predict)/len(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7225806451612903"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C--oj8zn8JQB",
        "colab_type": "code",
        "outputId": "618302e5-bde5-4ea1-88b4-2a4fe22217fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "target_names = ['negative', 'positive']\n",
        "C = confusion_matrix(y_test,y_predict) \n",
        "C = C / C.astype(np.float).sum(axis=1)*100\n",
        "sns.heatmap(C, annot=True, fmt=\".2f\",cmap=\"GnBu\",xticklabels=target_names, yticklabels=target_names)\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAEKCAYAAAA/2c+EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naBGRmFKAFhGJKQVoEZGYUoAWEYmp/wfu9fT+86blFgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8NLTtRz8JQC",
        "colab_type": "code",
        "outputId": "23a8bb4d-b821-4aad-e6cb-4e8f2bcace77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(classification_report(y_test, y_predict, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.78      0.63      0.70        78\n",
            "    positive       0.68      0.82      0.75        77\n",
            "\n",
            "    accuracy                           0.72       155\n",
            "   macro avg       0.73      0.72      0.72       155\n",
            "weighted avg       0.73      0.72      0.72       155\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDlmx5yk8JQF",
        "colab_type": "text"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtXkXkoc8JQF",
        "colab_type": "text"
      },
      "source": [
        "10-Folds Cross Validation Training Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hBNax_o8JQG",
        "colab_type": "code",
        "outputId": "87b5bb59-97a4-4db3-dab0-fe291dc9f63b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "params = {\n",
        "    'n_estimators' : [10,50,100,200,500],\n",
        "    'min_samples_leaf' : [1,2,4,8,16,32],\n",
        "    'max_features' : ['sqrt',0.5,0.8],\n",
        "    'criterion' : ['gini','entropy']\n",
        "}\n",
        "clf = GridSearchCV(RandomForestClassifier(random_state=0),params, cv = 10)\n",
        "clf.fit(X_train_norm, y_train)\n",
        "print(\"Best params : \" + str(clf.best_params_))\n",
        "print(\"10CV accuracy : \"+str(clf.best_score_*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best params : {'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 8, 'n_estimators': 100}\n",
            "10CV accuracy : 75.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysu3HGiI8JQJ",
        "colab_type": "text"
      },
      "source": [
        "Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhvwH76Z8JQK",
        "colab_type": "code",
        "outputId": "6e9cf8c3-1576-45d9-cd6f-a39c47317329",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_predict = clf.predict(X_test_norm)\n",
        "target_names = ['negative', 'positive']\n",
        "sum(y_test == y_predict)/len(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7096774193548387"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY-ZyplN8JQL",
        "colab_type": "code",
        "outputId": "07d29e68-5883-407c-d529-340efccbc503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "target_names = ['negative', 'positive']\n",
        "C = confusion_matrix(y_test,y_predict) \n",
        "C = C / C.astype(np.float).sum(axis=1)*100\n",
        "sns.heatmap(C, annot=True, fmt=\".2f\",cmap=\"GnBu\",xticklabels=target_names, yticklabels=target_names)\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAEKCAYAAAA/2c+EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzqS4AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwGUP-YJ8JQN",
        "colab_type": "code",
        "outputId": "28f7382e-7fe0-42ce-f92c-01edde49f8c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(classification_report(y_test, y_predict, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.77      0.60      0.68        78\n",
            "    positive       0.67      0.82      0.74        77\n",
            "\n",
            "    accuracy                           0.71       155\n",
            "   macro avg       0.72      0.71      0.71       155\n",
            "weighted avg       0.72      0.71      0.71       155\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP7YysqP8JQO",
        "colab_type": "text"
      },
      "source": [
        "# XGBoost\n",
        "\n",
        "10-Folds Cross Validation Training Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "8k6aTTLX8JQP",
        "colab_type": "code",
        "outputId": "6508eb7c-91ae-4e66-d4ae-393dcf041be1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#!pip install xgboost  // If you dont have XGBoost\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "params = {\n",
        "    'n_estimators' : [10,50,100,200],\n",
        "    'max_depth' : [2, 4, 8],\n",
        "     'gamma' : [0.03125, 0.0625, 0.125, 0.25, 0.5, 1, 2, 4, 8, 16, 32], # High gamma as much as possible\n",
        "    'learning_rate' : [0.001, 0.01, 0.1],\n",
        "    'minchildweight' : [1,2,4,8,16,32],  # High as much as possible\n",
        "    'subsample' : [0.5,0.8, 1],\n",
        "    'colsample_bytree' : [0.5, 0.8, 1]\n",
        "    #'reg_alpha','reg_lamnda' : [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]  # Only for linear\n",
        "    \n",
        "}\n",
        "clf = GridSearchCV(xgb.XGBClassifier(random_state=0, objective='binary:logistic',n_jobs=-1),params, cv = 10)\n",
        "clf.fit(X_train_norm1, y_train)\n",
        "print(\"Best params : \" + str(clf.best_params_))\n",
        "print(\"10CV accuracy : \"+str(clf.best_score_*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best params : {'colsample_bytree': 0.5, 'gamma': 0.5, 'learning_rate': 0.1, 'max_depth': 2, 'minchildweight': 1, 'n_estimators': 100, 'subsample': 0.8}\n",
            "10CV accuracy : 76.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCjZSw3y8JQR",
        "colab_type": "text"
      },
      "source": [
        "Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBzk9b858JQR",
        "colab_type": "code",
        "outputId": "ef64d40b-484d-4a01-a906-3ac35556eed4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_predict1 = clf.predict(X_test_norm)\n",
        "target_names = ['negative', 'positive']\n",
        "sum(y_test == y_predict1)/len(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7161290322580646"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "udIynozx8JQT",
        "colab_type": "code",
        "outputId": "834a2d42-3d17-4465-dd2d-186e8e0638ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "target_names = ['negative', 'positive']\n",
        "C = confusion_matrix(y_test,y_predict1) \n",
        "C = C / C.astype(np.float).sum(axis=1)*100\n",
        "sns.heatmap(C, annot=True, fmt=\".2f\",cmap=\"GnBu\",xticklabels=target_names, yticklabels=target_names)\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAEKCAYAAAA/2c+EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLKccoM3McsoB2swsp/4ffRJECUtxxwAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6GyVYcE8JQV",
        "colab_type": "code",
        "outputId": "fa7b45ea-b0a6-4c5e-fdb4-6fd83668a610",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(classification_report(y_test, y_predict1, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.76      0.64      0.69        78\n",
            "    positive       0.69      0.79      0.73        77\n",
            "\n",
            "    accuracy                           0.72       155\n",
            "   macro avg       0.72      0.72      0.71       155\n",
            "weighted avg       0.72      0.72      0.71       155\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGawdzqGHuvG",
        "colab_type": "text"
      },
      "source": [
        "# XGBoost Model 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXvhAQsZHt8Z",
        "colab_type": "code",
        "outputId": "d788acfc-c878-47f0-9832-a31a94c3024e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#!pip install xgboost  // If you dont have XGBoost\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "params = {\n",
        "    'n_estimators' : [10,50,100,200],\n",
        "    'max_depth' : [2, 4, 8],\n",
        "     'gamma' : [0.03125, 0.0625, 0.125, 0.25, 0.5, 1, 2, 4, 8, 16, 32], # High gamma as much as possible\n",
        "    'learning_rate' : [0.001, 0.01, 0.1],\n",
        "    'minchildweight' : [1,2,4,8,16,32],  # High as much as possible\n",
        "    'subsample' : [0.5,0.8, 1],\n",
        "    'colsample_bytree' : [0.5, 0.8, 1]\n",
        "    #'reg_alpha','reg_lamnda' : [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]  # Only for linear\n",
        "    \n",
        "}\n",
        "clf = GridSearchCV(xgb.XGBClassifier(random_state=0, objective='binary:logistic'),params, cv = 10)\n",
        "clf.fit(X_train_norm2, y_train2)\n",
        "print(\"Best params : \" + str(clf.best_params_))\n",
        "print(\"10CV accuracy : \"+str(clf.best_score_*100))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best params : {'colsample_bytree': 0.5, 'gamma': 0.0625, 'learning_rate': 0.1, 'max_depth': 8, 'minchildweight': 1, 'n_estimators': 50, 'subsample': 0.5}\n",
            "10CV accuracy : 75.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVvRtxDxdLKG",
        "colab_type": "code",
        "outputId": "c84a5f58-d310-4693-90f8-9fc9f7ccb521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_predict2 = clf.predict(X_test_norm)\n",
        "target_names = ['negative', 'positive']\n",
        "sum(y_test == y_predict2)/len(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7032258064516129"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXF9ri8idLU3",
        "colab_type": "code",
        "outputId": "271dd2d5-2c11-45ce-ac69-a215b95bb004",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "target_names = ['negative', 'positive']\n",
        "C = confusion_matrix(y_test,y_predict2) \n",
        "C = C / C.astype(np.float).sum(axis=1)*100\n",
        "sns.heatmap(C, annot=True, fmt=\".2f\",cmap=\"GnBu\",xticklabels=target_names, yticklabels=target_names)\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAEKCAYAAAA/2c+EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFhGJqf8PoHiip6N/MvcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpj-c6NowXWk",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQSupzLuyrcJ",
        "colab_type": "text"
      },
      "source": [
        "Install Tensorflow 2.0, if you dont have it, pls uncomment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apfoJf7KwjCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install tensorflow==2.0.0-beta1\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFIQ4e9By39Q",
        "colab_type": "text"
      },
      "source": [
        "Create Deep Learning Model \n",
        "Using Sequential = Feed-Forward Model\n",
        "1. The first hidden layer contains 16 hidden nodes connected to input layers with 18 nodes corresponding to number of features\n",
        "2. Other layers is chosen based on 2^1, 2^2, 2^3 concept with 'relu' activation function\n",
        "3. Output layer is sigmoid because it can output value which is close 0 and 1 (Binary Class)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYWq1QcjxVVn",
        "colab_type": "code",
        "outputId": "19165452-bbb8-4030-8178-ac7e13317bd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(16, activation='relu', input_shape=(14,)),\n",
        "  tf.keras.layers.Dense(8, activation='relu'),\n",
        "  tf.keras.layers.Dense(4, activation='relu'),\n",
        "  tf.keras.layers.Dense(2, activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')    \n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 16)                240       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 36        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 10        \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 3         \n",
            "=================================================================\n",
            "Total params: 425\n",
            "Trainable params: 425\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1HIk3Vbz8O5",
        "colab_type": "text"
      },
      "source": [
        "Set up Optimizer to 'adam' with is argubly the best one now, the loss function is set to binary_crossentropy (Binary Classification)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmgpa6cVz_G0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "bac4e296-2fa6-46e8-9e38-7a0fadebe72a"
      },
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufIG52Wj0dwp",
        "colab_type": "text"
      },
      "source": [
        "Train model around 20 epochs with batchsize 20\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYQMmo_A0swm",
        "colab_type": "code",
        "outputId": "040a02fa-7d7a-4922-eb0a-047282bdbdbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "model.fit(X_train_norm, y_train, epochs=3, batch_size=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "400/400 [==============================] - 0s 74us/sample - loss: 0.6211 - acc: 0.7150\n",
            "Epoch 2/3\n",
            "400/400 [==============================] - 0s 66us/sample - loss: 0.6179 - acc: 0.7150\n",
            "Epoch 3/3\n",
            "400/400 [==============================] - 0s 66us/sample - loss: 0.6118 - acc: 0.7300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5e8c3b0710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8i0vv5X1Qbt",
        "colab_type": "text"
      },
      "source": [
        "The model train accuracy is stable aroud 75, so stop train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSFwXFbj1RRC",
        "colab_type": "text"
      },
      "source": [
        "Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q36U3-5H1Qw8",
        "colab_type": "code",
        "outputId": "44b7254f-8b0b-4b24-c2e3-aa657fce9cc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_predict = np.round(model.predict(X_test_norm))\n",
        "y_predict = [i[0] for i in y_predict.tolist()]\n",
        "sum(y_predict == y_test)/len(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6709677419354839"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJxV7N9r3dwG",
        "colab_type": "code",
        "outputId": "1c7310cd-032c-4125-d9eb-64670e1c54cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "target_names = ['negative', 'positive']\n",
        "C = confusion_matrix(y_test,y_predict) \n",
        "C = C / C.astype(np.float).sum(axis=1)*100\n",
        "sns.heatmap(C, annot=True, fmt=\".2f\",cmap=\"GnBu\",xticklabels=target_names, yticklabels=target_names)\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAEKCAYAAAA/2c+EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\neCCrAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym5c3nO53rql",
        "colab_type": "code",
        "outputId": "e8a3ed95-2ba8-48ab-fe9c-2c931f1067bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(classification_report(y_test, y_predict, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.65      0.47      0.55        68\n",
            "    positive       0.62      0.78      0.69        76\n",
            "\n",
            "    accuracy                           0.63       144\n",
            "   macro avg       0.64      0.62      0.62       144\n",
            "weighted avg       0.64      0.63      0.62       144\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uom0bA6w9gur",
        "colab_type": "text"
      },
      "source": [
        "#Deep Learning with Early Stop to Prevent Overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTVxdiKh9w72",
        "colab_type": "text"
      },
      "source": [
        "Split validation data from training data and also create new training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dVtYji49nTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train_over, X_val_norm, y_train_over, y_val = train_test_split(X_train_norm, y_train, test_size=0.20, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XihTMWtN-tr-",
        "colab_type": "text"
      },
      "source": [
        "Create model again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yewRErEJ-wtm",
        "colab_type": "code",
        "outputId": "45ddb0e7-dac7-478b-beb7-c4ec28c8543c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(16, activation='relu', input_shape=(18,)),\n",
        "  tf.keras.layers.Dense(8, activation='relu'),\n",
        "  tf.keras.layers.Dense(4, activation='relu'),\n",
        "  tf.keras.layers.Dense(2, activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')    \n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_5 (Dense)              (None, 16)                304       \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 4)                 36        \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 2)                 10        \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 3         \n",
            "=================================================================\n",
            "Total params: 489\n",
            "Trainable params: 489\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdtWWJgN-zZF",
        "colab_type": "text"
      },
      "source": [
        "Create optimizer again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_Lnu8PI-_DP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRcpU0YB_Avn",
        "colab_type": "text"
      },
      "source": [
        "Create early stop function to prevent overfitting\n",
        "if the accuracy of validation data does not increase for 5 epoch (patience = 5), use the latest best validation accuracy model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpCuc6rY_EN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "overfit_prevent = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10KdH0jV_reE",
        "colab_type": "text"
      },
      "source": [
        "Train model with early stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8jnO-td_vgr",
        "colab_type": "code",
        "outputId": "e12a900b-c55b-4727-be9e-7dcd5770a79d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train_over, y_train_over, epochs= 100, batch_size = 20, validation_data= (X_val_norm, y_val), callbacks=[overfit_prevent])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 320 samples, validate on 80 samples\n",
            "Epoch 1/100\n",
            "320/320 [==============================] - 0s 555us/sample - loss: 0.7039 - acc: 0.4656 - val_loss: 0.6961 - val_acc: 0.4875\n",
            "Epoch 2/100\n",
            "320/320 [==============================] - 0s 86us/sample - loss: 0.6940 - acc: 0.4906 - val_loss: 0.6935 - val_acc: 0.4875\n",
            "Epoch 3/100\n",
            "320/320 [==============================] - 0s 81us/sample - loss: 0.6890 - acc: 0.5562 - val_loss: 0.6911 - val_acc: 0.4875\n",
            "Epoch 4/100\n",
            "320/320 [==============================] - 0s 85us/sample - loss: 0.6853 - acc: 0.6094 - val_loss: 0.6867 - val_acc: 0.5375\n",
            "Epoch 5/100\n",
            "320/320 [==============================] - 0s 101us/sample - loss: 0.6798 - acc: 0.6156 - val_loss: 0.6807 - val_acc: 0.5375\n",
            "Epoch 6/100\n",
            "320/320 [==============================] - 0s 96us/sample - loss: 0.6737 - acc: 0.6219 - val_loss: 0.6712 - val_acc: 0.5625\n",
            "Epoch 7/100\n",
            "320/320 [==============================] - 0s 98us/sample - loss: 0.6669 - acc: 0.6594 - val_loss: 0.6630 - val_acc: 0.5875\n",
            "Epoch 8/100\n",
            "320/320 [==============================] - 0s 82us/sample - loss: 0.6602 - acc: 0.6438 - val_loss: 0.6519 - val_acc: 0.6000\n",
            "Epoch 9/100\n",
            "320/320 [==============================] - 0s 92us/sample - loss: 0.6537 - acc: 0.6531 - val_loss: 0.6437 - val_acc: 0.6375\n",
            "Epoch 10/100\n",
            "320/320 [==============================] - 0s 77us/sample - loss: 0.6465 - acc: 0.6938 - val_loss: 0.6377 - val_acc: 0.6250\n",
            "Epoch 11/100\n",
            "320/320 [==============================] - 0s 83us/sample - loss: 0.6404 - acc: 0.6906 - val_loss: 0.6270 - val_acc: 0.6125\n",
            "Epoch 12/100\n",
            "320/320 [==============================] - 0s 91us/sample - loss: 0.6318 - acc: 0.7063 - val_loss: 0.6195 - val_acc: 0.6250\n",
            "Epoch 13/100\n",
            "320/320 [==============================] - 0s 77us/sample - loss: 0.6266 - acc: 0.7031 - val_loss: 0.6147 - val_acc: 0.6500\n",
            "Epoch 14/100\n",
            "320/320 [==============================] - 0s 80us/sample - loss: 0.6199 - acc: 0.7156 - val_loss: 0.6077 - val_acc: 0.6750\n",
            "Epoch 15/100\n",
            "320/320 [==============================] - 0s 86us/sample - loss: 0.6145 - acc: 0.7219 - val_loss: 0.6007 - val_acc: 0.6750\n",
            "Epoch 16/100\n",
            "320/320 [==============================] - 0s 81us/sample - loss: 0.6095 - acc: 0.7312 - val_loss: 0.5975 - val_acc: 0.6875\n",
            "Epoch 17/100\n",
            "320/320 [==============================] - 0s 85us/sample - loss: 0.6051 - acc: 0.7344 - val_loss: 0.5947 - val_acc: 0.6875\n",
            "Epoch 18/100\n",
            "320/320 [==============================] - 0s 85us/sample - loss: 0.5988 - acc: 0.7250 - val_loss: 0.5904 - val_acc: 0.6875\n",
            "Epoch 19/100\n",
            "320/320 [==============================] - 0s 91us/sample - loss: 0.5950 - acc: 0.7281 - val_loss: 0.5857 - val_acc: 0.6875\n",
            "Epoch 20/100\n",
            "320/320 [==============================] - 0s 86us/sample - loss: 0.5914 - acc: 0.7312 - val_loss: 0.5827 - val_acc: 0.6750\n",
            "Epoch 21/100\n",
            "320/320 [==============================] - 0s 85us/sample - loss: 0.5864 - acc: 0.7312 - val_loss: 0.5765 - val_acc: 0.6875\n",
            "Epoch 22/100\n",
            "320/320 [==============================] - 0s 87us/sample - loss: 0.5817 - acc: 0.7312 - val_loss: 0.5752 - val_acc: 0.6875\n",
            "Epoch 23/100\n",
            "320/320 [==============================] - 0s 80us/sample - loss: 0.5769 - acc: 0.7406 - val_loss: 0.5712 - val_acc: 0.7000\n",
            "Epoch 24/100\n",
            "320/320 [==============================] - 0s 89us/sample - loss: 0.5740 - acc: 0.7437 - val_loss: 0.5686 - val_acc: 0.7250\n",
            "Epoch 25/100\n",
            "320/320 [==============================] - 0s 81us/sample - loss: 0.5693 - acc: 0.7594 - val_loss: 0.5673 - val_acc: 0.7250\n",
            "Epoch 26/100\n",
            "320/320 [==============================] - 0s 80us/sample - loss: 0.5648 - acc: 0.7594 - val_loss: 0.5625 - val_acc: 0.7250\n",
            "Epoch 27/100\n",
            "320/320 [==============================] - 0s 84us/sample - loss: 0.5604 - acc: 0.7688 - val_loss: 0.5612 - val_acc: 0.7250\n",
            "Epoch 28/100\n",
            "320/320 [==============================] - 0s 86us/sample - loss: 0.5563 - acc: 0.7750 - val_loss: 0.5592 - val_acc: 0.7375\n",
            "Epoch 29/100\n",
            "320/320 [==============================] - 0s 91us/sample - loss: 0.5521 - acc: 0.7719 - val_loss: 0.5572 - val_acc: 0.7375\n",
            "Epoch 30/100\n",
            "320/320 [==============================] - 0s 83us/sample - loss: 0.5483 - acc: 0.7812 - val_loss: 0.5580 - val_acc: 0.7250\n",
            "Epoch 31/100\n",
            "320/320 [==============================] - 0s 78us/sample - loss: 0.5436 - acc: 0.7969 - val_loss: 0.5530 - val_acc: 0.7375\n",
            "Epoch 32/100\n",
            "320/320 [==============================] - 0s 81us/sample - loss: 0.5401 - acc: 0.7875 - val_loss: 0.5508 - val_acc: 0.7250\n",
            "Epoch 33/100\n",
            "320/320 [==============================] - 0s 75us/sample - loss: 0.5378 - acc: 0.7875 - val_loss: 0.5510 - val_acc: 0.7500\n",
            "Epoch 34/100\n",
            "320/320 [==============================] - 0s 74us/sample - loss: 0.5312 - acc: 0.8062 - val_loss: 0.5549 - val_acc: 0.7625\n",
            "Epoch 35/100\n",
            "320/320 [==============================] - 0s 79us/sample - loss: 0.5277 - acc: 0.8031 - val_loss: 0.5472 - val_acc: 0.7625\n",
            "Epoch 36/100\n",
            "320/320 [==============================] - 0s 82us/sample - loss: 0.5224 - acc: 0.8062 - val_loss: 0.5458 - val_acc: 0.7625\n",
            "Epoch 37/100\n",
            "320/320 [==============================] - 0s 95us/sample - loss: 0.5181 - acc: 0.8094 - val_loss: 0.5453 - val_acc: 0.7625\n",
            "Epoch 38/100\n",
            "320/320 [==============================] - 0s 87us/sample - loss: 0.5141 - acc: 0.8094 - val_loss: 0.5446 - val_acc: 0.7625\n",
            "Epoch 39/100\n",
            "320/320 [==============================] - 0s 85us/sample - loss: 0.5091 - acc: 0.8094 - val_loss: 0.5436 - val_acc: 0.7625\n",
            "Epoch 40/100\n",
            "320/320 [==============================] - 0s 83us/sample - loss: 0.5055 - acc: 0.8062 - val_loss: 0.5435 - val_acc: 0.7750\n",
            "Epoch 41/100\n",
            "320/320 [==============================] - 0s 84us/sample - loss: 0.5024 - acc: 0.8125 - val_loss: 0.5430 - val_acc: 0.7625\n",
            "Epoch 42/100\n",
            "320/320 [==============================] - 0s 82us/sample - loss: 0.4969 - acc: 0.8125 - val_loss: 0.5423 - val_acc: 0.7750\n",
            "Epoch 43/100\n",
            "320/320 [==============================] - 0s 82us/sample - loss: 0.4924 - acc: 0.8156 - val_loss: 0.5410 - val_acc: 0.7500\n",
            "Epoch 44/100\n",
            "320/320 [==============================] - 0s 82us/sample - loss: 0.4899 - acc: 0.8125 - val_loss: 0.5411 - val_acc: 0.7625\n",
            "Epoch 45/100\n",
            "320/320 [==============================] - 0s 78us/sample - loss: 0.4860 - acc: 0.8281 - val_loss: 0.5389 - val_acc: 0.7625\n",
            "Epoch 46/100\n",
            "320/320 [==============================] - 0s 84us/sample - loss: 0.4876 - acc: 0.7937 - val_loss: 0.5398 - val_acc: 0.7500\n",
            "Epoch 47/100\n",
            "320/320 [==============================] - 0s 84us/sample - loss: 0.4811 - acc: 0.8313 - val_loss: 0.5392 - val_acc: 0.7500\n",
            "Epoch 48/100\n",
            "320/320 [==============================] - 0s 72us/sample - loss: 0.4747 - acc: 0.8219 - val_loss: 0.5384 - val_acc: 0.7500\n",
            "Epoch 49/100\n",
            "320/320 [==============================] - 0s 75us/sample - loss: 0.4684 - acc: 0.8375 - val_loss: 0.5385 - val_acc: 0.7500\n",
            "Epoch 50/100\n",
            "320/320 [==============================] - 0s 82us/sample - loss: 0.4650 - acc: 0.8313 - val_loss: 0.5401 - val_acc: 0.7500\n",
            "Epoch 51/100\n",
            "320/320 [==============================] - 0s 96us/sample - loss: 0.4617 - acc: 0.8281 - val_loss: 0.5393 - val_acc: 0.7500\n",
            "Epoch 52/100\n",
            "320/320 [==============================] - 0s 75us/sample - loss: 0.4567 - acc: 0.8469 - val_loss: 0.5416 - val_acc: 0.7375\n",
            "Epoch 53/100\n",
            "320/320 [==============================] - 0s 79us/sample - loss: 0.4542 - acc: 0.8438 - val_loss: 0.5413 - val_acc: 0.7500\n",
            "Epoch 54/100\n",
            "320/320 [==============================] - 0s 81us/sample - loss: 0.4508 - acc: 0.8594 - val_loss: 0.5413 - val_acc: 0.7375\n",
            "Epoch 55/100\n",
            "320/320 [==============================] - 0s 83us/sample - loss: 0.4477 - acc: 0.8531 - val_loss: 0.5420 - val_acc: 0.7375\n",
            "Epoch 56/100\n",
            "320/320 [==============================] - 0s 82us/sample - loss: 0.4461 - acc: 0.8594 - val_loss: 0.5444 - val_acc: 0.7375\n",
            "Epoch 57/100\n",
            "320/320 [==============================] - 0s 80us/sample - loss: 0.4411 - acc: 0.8562 - val_loss: 0.5429 - val_acc: 0.7375\n",
            "Epoch 58/100\n",
            "320/320 [==============================] - 0s 250us/sample - loss: 0.4368 - acc: 0.8531 - val_loss: 0.5445 - val_acc: 0.7500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5e8e0f08d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOJCRse-CGUg",
        "colab_type": "code",
        "outputId": "3bca1913-54ad-4477-d1c2-cf1ce5867b47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_predict = np.round(model.predict(X_test_norm))\n",
        "y_predict = [i[0] for i in y_predict.tolist()]\n",
        "sum(y_predict == y_test)/len(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7152777777777778"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmTIvtc3GL-J",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning with validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKOWZZb5GK_E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "e3742acb-83bd-4dac-f501-4aba16a9de94"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "np.random.seed(1)\n",
        "tf.random.set_random_seed(1)\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(8, activation='relu', input_shape=(14,)),\n",
        "  tf.keras.layers.Dense(4, activation='relu'),\n",
        "  tf.keras.layers.Dense(2, activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')    \n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 8)                 120       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 36        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 10        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 3         \n",
            "=================================================================\n",
            "Total params: 169\n",
            "Trainable params: 169\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obm8KJsLGhvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpzqsFk0Gw2r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e470fb91-cbd1-47a5-856d-da9a43b3e350"
      },
      "source": [
        "train_acc = list()\n",
        "val_acc = list()\n",
        "for i in range(0,125):\n",
        "  history = model.fit(X_train_norm1, y_train1, epochs= 1, batch_size = 200, validation_data= (X_test_norm, y_test))\n",
        "  tmp_avg = np.mean(history.history['acc'])\n",
        "  tmp_avg_val = np.mean(history.history['val_acc'])\n",
        "  \n",
        "  history = model.fit(X_train_norm2, y_train2, epochs= 1, batch_size = 200, validation_data= (X_test_norm, y_test))\n",
        "  tmp_avg = tmp_avg + np.mean(history.history['acc'])\n",
        "  tmp_avg_val = tmp_avg_val + np.mean(history.history['val_acc'])\n",
        "  \n",
        "  history = model.fit(X_train_norm3, y_train3, epochs= 1, batch_size = 200, validation_data= (X_test_norm, y_test))\n",
        "  tmp_avg = tmp_avg + np.mean(history.history['acc'])\n",
        "  tmp_avg_val = tmp_avg_val + np.mean(history.history['val_acc'])\n",
        "  \n",
        "  history = model.fit(X_train_norm4, y_train4, epochs= 1, batch_size = 200, validation_data= (X_test_norm, y_test))\n",
        "  tmp_avg = tmp_avg + np.mean(history.history['acc'])\n",
        "  tmp_avg_val = tmp_avg_val + np.mean(history.history['val_acc'])\n",
        "  \n",
        "  history = model.fit(X_train_norm5, y_train5, epochs= 1, batch_size = 200, validation_data= (X_test_norm, y_test))\n",
        "  tmp_avg = tmp_avg + np.mean(history.history['acc'])\n",
        "  tmp_avg_val = tmp_avg_val + np.mean(history.history['val_acc'])\n",
        "  \n",
        "  history = model.fit(X_train_norm6, y_train6, epochs= 1, batch_size = 200, validation_data= (X_test_norm, y_test))\n",
        "  tmp_avg = tmp_avg + np.mean(history.history['acc'])\n",
        "  tmp_avg_val = tmp_avg_val + np.mean(history.history['val_acc'])\n",
        "  \n",
        "  train_acc.append(tmp_avg/6)\n",
        "  val_acc.append(tmp_avg_val/6)"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 270us/sample - loss: 0.6913 - acc: 0.4925 - val_loss: 0.6912 - val_acc: 0.4968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.6913 - acc: 0.4875 - val_loss: 0.6911 - val_acc: 0.4839\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.6917 - acc: 0.4775 - val_loss: 0.6909 - val_acc: 0.4774\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.6914 - acc: 0.4850 - val_loss: 0.6908 - val_acc: 0.4710\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.6917 - acc: 0.5025 - val_loss: 0.6906 - val_acc: 0.4839\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.6907 - acc: 0.5125 - val_loss: 0.6904 - val_acc: 0.5032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.6901 - acc: 0.5150 - val_loss: 0.6902 - val_acc: 0.5032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.6902 - acc: 0.5000 - val_loss: 0.6900 - val_acc: 0.5097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.6905 - acc: 0.5275 - val_loss: 0.6898 - val_acc: 0.5161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.6902 - acc: 0.5125 - val_loss: 0.6896 - val_acc: 0.5097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.6906 - acc: 0.5150 - val_loss: 0.6894 - val_acc: 0.5032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6895 - acc: 0.5225 - val_loss: 0.6892 - val_acc: 0.5032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.6888 - acc: 0.5425 - val_loss: 0.6890 - val_acc: 0.5032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.6888 - acc: 0.5300 - val_loss: 0.6888 - val_acc: 0.5032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.6891 - acc: 0.5575 - val_loss: 0.6886 - val_acc: 0.5032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.6887 - acc: 0.5400 - val_loss: 0.6883 - val_acc: 0.5161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.6892 - acc: 0.5350 - val_loss: 0.6881 - val_acc: 0.5226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.6880 - acc: 0.5650 - val_loss: 0.6879 - val_acc: 0.5226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6871 - acc: 0.5800 - val_loss: 0.6876 - val_acc: 0.5290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.6871 - acc: 0.5650 - val_loss: 0.6874 - val_acc: 0.5290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.6873 - acc: 0.5900 - val_loss: 0.6871 - val_acc: 0.5484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.6869 - acc: 0.5800 - val_loss: 0.6868 - val_acc: 0.5484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6875 - acc: 0.5750 - val_loss: 0.6865 - val_acc: 0.5548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.6862 - acc: 0.5825 - val_loss: 0.6862 - val_acc: 0.5613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.6852 - acc: 0.6000 - val_loss: 0.6860 - val_acc: 0.5613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.6851 - acc: 0.5650 - val_loss: 0.6857 - val_acc: 0.5613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.6852 - acc: 0.6025 - val_loss: 0.6854 - val_acc: 0.5742\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.6848 - acc: 0.5875 - val_loss: 0.6851 - val_acc: 0.5677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.6855 - acc: 0.5775 - val_loss: 0.6847 - val_acc: 0.5677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6843 - acc: 0.5800 - val_loss: 0.6844 - val_acc: 0.5677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 16us/sample - loss: 0.6830 - acc: 0.5975 - val_loss: 0.6841 - val_acc: 0.5677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.6828 - acc: 0.5675 - val_loss: 0.6838 - val_acc: 0.5677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.6829 - acc: 0.6100 - val_loss: 0.6834 - val_acc: 0.5677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.6825 - acc: 0.5900 - val_loss: 0.6831 - val_acc: 0.5677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.6832 - acc: 0.5900 - val_loss: 0.6828 - val_acc: 0.5677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6821 - acc: 0.5850 - val_loss: 0.6824 - val_acc: 0.5677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.6805 - acc: 0.6000 - val_loss: 0.6820 - val_acc: 0.5677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6803 - acc: 0.5700 - val_loss: 0.6817 - val_acc: 0.5677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.6802 - acc: 0.6050 - val_loss: 0.6813 - val_acc: 0.5677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.6798 - acc: 0.5975 - val_loss: 0.6809 - val_acc: 0.5677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6807 - acc: 0.5950 - val_loss: 0.6805 - val_acc: 0.5677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.6797 - acc: 0.5850 - val_loss: 0.6801 - val_acc: 0.5677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6778 - acc: 0.6050 - val_loss: 0.6797 - val_acc: 0.5677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6775 - acc: 0.5675 - val_loss: 0.6793 - val_acc: 0.5742\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6773 - acc: 0.6175 - val_loss: 0.6788 - val_acc: 0.5742\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6769 - acc: 0.6050 - val_loss: 0.6784 - val_acc: 0.5742\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.6777 - acc: 0.6075 - val_loss: 0.6779 - val_acc: 0.5742\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6770 - acc: 0.5925 - val_loss: 0.6774 - val_acc: 0.5742\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6747 - acc: 0.6075 - val_loss: 0.6770 - val_acc: 0.5806\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6743 - acc: 0.5825 - val_loss: 0.6765 - val_acc: 0.5806\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6739 - acc: 0.6425 - val_loss: 0.6760 - val_acc: 0.5806\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.6735 - acc: 0.6125 - val_loss: 0.6755 - val_acc: 0.5871\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.6744 - acc: 0.6150 - val_loss: 0.6749 - val_acc: 0.5871\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.6740 - acc: 0.6000 - val_loss: 0.6744 - val_acc: 0.5871\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6714 - acc: 0.6225 - val_loss: 0.6739 - val_acc: 0.5871\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6707 - acc: 0.5950 - val_loss: 0.6734 - val_acc: 0.5935\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.6703 - acc: 0.6500 - val_loss: 0.6729 - val_acc: 0.5935\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.6698 - acc: 0.6200 - val_loss: 0.6724 - val_acc: 0.5935\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 16us/sample - loss: 0.6709 - acc: 0.6150 - val_loss: 0.6718 - val_acc: 0.5935\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.6708 - acc: 0.6025 - val_loss: 0.6713 - val_acc: 0.6000\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.6678 - acc: 0.6200 - val_loss: 0.6708 - val_acc: 0.6000\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.6669 - acc: 0.5950 - val_loss: 0.6702 - val_acc: 0.6000\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.6665 - acc: 0.6475 - val_loss: 0.6697 - val_acc: 0.6000\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 16us/sample - loss: 0.6658 - acc: 0.6250 - val_loss: 0.6691 - val_acc: 0.6000\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.6670 - acc: 0.6125 - val_loss: 0.6685 - val_acc: 0.6000\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.6674 - acc: 0.6025 - val_loss: 0.6679 - val_acc: 0.6000\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.6639 - acc: 0.6250 - val_loss: 0.6673 - val_acc: 0.6000\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.6628 - acc: 0.6050 - val_loss: 0.6666 - val_acc: 0.6000\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.6623 - acc: 0.6450 - val_loss: 0.6660 - val_acc: 0.6000\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.6616 - acc: 0.6325 - val_loss: 0.6654 - val_acc: 0.6000\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.6628 - acc: 0.6300 - val_loss: 0.6648 - val_acc: 0.6000\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.6639 - acc: 0.6075 - val_loss: 0.6641 - val_acc: 0.6065\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6599 - acc: 0.6275 - val_loss: 0.6635 - val_acc: 0.6129\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.6585 - acc: 0.6225 - val_loss: 0.6629 - val_acc: 0.6129\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.6579 - acc: 0.6500 - val_loss: 0.6622 - val_acc: 0.6129\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.6570 - acc: 0.6450 - val_loss: 0.6616 - val_acc: 0.6129\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.6582 - acc: 0.6400 - val_loss: 0.6609 - val_acc: 0.6129\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.6603 - acc: 0.6050 - val_loss: 0.6603 - val_acc: 0.6194\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.6556 - acc: 0.6400 - val_loss: 0.6596 - val_acc: 0.6194\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.6539 - acc: 0.6350 - val_loss: 0.6590 - val_acc: 0.6194\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.6534 - acc: 0.6575 - val_loss: 0.6583 - val_acc: 0.6194\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6523 - acc: 0.6525 - val_loss: 0.6577 - val_acc: 0.6194\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6534 - acc: 0.6500 - val_loss: 0.6570 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.6566 - acc: 0.6225 - val_loss: 0.6563 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.6513 - acc: 0.6550 - val_loss: 0.6556 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.6495 - acc: 0.6525 - val_loss: 0.6550 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.6487 - acc: 0.6750 - val_loss: 0.6543 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.6474 - acc: 0.6650 - val_loss: 0.6536 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6487 - acc: 0.6600 - val_loss: 0.6530 - val_acc: 0.6323\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 16us/sample - loss: 0.6529 - acc: 0.6250 - val_loss: 0.6523 - val_acc: 0.6323\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 16us/sample - loss: 0.6471 - acc: 0.6600 - val_loss: 0.6516 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.6449 - acc: 0.6650 - val_loss: 0.6509 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.6439 - acc: 0.6800 - val_loss: 0.6502 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.6425 - acc: 0.6725 - val_loss: 0.6495 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.6440 - acc: 0.6575 - val_loss: 0.6487 - val_acc: 0.6323\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6491 - acc: 0.6350 - val_loss: 0.6480 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.6428 - acc: 0.6675 - val_loss: 0.6472 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6405 - acc: 0.6675 - val_loss: 0.6465 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.6390 - acc: 0.6825 - val_loss: 0.6457 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.6375 - acc: 0.6775 - val_loss: 0.6451 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6392 - acc: 0.6675 - val_loss: 0.6444 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.6452 - acc: 0.6475 - val_loss: 0.6437 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6387 - acc: 0.6775 - val_loss: 0.6429 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.6365 - acc: 0.6725 - val_loss: 0.6423 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.6343 - acc: 0.6850 - val_loss: 0.6416 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.6328 - acc: 0.6875 - val_loss: 0.6410 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.6347 - acc: 0.6775 - val_loss: 0.6403 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6416 - acc: 0.6550 - val_loss: 0.6396 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.6348 - acc: 0.6900 - val_loss: 0.6390 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.6328 - acc: 0.6725 - val_loss: 0.6384 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6297 - acc: 0.6800 - val_loss: 0.6379 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.6283 - acc: 0.6875 - val_loss: 0.6373 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6306 - acc: 0.6875 - val_loss: 0.6368 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.6383 - acc: 0.6500 - val_loss: 0.6362 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.6311 - acc: 0.6950 - val_loss: 0.6356 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.6293 - acc: 0.6700 - val_loss: 0.6350 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.6253 - acc: 0.6825 - val_loss: 0.6345 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.6242 - acc: 0.6900 - val_loss: 0.6340 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.6264 - acc: 0.7025 - val_loss: 0.6334 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.6349 - acc: 0.6625 - val_loss: 0.6329 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6274 - acc: 0.6925 - val_loss: 0.6322 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.6260 - acc: 0.6675 - val_loss: 0.6316 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6209 - acc: 0.6875 - val_loss: 0.6311 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.6201 - acc: 0.7025 - val_loss: 0.6305 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.6224 - acc: 0.7000 - val_loss: 0.6300 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6315 - acc: 0.6625 - val_loss: 0.6294 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.6239 - acc: 0.6925 - val_loss: 0.6288 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.6227 - acc: 0.6775 - val_loss: 0.6282 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.6165 - acc: 0.6975 - val_loss: 0.6278 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.6158 - acc: 0.7050 - val_loss: 0.6273 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.6184 - acc: 0.7100 - val_loss: 0.6268 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6282 - acc: 0.6675 - val_loss: 0.6263 - val_acc: 0.6452\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6205 - acc: 0.6975 - val_loss: 0.6256 - val_acc: 0.6452\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6194 - acc: 0.6900 - val_loss: 0.6251 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.6121 - acc: 0.7025 - val_loss: 0.6246 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.6115 - acc: 0.7025 - val_loss: 0.6242 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.6145 - acc: 0.7125 - val_loss: 0.6237 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.6251 - acc: 0.6650 - val_loss: 0.6232 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.6172 - acc: 0.6975 - val_loss: 0.6227 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.6161 - acc: 0.6975 - val_loss: 0.6223 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6074 - acc: 0.7075 - val_loss: 0.6219 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.6072 - acc: 0.7175 - val_loss: 0.6215 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.6109 - acc: 0.7175 - val_loss: 0.6211 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.6219 - acc: 0.6750 - val_loss: 0.6205 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.6141 - acc: 0.7075 - val_loss: 0.6200 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.6128 - acc: 0.6925 - val_loss: 0.6195 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6027 - acc: 0.7150 - val_loss: 0.6191 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6031 - acc: 0.7225 - val_loss: 0.6187 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6075 - acc: 0.7200 - val_loss: 0.6183 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.6189 - acc: 0.6800 - val_loss: 0.6179 - val_acc: 0.6452\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.6110 - acc: 0.7050 - val_loss: 0.6173 - val_acc: 0.6452\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.6096 - acc: 0.6950 - val_loss: 0.6168 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5980 - acc: 0.7275 - val_loss: 0.6164 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5991 - acc: 0.7175 - val_loss: 0.6160 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.6042 - acc: 0.7150 - val_loss: 0.6157 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.6159 - acc: 0.6825 - val_loss: 0.6153 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.6080 - acc: 0.7075 - val_loss: 0.6148 - val_acc: 0.6710\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6063 - acc: 0.6925 - val_loss: 0.6144 - val_acc: 0.6710\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 36us/sample - loss: 0.5938 - acc: 0.7325 - val_loss: 0.6140 - val_acc: 0.6710\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5954 - acc: 0.7100 - val_loss: 0.6137 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6011 - acc: 0.7175 - val_loss: 0.6134 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.6132 - acc: 0.6875 - val_loss: 0.6129 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.6051 - acc: 0.7100 - val_loss: 0.6125 - val_acc: 0.6710\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.6032 - acc: 0.7025 - val_loss: 0.6121 - val_acc: 0.6710\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5896 - acc: 0.7400 - val_loss: 0.6118 - val_acc: 0.6710\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.5918 - acc: 0.7100 - val_loss: 0.6116 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5982 - acc: 0.7250 - val_loss: 0.6113 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6108 - acc: 0.7000 - val_loss: 0.6109 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6022 - acc: 0.7150 - val_loss: 0.6105 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6001 - acc: 0.7075 - val_loss: 0.6101 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5856 - acc: 0.7425 - val_loss: 0.6098 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5884 - acc: 0.7100 - val_loss: 0.6096 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5954 - acc: 0.7300 - val_loss: 0.6094 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.6084 - acc: 0.7050 - val_loss: 0.6090 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5994 - acc: 0.7250 - val_loss: 0.6085 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5971 - acc: 0.7175 - val_loss: 0.6082 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5818 - acc: 0.7550 - val_loss: 0.6078 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5851 - acc: 0.7200 - val_loss: 0.6076 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5928 - acc: 0.7350 - val_loss: 0.6073 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6062 - acc: 0.7075 - val_loss: 0.6069 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5967 - acc: 0.7275 - val_loss: 0.6065 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5943 - acc: 0.7200 - val_loss: 0.6060 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5784 - acc: 0.7625 - val_loss: 0.6055 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5821 - acc: 0.7225 - val_loss: 0.6053 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5904 - acc: 0.7375 - val_loss: 0.6050 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6038 - acc: 0.7100 - val_loss: 0.6046 - val_acc: 0.6581\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5942 - acc: 0.7275 - val_loss: 0.6041 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5916 - acc: 0.7225 - val_loss: 0.6037 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5751 - acc: 0.7750 - val_loss: 0.6033 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5793 - acc: 0.7275 - val_loss: 0.6031 - val_acc: 0.6710\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5881 - acc: 0.7375 - val_loss: 0.6029 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.6017 - acc: 0.7175 - val_loss: 0.6025 - val_acc: 0.6710\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5919 - acc: 0.7275 - val_loss: 0.6018 - val_acc: 0.6710\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5890 - acc: 0.7375 - val_loss: 0.6014 - val_acc: 0.6710\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5720 - acc: 0.7800 - val_loss: 0.6011 - val_acc: 0.6710\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5765 - acc: 0.7350 - val_loss: 0.6010 - val_acc: 0.6710\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5860 - acc: 0.7375 - val_loss: 0.6007 - val_acc: 0.6774\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5996 - acc: 0.7150 - val_loss: 0.6003 - val_acc: 0.6774\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5892 - acc: 0.7375 - val_loss: 0.5999 - val_acc: 0.6774\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5865 - acc: 0.7375 - val_loss: 0.5996 - val_acc: 0.6774\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5688 - acc: 0.7825 - val_loss: 0.5993 - val_acc: 0.6774\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5737 - acc: 0.7350 - val_loss: 0.5991 - val_acc: 0.6774\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5840 - acc: 0.7350 - val_loss: 0.5988 - val_acc: 0.6774\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5977 - acc: 0.7150 - val_loss: 0.5984 - val_acc: 0.6774\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5870 - acc: 0.7375 - val_loss: 0.5977 - val_acc: 0.6903\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5842 - acc: 0.7350 - val_loss: 0.5972 - val_acc: 0.6903\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5660 - acc: 0.7800 - val_loss: 0.5968 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5713 - acc: 0.7400 - val_loss: 0.5966 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5821 - acc: 0.7350 - val_loss: 0.5963 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5955 - acc: 0.7175 - val_loss: 0.5959 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5845 - acc: 0.7400 - val_loss: 0.5954 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5820 - acc: 0.7375 - val_loss: 0.5950 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5632 - acc: 0.7825 - val_loss: 0.5946 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.5687 - acc: 0.7450 - val_loss: 0.5944 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5802 - acc: 0.7450 - val_loss: 0.5940 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5936 - acc: 0.7250 - val_loss: 0.5935 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 34us/sample - loss: 0.5822 - acc: 0.7400 - val_loss: 0.5930 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5801 - acc: 0.7425 - val_loss: 0.5923 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5608 - acc: 0.7875 - val_loss: 0.5921 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5663 - acc: 0.7500 - val_loss: 0.5921 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5785 - acc: 0.7450 - val_loss: 0.5921 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5919 - acc: 0.7250 - val_loss: 0.5917 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5798 - acc: 0.7425 - val_loss: 0.5912 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5778 - acc: 0.7450 - val_loss: 0.5907 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5581 - acc: 0.7900 - val_loss: 0.5906 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5637 - acc: 0.7500 - val_loss: 0.5905 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5768 - acc: 0.7450 - val_loss: 0.5902 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5902 - acc: 0.7250 - val_loss: 0.5896 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5777 - acc: 0.7375 - val_loss: 0.5889 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5759 - acc: 0.7400 - val_loss: 0.5883 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5556 - acc: 0.7850 - val_loss: 0.5881 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5616 - acc: 0.7475 - val_loss: 0.5882 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5751 - acc: 0.7450 - val_loss: 0.5881 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5883 - acc: 0.7275 - val_loss: 0.5875 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5755 - acc: 0.7375 - val_loss: 0.5871 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5741 - acc: 0.7425 - val_loss: 0.5867 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5532 - acc: 0.7850 - val_loss: 0.5864 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5592 - acc: 0.7475 - val_loss: 0.5864 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5735 - acc: 0.7475 - val_loss: 0.5861 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5867 - acc: 0.7250 - val_loss: 0.5856 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5736 - acc: 0.7350 - val_loss: 0.5850 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5725 - acc: 0.7525 - val_loss: 0.5846 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5509 - acc: 0.7850 - val_loss: 0.5844 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5570 - acc: 0.7475 - val_loss: 0.5846 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5721 - acc: 0.7475 - val_loss: 0.5846 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5856 - acc: 0.7250 - val_loss: 0.5844 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5714 - acc: 0.7375 - val_loss: 0.5838 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5708 - acc: 0.7500 - val_loss: 0.5833 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5484 - acc: 0.7850 - val_loss: 0.5831 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5548 - acc: 0.7500 - val_loss: 0.5829 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5705 - acc: 0.7500 - val_loss: 0.5826 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5837 - acc: 0.7275 - val_loss: 0.5820 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5695 - acc: 0.7375 - val_loss: 0.5817 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5693 - acc: 0.7550 - val_loss: 0.5812 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5466 - acc: 0.7925 - val_loss: 0.5813 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5527 - acc: 0.7525 - val_loss: 0.5815 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5693 - acc: 0.7500 - val_loss: 0.5816 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5824 - acc: 0.7275 - val_loss: 0.5811 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5678 - acc: 0.7375 - val_loss: 0.5805 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5677 - acc: 0.7550 - val_loss: 0.5802 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5442 - acc: 0.7900 - val_loss: 0.5801 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5507 - acc: 0.7550 - val_loss: 0.5802 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5680 - acc: 0.7525 - val_loss: 0.5802 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5810 - acc: 0.7275 - val_loss: 0.5796 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5659 - acc: 0.7400 - val_loss: 0.5791 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5663 - acc: 0.7550 - val_loss: 0.5787 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5421 - acc: 0.7900 - val_loss: 0.5786 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5488 - acc: 0.7550 - val_loss: 0.5787 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5666 - acc: 0.7525 - val_loss: 0.5785 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5796 - acc: 0.7325 - val_loss: 0.5780 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5641 - acc: 0.7425 - val_loss: 0.5775 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5649 - acc: 0.7550 - val_loss: 0.5771 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.5402 - acc: 0.7875 - val_loss: 0.5770 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5469 - acc: 0.7550 - val_loss: 0.5771 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5654 - acc: 0.7525 - val_loss: 0.5769 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5781 - acc: 0.7325 - val_loss: 0.5764 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5622 - acc: 0.7475 - val_loss: 0.5760 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5636 - acc: 0.7575 - val_loss: 0.5754 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5384 - acc: 0.7875 - val_loss: 0.5753 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5451 - acc: 0.7600 - val_loss: 0.5756 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5642 - acc: 0.7550 - val_loss: 0.5757 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5770 - acc: 0.7350 - val_loss: 0.5752 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.5605 - acc: 0.7550 - val_loss: 0.5747 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5622 - acc: 0.7625 - val_loss: 0.5743 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5365 - acc: 0.7875 - val_loss: 0.5742 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5432 - acc: 0.7675 - val_loss: 0.5744 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5629 - acc: 0.7550 - val_loss: 0.5744 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5757 - acc: 0.7350 - val_loss: 0.5737 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5587 - acc: 0.7575 - val_loss: 0.5733 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5609 - acc: 0.7650 - val_loss: 0.5732 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5345 - acc: 0.7925 - val_loss: 0.5731 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5412 - acc: 0.7675 - val_loss: 0.5733 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5617 - acc: 0.7525 - val_loss: 0.5733 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5743 - acc: 0.7350 - val_loss: 0.5728 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5568 - acc: 0.7550 - val_loss: 0.5722 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5595 - acc: 0.7650 - val_loss: 0.5719 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5327 - acc: 0.7900 - val_loss: 0.5717 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5393 - acc: 0.7650 - val_loss: 0.5720 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5605 - acc: 0.7525 - val_loss: 0.5720 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5729 - acc: 0.7350 - val_loss: 0.5715 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5551 - acc: 0.7525 - val_loss: 0.5708 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5584 - acc: 0.7650 - val_loss: 0.5703 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5310 - acc: 0.7900 - val_loss: 0.5705 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5375 - acc: 0.7675 - val_loss: 0.5710 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5594 - acc: 0.7550 - val_loss: 0.5713 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5717 - acc: 0.7375 - val_loss: 0.5709 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5534 - acc: 0.7525 - val_loss: 0.5702 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5570 - acc: 0.7675 - val_loss: 0.5699 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5291 - acc: 0.7875 - val_loss: 0.5700 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5356 - acc: 0.7700 - val_loss: 0.5703 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5581 - acc: 0.7550 - val_loss: 0.5701 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5703 - acc: 0.7375 - val_loss: 0.5694 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5518 - acc: 0.7500 - val_loss: 0.5691 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5559 - acc: 0.7700 - val_loss: 0.5691 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5273 - acc: 0.7875 - val_loss: 0.5693 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5341 - acc: 0.7700 - val_loss: 0.5699 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5571 - acc: 0.7550 - val_loss: 0.5699 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.5693 - acc: 0.7400 - val_loss: 0.5692 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5503 - acc: 0.7500 - val_loss: 0.5684 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5547 - acc: 0.7675 - val_loss: 0.5681 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.5258 - acc: 0.7850 - val_loss: 0.5683 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.5323 - acc: 0.7675 - val_loss: 0.5686 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5559 - acc: 0.7525 - val_loss: 0.5686 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5679 - acc: 0.7350 - val_loss: 0.5681 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5487 - acc: 0.7500 - val_loss: 0.5674 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5536 - acc: 0.7700 - val_loss: 0.5672 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.5242 - acc: 0.7850 - val_loss: 0.5674 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 34us/sample - loss: 0.5307 - acc: 0.7675 - val_loss: 0.5677 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5548 - acc: 0.7525 - val_loss: 0.5676 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5667 - acc: 0.7325 - val_loss: 0.5672 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5472 - acc: 0.7500 - val_loss: 0.5669 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5525 - acc: 0.7700 - val_loss: 0.5668 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5226 - acc: 0.7850 - val_loss: 0.5669 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5292 - acc: 0.7675 - val_loss: 0.5671 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5540 - acc: 0.7475 - val_loss: 0.5673 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.5659 - acc: 0.7325 - val_loss: 0.5666 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5459 - acc: 0.7525 - val_loss: 0.5660 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.5515 - acc: 0.7725 - val_loss: 0.5657 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5211 - acc: 0.7850 - val_loss: 0.5658 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5277 - acc: 0.7750 - val_loss: 0.5662 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5529 - acc: 0.7475 - val_loss: 0.5663 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5648 - acc: 0.7325 - val_loss: 0.5659 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5446 - acc: 0.7550 - val_loss: 0.5654 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5505 - acc: 0.7725 - val_loss: 0.5653 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5198 - acc: 0.7875 - val_loss: 0.5651 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5263 - acc: 0.7775 - val_loss: 0.5656 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5521 - acc: 0.7500 - val_loss: 0.5656 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5638 - acc: 0.7350 - val_loss: 0.5650 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.5432 - acc: 0.7550 - val_loss: 0.5644 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.5496 - acc: 0.7750 - val_loss: 0.5641 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5184 - acc: 0.7875 - val_loss: 0.5645 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5249 - acc: 0.7775 - val_loss: 0.5651 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5512 - acc: 0.7525 - val_loss: 0.5652 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5630 - acc: 0.7375 - val_loss: 0.5646 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5418 - acc: 0.7550 - val_loss: 0.5641 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5486 - acc: 0.7725 - val_loss: 0.5638 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5168 - acc: 0.7875 - val_loss: 0.5637 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5237 - acc: 0.7775 - val_loss: 0.5641 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5501 - acc: 0.7525 - val_loss: 0.5640 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.5619 - acc: 0.7375 - val_loss: 0.5632 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5405 - acc: 0.7575 - val_loss: 0.5628 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.5477 - acc: 0.7800 - val_loss: 0.5626 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5157 - acc: 0.7900 - val_loss: 0.5623 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5226 - acc: 0.7800 - val_loss: 0.5629 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.5492 - acc: 0.7525 - val_loss: 0.5629 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5608 - acc: 0.7375 - val_loss: 0.5621 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5390 - acc: 0.7575 - val_loss: 0.5615 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5467 - acc: 0.7800 - val_loss: 0.5610 - val_acc: 0.7677\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5145 - acc: 0.7875 - val_loss: 0.5612 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5215 - acc: 0.7825 - val_loss: 0.5615 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.5480 - acc: 0.7575 - val_loss: 0.5617 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5598 - acc: 0.7400 - val_loss: 0.5616 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5374 - acc: 0.7625 - val_loss: 0.5613 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5454 - acc: 0.7800 - val_loss: 0.5610 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5129 - acc: 0.7925 - val_loss: 0.5613 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5202 - acc: 0.7750 - val_loss: 0.5622 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5474 - acc: 0.7575 - val_loss: 0.5623 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5592 - acc: 0.7425 - val_loss: 0.5618 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5359 - acc: 0.7625 - val_loss: 0.5612 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.5444 - acc: 0.7750 - val_loss: 0.5609 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5114 - acc: 0.7925 - val_loss: 0.5610 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5189 - acc: 0.7800 - val_loss: 0.5614 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5466 - acc: 0.7575 - val_loss: 0.5617 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5582 - acc: 0.7425 - val_loss: 0.5611 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5345 - acc: 0.7625 - val_loss: 0.5609 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5433 - acc: 0.7775 - val_loss: 0.5607 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5099 - acc: 0.7950 - val_loss: 0.5609 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5177 - acc: 0.7800 - val_loss: 0.5613 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5457 - acc: 0.7600 - val_loss: 0.5611 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5572 - acc: 0.7375 - val_loss: 0.5605 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 34us/sample - loss: 0.5333 - acc: 0.7650 - val_loss: 0.5602 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5423 - acc: 0.7775 - val_loss: 0.5600 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5086 - acc: 0.7950 - val_loss: 0.5603 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5164 - acc: 0.7775 - val_loss: 0.5610 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5449 - acc: 0.7575 - val_loss: 0.5611 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5566 - acc: 0.7400 - val_loss: 0.5603 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5322 - acc: 0.7700 - val_loss: 0.5598 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5413 - acc: 0.7775 - val_loss: 0.5595 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5075 - acc: 0.7975 - val_loss: 0.5597 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.5153 - acc: 0.7825 - val_loss: 0.5603 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5439 - acc: 0.7575 - val_loss: 0.5605 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 36us/sample - loss: 0.5557 - acc: 0.7425 - val_loss: 0.5601 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5310 - acc: 0.7700 - val_loss: 0.5597 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5402 - acc: 0.7750 - val_loss: 0.5595 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5061 - acc: 0.8000 - val_loss: 0.5597 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5140 - acc: 0.7825 - val_loss: 0.5602 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.5431 - acc: 0.7575 - val_loss: 0.5605 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.5549 - acc: 0.7425 - val_loss: 0.5599 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.5300 - acc: 0.7700 - val_loss: 0.5595 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5393 - acc: 0.7775 - val_loss: 0.5593 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.5047 - acc: 0.8025 - val_loss: 0.5595 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5128 - acc: 0.7875 - val_loss: 0.5601 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5425 - acc: 0.7575 - val_loss: 0.5606 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5542 - acc: 0.7425 - val_loss: 0.5601 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5291 - acc: 0.7750 - val_loss: 0.5594 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5383 - acc: 0.7775 - val_loss: 0.5590 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5034 - acc: 0.8025 - val_loss: 0.5594 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.5117 - acc: 0.7850 - val_loss: 0.5599 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5417 - acc: 0.7600 - val_loss: 0.5604 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5534 - acc: 0.7425 - val_loss: 0.5598 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5281 - acc: 0.7750 - val_loss: 0.5591 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5374 - acc: 0.7825 - val_loss: 0.5590 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 16us/sample - loss: 0.5022 - acc: 0.8025 - val_loss: 0.5595 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5108 - acc: 0.7850 - val_loss: 0.5603 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5409 - acc: 0.7600 - val_loss: 0.5605 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5528 - acc: 0.7425 - val_loss: 0.5597 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5272 - acc: 0.7750 - val_loss: 0.5591 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5366 - acc: 0.7850 - val_loss: 0.5588 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5011 - acc: 0.8050 - val_loss: 0.5592 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.5096 - acc: 0.7850 - val_loss: 0.5598 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5402 - acc: 0.7600 - val_loss: 0.5601 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5520 - acc: 0.7425 - val_loss: 0.5595 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.5263 - acc: 0.7750 - val_loss: 0.5592 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5357 - acc: 0.7825 - val_loss: 0.5588 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4999 - acc: 0.8075 - val_loss: 0.5591 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5087 - acc: 0.7850 - val_loss: 0.5599 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5396 - acc: 0.7625 - val_loss: 0.5601 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5514 - acc: 0.7475 - val_loss: 0.5594 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5255 - acc: 0.7750 - val_loss: 0.5589 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5350 - acc: 0.7825 - val_loss: 0.5584 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.4990 - acc: 0.8025 - val_loss: 0.5588 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5079 - acc: 0.7825 - val_loss: 0.5597 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5390 - acc: 0.7650 - val_loss: 0.5599 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5508 - acc: 0.7450 - val_loss: 0.5594 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5247 - acc: 0.7775 - val_loss: 0.5589 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5342 - acc: 0.7800 - val_loss: 0.5586 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4981 - acc: 0.8025 - val_loss: 0.5587 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5071 - acc: 0.7850 - val_loss: 0.5598 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5385 - acc: 0.7600 - val_loss: 0.5600 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5502 - acc: 0.7425 - val_loss: 0.5596 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5239 - acc: 0.7775 - val_loss: 0.5591 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5334 - acc: 0.7825 - val_loss: 0.5588 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4969 - acc: 0.8075 - val_loss: 0.5596 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5061 - acc: 0.7850 - val_loss: 0.5600 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5380 - acc: 0.7600 - val_loss: 0.5599 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5496 - acc: 0.7425 - val_loss: 0.5594 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5232 - acc: 0.7750 - val_loss: 0.5589 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5327 - acc: 0.7850 - val_loss: 0.5585 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.4959 - acc: 0.8100 - val_loss: 0.5588 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5053 - acc: 0.7850 - val_loss: 0.5594 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.5373 - acc: 0.7600 - val_loss: 0.5597 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5488 - acc: 0.7425 - val_loss: 0.5592 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5224 - acc: 0.7750 - val_loss: 0.5587 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.5319 - acc: 0.7875 - val_loss: 0.5589 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.4950 - acc: 0.8100 - val_loss: 0.5592 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5046 - acc: 0.7850 - val_loss: 0.5599 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5369 - acc: 0.7600 - val_loss: 0.5600 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5483 - acc: 0.7425 - val_loss: 0.5592 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5216 - acc: 0.7750 - val_loss: 0.5589 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5311 - acc: 0.7875 - val_loss: 0.5587 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.4943 - acc: 0.8050 - val_loss: 0.5595 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 51us/sample - loss: 0.5039 - acc: 0.7850 - val_loss: 0.5602 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5365 - acc: 0.7575 - val_loss: 0.5604 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5478 - acc: 0.7425 - val_loss: 0.5594 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5209 - acc: 0.7750 - val_loss: 0.5588 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5304 - acc: 0.7850 - val_loss: 0.5586 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.4933 - acc: 0.8050 - val_loss: 0.5588 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5036 - acc: 0.7825 - val_loss: 0.5600 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5357 - acc: 0.7600 - val_loss: 0.5601 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5469 - acc: 0.7425 - val_loss: 0.5596 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.5200 - acc: 0.7750 - val_loss: 0.5593 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5298 - acc: 0.7850 - val_loss: 0.5588 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.4925 - acc: 0.8050 - val_loss: 0.5595 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5025 - acc: 0.7825 - val_loss: 0.5604 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5354 - acc: 0.7575 - val_loss: 0.5604 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5464 - acc: 0.7425 - val_loss: 0.5597 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5193 - acc: 0.7700 - val_loss: 0.5593 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5289 - acc: 0.7825 - val_loss: 0.5591 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4916 - acc: 0.8050 - val_loss: 0.5593 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5019 - acc: 0.7825 - val_loss: 0.5603 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5349 - acc: 0.7575 - val_loss: 0.5608 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5458 - acc: 0.7425 - val_loss: 0.5601 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5186 - acc: 0.7700 - val_loss: 0.5596 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.5282 - acc: 0.7825 - val_loss: 0.5594 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.4907 - acc: 0.8050 - val_loss: 0.5602 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5012 - acc: 0.7825 - val_loss: 0.5611 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5346 - acc: 0.7575 - val_loss: 0.5614 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5452 - acc: 0.7375 - val_loss: 0.5604 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5177 - acc: 0.7700 - val_loss: 0.5600 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5274 - acc: 0.7825 - val_loss: 0.5596 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.4898 - acc: 0.8050 - val_loss: 0.5602 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5006 - acc: 0.7800 - val_loss: 0.5607 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5337 - acc: 0.7525 - val_loss: 0.5608 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5443 - acc: 0.7375 - val_loss: 0.5605 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5171 - acc: 0.7700 - val_loss: 0.5604 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5267 - acc: 0.7825 - val_loss: 0.5600 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4888 - acc: 0.8050 - val_loss: 0.5606 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5000 - acc: 0.7825 - val_loss: 0.5615 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5334 - acc: 0.7500 - val_loss: 0.5617 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5439 - acc: 0.7375 - val_loss: 0.5608 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5163 - acc: 0.7700 - val_loss: 0.5605 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5258 - acc: 0.7825 - val_loss: 0.5605 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4880 - acc: 0.8050 - val_loss: 0.5610 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4996 - acc: 0.7825 - val_loss: 0.5617 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5330 - acc: 0.7500 - val_loss: 0.5615 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5431 - acc: 0.7375 - val_loss: 0.5606 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.5158 - acc: 0.7725 - val_loss: 0.5600 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5252 - acc: 0.7900 - val_loss: 0.5599 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.4875 - acc: 0.8050 - val_loss: 0.5606 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.4991 - acc: 0.7800 - val_loss: 0.5618 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5325 - acc: 0.7525 - val_loss: 0.5621 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5426 - acc: 0.7375 - val_loss: 0.5613 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5152 - acc: 0.7750 - val_loss: 0.5607 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5244 - acc: 0.7925 - val_loss: 0.5608 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 16us/sample - loss: 0.4865 - acc: 0.8075 - val_loss: 0.5613 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.4988 - acc: 0.7825 - val_loss: 0.5626 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5322 - acc: 0.7500 - val_loss: 0.5628 - val_acc: 0.7613\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5421 - acc: 0.7375 - val_loss: 0.5622 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5143 - acc: 0.7750 - val_loss: 0.5614 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5237 - acc: 0.7875 - val_loss: 0.5612 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4856 - acc: 0.8050 - val_loss: 0.5614 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4979 - acc: 0.7825 - val_loss: 0.5620 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5313 - acc: 0.7575 - val_loss: 0.5618 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 36us/sample - loss: 0.5413 - acc: 0.7375 - val_loss: 0.5611 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5141 - acc: 0.7750 - val_loss: 0.5605 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5232 - acc: 0.7950 - val_loss: 0.5607 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.4850 - acc: 0.8050 - val_loss: 0.5614 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.4973 - acc: 0.7825 - val_loss: 0.5625 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5311 - acc: 0.7550 - val_loss: 0.5629 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5410 - acc: 0.7400 - val_loss: 0.5623 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5131 - acc: 0.7750 - val_loss: 0.5619 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5224 - acc: 0.7950 - val_loss: 0.5616 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 16us/sample - loss: 0.4841 - acc: 0.8075 - val_loss: 0.5619 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.4968 - acc: 0.7825 - val_loss: 0.5628 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5305 - acc: 0.7550 - val_loss: 0.5629 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5405 - acc: 0.7400 - val_loss: 0.5620 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5129 - acc: 0.7775 - val_loss: 0.5613 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 16us/sample - loss: 0.5219 - acc: 0.7950 - val_loss: 0.5611 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.4836 - acc: 0.8050 - val_loss: 0.5617 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.4966 - acc: 0.7850 - val_loss: 0.5630 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5304 - acc: 0.7550 - val_loss: 0.5636 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5401 - acc: 0.7400 - val_loss: 0.5630 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5121 - acc: 0.7750 - val_loss: 0.5622 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5211 - acc: 0.7950 - val_loss: 0.5620 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.4827 - acc: 0.8050 - val_loss: 0.5623 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.4957 - acc: 0.7850 - val_loss: 0.5626 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5293 - acc: 0.7550 - val_loss: 0.5626 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5392 - acc: 0.7425 - val_loss: 0.5621 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5115 - acc: 0.7775 - val_loss: 0.5617 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 16us/sample - loss: 0.5205 - acc: 0.7950 - val_loss: 0.5617 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 16us/sample - loss: 0.4821 - acc: 0.8050 - val_loss: 0.5622 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.4954 - acc: 0.7850 - val_loss: 0.5630 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5290 - acc: 0.7550 - val_loss: 0.5631 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5389 - acc: 0.7425 - val_loss: 0.5627 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5110 - acc: 0.7775 - val_loss: 0.5621 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5199 - acc: 0.7950 - val_loss: 0.5617 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.4815 - acc: 0.8075 - val_loss: 0.5623 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.4948 - acc: 0.7875 - val_loss: 0.5630 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5284 - acc: 0.7550 - val_loss: 0.5630 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5384 - acc: 0.7425 - val_loss: 0.5626 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5106 - acc: 0.7775 - val_loss: 0.5620 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5194 - acc: 0.7950 - val_loss: 0.5621 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4808 - acc: 0.8050 - val_loss: 0.5627 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.4944 - acc: 0.7875 - val_loss: 0.5631 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5281 - acc: 0.7575 - val_loss: 0.5634 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5379 - acc: 0.7425 - val_loss: 0.5628 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5100 - acc: 0.7775 - val_loss: 0.5623 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5187 - acc: 0.7950 - val_loss: 0.5621 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.4803 - acc: 0.8100 - val_loss: 0.5627 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.4941 - acc: 0.7850 - val_loss: 0.5636 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5278 - acc: 0.7550 - val_loss: 0.5639 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5379 - acc: 0.7425 - val_loss: 0.5627 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 16us/sample - loss: 0.5096 - acc: 0.7775 - val_loss: 0.5622 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 16us/sample - loss: 0.5182 - acc: 0.7950 - val_loss: 0.5620 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4796 - acc: 0.8075 - val_loss: 0.5626 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 16us/sample - loss: 0.4934 - acc: 0.7900 - val_loss: 0.5634 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5271 - acc: 0.7600 - val_loss: 0.5637 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5371 - acc: 0.7425 - val_loss: 0.5630 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5091 - acc: 0.7775 - val_loss: 0.5628 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5177 - acc: 0.7975 - val_loss: 0.5629 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.4787 - acc: 0.8050 - val_loss: 0.5633 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.4929 - acc: 0.7900 - val_loss: 0.5639 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5268 - acc: 0.7575 - val_loss: 0.5640 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5367 - acc: 0.7425 - val_loss: 0.5635 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5087 - acc: 0.7750 - val_loss: 0.5628 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5172 - acc: 0.7950 - val_loss: 0.5626 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 16us/sample - loss: 0.4782 - acc: 0.8050 - val_loss: 0.5632 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.4924 - acc: 0.7900 - val_loss: 0.5640 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5265 - acc: 0.7600 - val_loss: 0.5643 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5364 - acc: 0.7450 - val_loss: 0.5639 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5082 - acc: 0.7800 - val_loss: 0.5631 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.5166 - acc: 0.7950 - val_loss: 0.5628 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.4777 - acc: 0.8075 - val_loss: 0.5631 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.4919 - acc: 0.7900 - val_loss: 0.5635 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5259 - acc: 0.7600 - val_loss: 0.5638 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5358 - acc: 0.7425 - val_loss: 0.5635 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5077 - acc: 0.7775 - val_loss: 0.5632 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5162 - acc: 0.7975 - val_loss: 0.5635 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4769 - acc: 0.8075 - val_loss: 0.5638 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4913 - acc: 0.7875 - val_loss: 0.5644 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5257 - acc: 0.7600 - val_loss: 0.5642 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5356 - acc: 0.7450 - val_loss: 0.5631 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5074 - acc: 0.7750 - val_loss: 0.5629 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5157 - acc: 0.7950 - val_loss: 0.5626 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.4766 - acc: 0.8075 - val_loss: 0.5632 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.4911 - acc: 0.7900 - val_loss: 0.5643 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5252 - acc: 0.7600 - val_loss: 0.5645 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5351 - acc: 0.7450 - val_loss: 0.5640 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5069 - acc: 0.7775 - val_loss: 0.5636 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5149 - acc: 0.7975 - val_loss: 0.5636 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.4760 - acc: 0.8100 - val_loss: 0.5642 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 16us/sample - loss: 0.4901 - acc: 0.7925 - val_loss: 0.5647 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5250 - acc: 0.7600 - val_loss: 0.5646 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5347 - acc: 0.7475 - val_loss: 0.5640 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5068 - acc: 0.7800 - val_loss: 0.5633 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5146 - acc: 0.7975 - val_loss: 0.5631 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.4753 - acc: 0.8075 - val_loss: 0.5635 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.4897 - acc: 0.7900 - val_loss: 0.5644 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5244 - acc: 0.7625 - val_loss: 0.5647 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 16us/sample - loss: 0.5344 - acc: 0.7475 - val_loss: 0.5640 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5062 - acc: 0.7800 - val_loss: 0.5637 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 16us/sample - loss: 0.5141 - acc: 0.7950 - val_loss: 0.5634 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.4747 - acc: 0.8075 - val_loss: 0.5639 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4890 - acc: 0.7900 - val_loss: 0.5645 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5240 - acc: 0.7625 - val_loss: 0.5648 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5339 - acc: 0.7475 - val_loss: 0.5642 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.5058 - acc: 0.7800 - val_loss: 0.5639 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5135 - acc: 0.7950 - val_loss: 0.5636 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 16us/sample - loss: 0.4742 - acc: 0.8075 - val_loss: 0.5643 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.4885 - acc: 0.7925 - val_loss: 0.5649 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 15us/sample - loss: 0.5237 - acc: 0.7625 - val_loss: 0.5648 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5336 - acc: 0.7475 - val_loss: 0.5645 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5054 - acc: 0.7800 - val_loss: 0.5639 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5131 - acc: 0.7950 - val_loss: 0.5634 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4738 - acc: 0.8100 - val_loss: 0.5636 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4881 - acc: 0.7900 - val_loss: 0.5641 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5230 - acc: 0.7650 - val_loss: 0.5645 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5331 - acc: 0.7450 - val_loss: 0.5641 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5053 - acc: 0.7800 - val_loss: 0.5636 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5124 - acc: 0.7925 - val_loss: 0.5636 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4732 - acc: 0.8100 - val_loss: 0.5640 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4876 - acc: 0.7925 - val_loss: 0.5647 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5228 - acc: 0.7625 - val_loss: 0.5649 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5330 - acc: 0.7475 - val_loss: 0.5642 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5048 - acc: 0.7775 - val_loss: 0.5638 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5119 - acc: 0.7925 - val_loss: 0.5636 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4727 - acc: 0.8075 - val_loss: 0.5638 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4873 - acc: 0.7950 - val_loss: 0.5648 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5223 - acc: 0.7625 - val_loss: 0.5649 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5326 - acc: 0.7475 - val_loss: 0.5645 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5044 - acc: 0.7800 - val_loss: 0.5640 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5114 - acc: 0.7925 - val_loss: 0.5636 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 16us/sample - loss: 0.4723 - acc: 0.8075 - val_loss: 0.5640 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 16us/sample - loss: 0.4867 - acc: 0.7950 - val_loss: 0.5645 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5221 - acc: 0.7625 - val_loss: 0.5650 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.5322 - acc: 0.7475 - val_loss: 0.5644 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5041 - acc: 0.7800 - val_loss: 0.5638 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5109 - acc: 0.7925 - val_loss: 0.5636 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.4718 - acc: 0.8075 - val_loss: 0.5639 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.4864 - acc: 0.7950 - val_loss: 0.5647 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5214 - acc: 0.7650 - val_loss: 0.5650 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5319 - acc: 0.7500 - val_loss: 0.5646 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.5037 - acc: 0.7825 - val_loss: 0.5639 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5104 - acc: 0.7925 - val_loss: 0.5636 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.4713 - acc: 0.8075 - val_loss: 0.5640 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.4859 - acc: 0.7975 - val_loss: 0.5646 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5212 - acc: 0.7650 - val_loss: 0.5650 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 16us/sample - loss: 0.5316 - acc: 0.7500 - val_loss: 0.5645 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 16us/sample - loss: 0.5034 - acc: 0.7750 - val_loss: 0.5638 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5101 - acc: 0.7925 - val_loss: 0.5634 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4710 - acc: 0.8075 - val_loss: 0.5636 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4855 - acc: 0.7950 - val_loss: 0.5644 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5206 - acc: 0.7650 - val_loss: 0.5647 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5313 - acc: 0.7475 - val_loss: 0.5643 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 15us/sample - loss: 0.5030 - acc: 0.7825 - val_loss: 0.5639 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5094 - acc: 0.7925 - val_loss: 0.5638 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.4704 - acc: 0.8100 - val_loss: 0.5642 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.4851 - acc: 0.7950 - val_loss: 0.5650 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5204 - acc: 0.7650 - val_loss: 0.5651 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5311 - acc: 0.7550 - val_loss: 0.5643 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5026 - acc: 0.7825 - val_loss: 0.5637 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5091 - acc: 0.7925 - val_loss: 0.5636 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4700 - acc: 0.8075 - val_loss: 0.5638 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.4847 - acc: 0.7925 - val_loss: 0.5644 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5198 - acc: 0.7650 - val_loss: 0.5646 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5306 - acc: 0.7550 - val_loss: 0.5641 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5024 - acc: 0.7875 - val_loss: 0.5637 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5087 - acc: 0.7925 - val_loss: 0.5637 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4696 - acc: 0.8100 - val_loss: 0.5642 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4842 - acc: 0.7950 - val_loss: 0.5649 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5197 - acc: 0.7650 - val_loss: 0.5652 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5306 - acc: 0.7550 - val_loss: 0.5649 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5020 - acc: 0.7825 - val_loss: 0.5643 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5083 - acc: 0.7925 - val_loss: 0.5642 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4690 - acc: 0.8100 - val_loss: 0.5644 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4837 - acc: 0.7950 - val_loss: 0.5648 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5192 - acc: 0.7650 - val_loss: 0.5650 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5301 - acc: 0.7575 - val_loss: 0.5645 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5018 - acc: 0.7875 - val_loss: 0.5641 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5078 - acc: 0.7925 - val_loss: 0.5640 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.4688 - acc: 0.8075 - val_loss: 0.5644 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.4834 - acc: 0.7950 - val_loss: 0.5651 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5190 - acc: 0.7650 - val_loss: 0.5653 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5299 - acc: 0.7550 - val_loss: 0.5648 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5014 - acc: 0.7875 - val_loss: 0.5644 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5074 - acc: 0.7925 - val_loss: 0.5641 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.4683 - acc: 0.8075 - val_loss: 0.5644 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 15us/sample - loss: 0.4829 - acc: 0.7950 - val_loss: 0.5649 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5186 - acc: 0.7650 - val_loss: 0.5650 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5295 - acc: 0.7575 - val_loss: 0.5648 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5009 - acc: 0.7925 - val_loss: 0.5644 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5069 - acc: 0.7925 - val_loss: 0.5644 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.4678 - acc: 0.8100 - val_loss: 0.5645 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.4826 - acc: 0.7950 - val_loss: 0.5650 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5183 - acc: 0.7650 - val_loss: 0.5653 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5293 - acc: 0.7575 - val_loss: 0.5650 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5007 - acc: 0.7875 - val_loss: 0.5645 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5065 - acc: 0.7925 - val_loss: 0.5643 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.4675 - acc: 0.8100 - val_loss: 0.5644 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.4823 - acc: 0.7950 - val_loss: 0.5651 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5181 - acc: 0.7650 - val_loss: 0.5650 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.5289 - acc: 0.7575 - val_loss: 0.5645 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5006 - acc: 0.7900 - val_loss: 0.5641 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.5062 - acc: 0.7950 - val_loss: 0.5640 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.4673 - acc: 0.8100 - val_loss: 0.5648 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4820 - acc: 0.7925 - val_loss: 0.5657 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5179 - acc: 0.7650 - val_loss: 0.5657 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5289 - acc: 0.7575 - val_loss: 0.5653 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5001 - acc: 0.7925 - val_loss: 0.5647 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5059 - acc: 0.7950 - val_loss: 0.5643 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 43us/sample - loss: 0.4668 - acc: 0.8125 - val_loss: 0.5645 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.4814 - acc: 0.7950 - val_loss: 0.5651 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5174 - acc: 0.7650 - val_loss: 0.5656 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5285 - acc: 0.7575 - val_loss: 0.5652 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4998 - acc: 0.7900 - val_loss: 0.5647 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5053 - acc: 0.7950 - val_loss: 0.5646 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4663 - acc: 0.8125 - val_loss: 0.5647 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.4811 - acc: 0.7950 - val_loss: 0.5653 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5170 - acc: 0.7650 - val_loss: 0.5655 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 17us/sample - loss: 0.5282 - acc: 0.7575 - val_loss: 0.5652 - val_acc: 0.7548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdeH91eRHhIq",
        "colab_type": "text"
      },
      "source": [
        "# Model History"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDuZ9cMcIkDZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "outputId": "f57812c8-7cc4-414a-da9c-11ff01f04397"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "plt.figure(num=None, figsize=(16, 8), dpi=90, facecolor='w', edgecolor='k')\n",
        "plt.plot()\n",
        "plt.plot(train_acc)\n",
        "plt.plot(val_acc)\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABLIAAAJyCAYAAADQJOl3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAN1wAADdcBQiibeAAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xd8VHW+P/7XOdMnZUp6IQkklBQI\nvUME1oZlQVh1r+u6KFhwd3Ut235cvXp37yp+rysq6i7iilyv17XhKipWQKRGIAQTOklIbzOp086c\n8/tjwpAhCS2BZJjX8/HgoTlzzpnPOZ/JJPPK5/P+CIqiKCAiIiIiIiIiIhrgxP5uABERERERERER\n0blgkEVEREREREREREGBQRYREREREREREQUFBllERERERERERBQUGGQREREREREREVFQYJBFRERE\nRERERERBgUEWEREREREREREFBQZZREREREREREQUFBhkEREREfWxb775BsOHD0d5efl5HTd79mw8\n/fTTF6lVRERERMGPQRYREREREREREQUFBllERERE1Ce8Xi/cbnd/N4OIiIguYwyyiIiI6LL2+9//\nHjfddBM2btyIuXPnIjc3F3fffTfsdjtKS0tx++23Y/To0bjppptw4MCBgGMdDgf+9Kc/Ydq0aRg5\nciQWLFiALVu2BOyjKApeeOEFTJkyBWPGjMFvf/tbtLa2dmmHy+XC8uXLkZeXh5ycHNx4443YtGnT\neV/PunXr8NOf/hQTJ07EhAkTcPvtt6OwsLDLfrt27cLtt9+OMWPGYNy4cbj99ttRVFTkf7yiogIP\nPfQQJk2ahNzcXNxwww346KOPAAA7duzA8OHDcejQoYBz3n777fj1r3/d5d5++eWXuO666zBq1Cjs\n27cPtbW1+MMf/oA5c+Zg1KhRuPrqq/HXv/61S8jldDqxfPlyzJo1Czk5OZg9ezb++7//GwCwfPly\nzJkzB4qiBBzz/vvvIycnB42Njed974iIiCj4qfu7AUREREQXW1VVFZ5//nk8+OCD/nDqscceQ3l5\nOW6++WYsXrwYzz77LB566CGsX78egiAAAJYtW4avv/4aDz30EFJSUvDOO+/gnnvuwZo1azB+/HgA\nwBtvvIGVK1finnvuwfjx4/HFF1/gmWee6dKGX//619i3bx9+9atfISUlBZ9++inuu+8+vPfee8jM\nzDznaykvL8e8efOQkpICt9uN9evX47bbbsP69esxaNAgAL4g6s4778SkSZPw1FNPwWAwYPfu3aip\nqUFWVhYaGhpwyy23wGAw4He/+x0SEhJw6NAhVFVVnfe9raiowDPPPIOlS5ciJiYGycnJsNlsMJvN\n+MMf/oDIyEiUlJTghRdegM1mw5NPPgnAFwAuXboUe/bswdKlS5GTk4Oamhrk5+cDABYsWIDVq1dj\n586dmDRpkv/53n//fcyaNQtWq/W820pERETBj0EWERERXfaamprw9ttvIyUlBQBw8OBBrF69Gk8/\n/TTmzZvn3+/uu+/GsWPHkJ6ejqNHj2L9+vX4y1/+gvnz5wMAZsyYgRtvvBEvv/wyVq9eDa/Xi1Wr\nVuGWW27Bb37zG/8+ixYtQk1Njf+827Ztw8aNG7F27VpMnDgRADB9+nSUlJTg5ZdfxvPPP3/O1/LL\nX/7S//+yLGPatGnYt28fPvzwQ/9jzz77LIYPH47Vq1f7Q7mZM2f6j3v99dfR2tqK999/H7GxsQCA\nKVOmnPsN7cRut+P1118PCOPi4+Pxu9/9zv/12LFjYTAY8Mc//hHLli2DVqvFli1b8N133+Gll17C\nnDlz/Pue7I/09HSMHTsW77//vj/IOnHiBPLz8/Hyyy9fUFuJiIgo+HFqIREREV32kpKS/CEWAKSm\npgIAJk+e7N928vGTAVRhYSEURcE111zj30cURVxzzTX4/vvvAfhGetXV1QUEMQBw5ZVXBny9detW\nxMTEYOzYsZAkyf9vypQp2L9//3ldy9GjR3H//fdj6tSpyMzMRHZ2No4fP46SkhIAQHt7OwoKCjB/\n/nx/iHW67du3Y8aMGf4Qqzfi4uK6jChTFAWvv/465s6di1GjRiE7OxuPPPII3G63f9TX9u3bYTab\nu9y7zhYuXIjPP/8cbW1tAHyjsaKjozFjxoxet5uIiIiCE0dkERER0WUvIiIi4GuNRtNl+8ltLpcL\nAFBbWwuj0QiDwRBwbFRUFBwOB9xuN+rr6/3bTt+nM5vNhrq6OmRnZ3dpm0qlOufraG1txZ133omo\nqCj8/ve/R2JiInQ6HZYtW+avP9Xc3AxFURATE9Pjeex2O0aOHHnOz3sm0dHRXbatWbMGy5cvx5Il\nSzBhwgRERkaisLAQTz75pP/+2u32M7YRAK699lr8+c9/xqeffooFCxZg3bp1+PGPfwy1mr/CEhER\nhSr+FkBERETUjdjYWLS3t8PhcASEWQ0NDTAYDNBqtf4Qp6GhIeDY0782mUyIi4vDypUre9WmvXv3\norq6Gq+99hrS09P921taWvz/HxkZCVEUUVdX1+N5zGbzGR/X6XQAAI/HE7C9qakJFovlrO387LPP\ncPXVV/unWwK+kWTn0wYAMBqNuO666/DBBx8gKSkJlZWVuOmmm876/ERERHT54tRCIiIiom6MHDkS\ngiBgw4YN/m2KomDDhg0YN24cACAhIQExMTH46quvAo794osvAr6eMmUK6uvrYTQaMXLkyC7/zpXT\n6QQAaLVa/7bdu3ejoqLC/7XRaERubi7WrVvXZcW/zu3ZsmWLf0TZ6eLj4wEEhk9VVVU4duzYObez\ncxsB+FdE7NwGu92Ob7755oznWrhwIfLz8/HCCy9g9OjRAQEeERERhR6OyCIiIiLqRnp6Oq677jo8\n+eSTaGtrw6BBg/DOO+/g2LFjePzxxwH4pgUuXrwYTz/9NCwWC8aPH4/PP/+8y+ijadOmYfr06bjz\nzjuxZMkSZGRkoLW1FQcOHIDL5cLDDz98Tm0aPXo0jEYj/v3f/x2LFy9GdXU1XnzxRcTFxQXs9/DD\nD2PRokVYvHixf3XCvXv3IicnB7NmzcIvfvELrFu3DrfddhvuvfdexMfH49ixY2hvb8eSJUsQHx+P\nnJwcrFixAgaDAbIs429/+xvMZvM5tXPq1KlYu3YtRo0ahZSUFHz00UcoLS3t9p48/PDDuP/++5GV\nlYW6ujrk5+f7VzYEgNzcXAwdOhTff/99wHYiIiIKTRyRRURERNSDP/3pT5g/fz5WrlyJpUuXoqKi\nAq+88grGjx/v3+eOO+7APffcg//7v//Dr371K7S1teHRRx8NOI8gCHjxxRexYMECrFmzBosXL8bj\njz+OPXv2+Ed3nYvo6GisWLEC9fX1WLp0KdasWYMnnnjCX7z+pAkTJuC1116D0+nEo48+it/85jfY\nuXOnf6SV1WrFW2+9hczMTPzXf/0X7r33Xrz99ttITEz0n+PZZ59FYmIiHn30UTz77LNYunQpBg8e\nfE7tvP/++3H99ddjxYoVePjhh6HRaLBs2bIu92TlypW45ZZbsGbNGixZsgTPPfdct1MX58yZA71e\nj+uuu+6c7xURERFdngSlpzHnREREREQDwMKFCzF48GA888wz/d0UIiIi6mecWkhEREREA1JhYSG2\nb9+OwsJCPPbYY/3dHCIiIhoAOCKLiIiIiAak4cOHIzIyEkuWLMHdd9/d380hIiKiAYBBFhERERER\nERERBQUWeyciIiIiIiIioqDAIIuIiIiIiIiIiIICgywiIiIiIiIiIgoKXLXwLGRZgdcr93czek2t\nFiFJwX8ddGHY/6GLfR/a2P+hi30f2tj/oYt9H7rY96EtGPtfpRIhisIFHcsg6yy8Xhl2e3t/N6NX\nRFFAVFQ4mpsdkGXW9g817P/Qxb4Pbez/0MW+D23s/9DFvg9d7PvQFqz9bzYbIYqqCzqWUwuJiIiI\niIiIiCgoMMgiIiIiIiIiIqKgwCCLiIiIiIiIiIiCAmtk9ZKiKJBlL5QBPBVVFAW43W5IkjTg5swK\nAiCKKgjChRV5IyIiIiIiIqLQwSDrAimKgtbWJrS1NQMYWOFQd+rrRcjyQF3FQEBYWCTCw00MtIiI\niIiIiIioR/0eZL355ptYvXo16urqkJmZiWXLlmHUqFHd7itJEp5//nl8/PHHqK+vR3x8PG655Rbc\ndddd/n0URcHzzz+Pd955B83NzRg7diyeeOIJpKam9mm7T4ZYkZFWaLU6AAM7gFGrBUjSQAzcFLjd\nLjQ3NwIAIiLM/dweIiIiIiIiIhqo+jXI+uSTT/CXv/wFTzzxBHJzc7FmzRosXrwYn332GaxWa5f9\n//73v+Odd97BU089hfT0dBQUFOCPf/wjoqKiMG/ePADAqlWrsHbtWjz11FNITk7GihUrsHjxYqxf\nvx5arbZP2q0oij/EMhrD++ScF5taLQIYmCOy1GoNAKC5uZGjsoiIiIiIiIioR/0aZP3jH//ALbfc\nggULFgAAnnjiCWzcuBEffPBBwCirkwoKCnDllVciLy8PAJCcnIz3338f+/btw7x586AoCt544w0s\nXboUP/rRjwAAy5cvx9SpU/H111/jmmuuuaB2imJgsOL1egEoHSOxqC/47qUCQIYo9vtAwcvKydfv\n6a9juvyx70Mb+z90se9DG/s/dLHvQxf7PrSFYv/3W2Lgdrvxww8/4L777vNvE0URU6dOxd69e7s9\nZsyYMXj33XdRUlKCtLQ0FBYWYv/+/fjZz34GACgvL0ddXR2mTZvmPyYiIgK5ubnYu3fvBQVZarWI\nqKjAUVdutxt1dSLUarFjpFNwGNhtFSGKIsxmY5+NnKNAFktYfzeB+gn7PrSx/0MX+z60sf9DF/s+\ndLHvQ1so9X+/BVk2mw1erxfR0dEB26OiolBaWtrtMXfffTeam5tx9dVXQ61WQ1EU/P73v8esWbMA\nAHV1dQDQ7TlPPna+JElGc7PjtG0SZFnuqDk1MKfrnU6tFiFJA7etkqRAlmXYbO1Qq9393ZzLiigK\nsFjCYLO1DbhVK+niYt+HNvZ/6GLfhzb2f+hi34cu9n1oC9b+j4w0QKNRXdCxQTWH69NPP8Vnn32G\n5557DkOGDEFhYSGeeuopJCQk4Morr7xoz3v6iyGYXhzBRpYV3t+LhPc2dLHvQxv7P3Sx70Mb+z90\nse9DF/s+tIVS//fbXDOLxQKVSoX6+vqA7Q0NDYiJien2mOXLl+Puu+/Gtddei+HDh2PhwoW49dZb\nsWrVKgDwH3c+56Tec7lcmD59PHbt2tHfTSEiIiIiIiKiy1i/jcjSarXIzs7G1q1bMXv2bACALMvY\ntm0b7rjjjm6PcTqdUKkCh56pVCrIsm/KXHJyMmJiYrB161YMHz4cANDa2oqCggJ/Ha1QNXny2DM+\nvmjREtx11z0XdG6dTocPP/wMkZGmCzqeiIiIiIiIiOhc9OvUwkWLFuF3v/sdsrOzMWrUKKxZswZO\npxPz588HAPz2t79FXFwcHn74YQDArFmz8PLLLyMuLg5DhgzBvn378Oabb/pXOBQEAT//+c/x0ksv\nISUlBcnJyVixYgXi4+P9YVmoWr/+c3+NrE8++QgffPAuVq1a43/cYDB2OUaWZSiK0iU87E5UVPRZ\n9yEiIiIiIiIi6o1+DbLmzp2LxsZGPP/886irq0NmZiZeffVVWK1WAEBVVRVE8dTsx2XLluG5557D\n448/joaGBsTFxWHRokVYsmSJf58lS5bA4XDgscceQ3NzM8aNG4dVq1aF/Ep4UVHR/iDLaDRCFMUu\n4dP27VvxyCO/xl//+iJefHEFSkqO4bXX3oQoinjppRUoLi6Cy+VCRkYGfvnLh5CdnQPAN7Vwzpxp\n+OtfV2LChEkoLS3BbbctxFNPPYu1a/+BI0cOYejQ4fjjHx9HSkrqJb92IiIiIiIiIro89Hux95/9\n7Gc9Tvtbu3ZtwNfh4eFYtmwZli1b1uP5BEHAAw88gAceeKBP23kuVn9chN2H68++Yx8ZOzQad12f\n1efn/dvfXsKDDz6C6OgYWK1WlJaWIC9vFu6779dQq1V4771/4tFHH8Dbb69DREREj+d59dVX8Mtf\nPojo6Bg8/fSf8Mwz/4UXXvhbn7eXiIiIiIiIiEJDvxV7p4Hr3nt/iTFjxmHQoBSEhYUjKysH118/\nD0OGpCMlJQ0PPvgoNBo18vPPXNz99tsXYfz4iUhLG4zbbvs59u3bC6/Xe4mugoiIiIiIiIguN/0+\nIutyctf1WbirvxvRB0aMCBzl1draildffQU7dmxFY2MDZFmGy+VCTU31Gc+Tnp7h//+oqGh4vV40\nNzfBYrFelHYTERERERER0eWNQRZ1odfrA75eseL/oahoP+6//wEkJiZBq9XhgQfug8cjnfE8avWp\nl5cgCADgX2GSiIiIiIiIiOh8MciisyosLMD8+QsxfXoeAMBma0RDw6WrBUZEREREREREBDDIonOQ\nnDwIX3/9JcaPnwSPx4OXX34eGo2mv5tFRERERERERCGGQRad1YMPPoq//OVJLFlyB6KionHnnUvQ\n2NjQ380iIiIioiAmeWUcq2xGUUkjqhvbMTghEllpViTFhEHsKEtBdDlSFAUKwNc50QUSFEVR+rsR\nA5nH44Xd3h6wTZIk1NdXIDo6KaAO1ECmVouQpIFbnyoY72mwEEUBUVHhaGhohSzz2z2UsO9DG/s/\ndLHvQ9tA6v86uwNHK5pweiuaWt0oLrXh0Ak7XJ6uK1pHGjUYkWpBdpoV44bHwKjnTIBzMZD6nnxc\nHi8Ol9tRWdeGOrsTdU0O1NkdqG9yQlGAaJMeMWYDYswn/+v7F23Sw6A79ZmozenBgVI7ikobUVRi\nQ3ObG7PGJOHaySkI02vY9yEuWPvfbDZCo1Fd0LFMDIiIiIiIqM/YW1346LsSbC6ohPcMH6p0WhVy\n06OQmWZFYrQRRyt8o7OOVTZjZ3EtdhbX4n++OIQJI2KRNzoRGUkm/wJCRBdDcUkjthfVYNbYJKTF\nR/a4X3ObGx9vLYFXVvzBU4zZgCiTHjW2dhSV2FBc0ogjFU2QvF2/B8INvnC2urEd1Y3tXR4/uU+M\n2QBFUVBa04LOw09UooBPtpdi454KXDs5BVdNTOndhRMFGQZZRERERER0zirq22BvcSHarEdUpB5q\nlQjAN2rk0+1l+DL/BNySDINOhak5CdBrA//irteqMGyQGYMTIv3HAkDO4Cj8ePpgOFwSDp2wY8/h\neuworsHW/dXYur8aCVFGzMxNRHqSCTFmAyKNmi7BlsvtRX2Tb8RLVKQeSTFhDL/orI5XNeO9TUdR\nVGIDAGz7oQa3Xz0MM0Yldrvvi+8XwtbiOut5ww2+0YVDEiL9o66iTQYY9b6P4Q6X5B+hVWd3dPxz\ndmxz4HhVMwDAEqFDVpoFWalWZKZZoCjAv747jm8LqvDepmP4Mr8ct141HNkpZn9IRnQ549TCs+DU\nwksjGO9psAjWoabUe+z70Mb+D13s+9B2Mfu/qqENH2w+hvyDdf5tggBYI3SINhlworYV7S4JGrWI\nH41LxrWTU3v9odrhkrCzuAabCypxvKol4DGtRvSNgonUo83pQZ3dieY2d8A+kWFaZKVakNkRAkSZ\n9F2eo90pnQoRmnxBQlSkDldPTAkI2gY6fu+fv6qGNry/+Ri+73hNx1kMyM2Ixpf55ZAVBbPGJuGn\nc4b6XwffFlRi7eeHIHllTMmOw/gRsf7gqc7uQEOTE+ZwLbIGW5GVasWguPALroMlKwqaWt3weGXE\nmPTdBrLVje1Y9+0x7Cyu9W8bFBvuC73SrMhIMvm/N062sbHZ2e1IyaQY33Fp8RFQicHzug8mHknG\n7kN1OFBmQ3LH/Y63Gnsdtgfr935vphYyyDoLBlmXRjDe02ARrG9s1Hvs+9DG/g9d7PvQdjH6v7HZ\niX99dxxb9lVDVhTfB/U0KxqafDV/bM0uf+HqmbkJuGHaYFgidH3y3J2V1bRg14FaVDe2+z+UO1yn\namypRAFRHVO8rBE6VDW041hlM+ROH3dUYuAHRkVBwOOdZSSbsHReDszh3V9Lq8ODspoWWCJ0iDbp\noVGf+kAmeWUcr2r2TzGrbGhHzmAr8kYnYtgg8zl/cJUVBccrmxFu0CDWYujxOK8so7yuDckJJmig\n8HsfgNvjxeGKJhSV+OpKVdS14vSuPhnoWCJ0uHFaGqaNTIBaJeJgmQ0vr9uP5nYPMpJNuPuGLHy6\nvQzf7KmAShRwy+wMzBmXPGBG+5XVtmBbUS32HKxFrc3Rq3MZdCqMSPEFYX0VtASzGls7Wh0epMZF\nnDHYrrU7UGdzwBrZ9f2gqqENmwsq8V1hNVodnoDjLBE6f9ieFB2OGLM+oC6goiiosTn8r+OjFU24\nauIgXDsp1b9PsP7cZ5B1ETHIujSC8Z4Gi2B9Y6PeY9+HNvZ/6GLfh7be9n9759EbTQ5U1rdhR1Et\nJK+MML0ac6ekYs7YZGg7ffjwSDIamp3Qa1U9hj4Xg6IoaHNKaGhyItyggSVCB/G0oMrhknDwhB1F\nJY04UGpHq8Pd5TwGnRqxnQptWyN1+GR7KY5XtcAUrsX980ciI8kUcM7Pd53AZzvL4HL7gjQBgDlC\nhxiTHlqNCocrmvyPnXz8ZG/EWY3Iy03E1JHxiDRqu702W4sLWwqr8G1BJeqbnACAqEgdMtOsvg+9\nqRa0OSUUlTSiuNSGA2U2OFxeiAIwbWQCbpw2uNvRZ5ejk6+/U1PzHCiracXh8iZI3lOff8INGqhV\nga8PrUaFK0YnYfbYpIDXNOALcFd+sB/Hq5ohCL7QM9KowX3zcjA8xXJJru1cdf6+r2lsR3GpDUUl\njSipakFkmDagmHxUpB4adWAgI3llHOsIXg+dsMPT6XPjyeA6K82CzFTrRQmpBxqP5EX+wTps2luJ\nQyfsAHw1/UYMMiMrzTe9M9KoxYEymz9gOvl9epKl4/3Aqyg4WuGbIqoSBYzOiMa44TGoqG9DUYkN\nJdXNXQLWML0a0WYDLOE6lNa0BExjVatE/OyqYZiZe2raa7D+3GeQdRExyLo0gvGeBotgfWOj3mPf\nhzb2f+hi34e2C+l/ryxjy74qrN9W2uXDGOCbwnfVhEG4ZmJKyKwg6JG8+J/PD+HbfVVQiQJuu3IY\npo2Mxzd7KvHx1hK0Ojz+D6XtHXWOGpqd/g+kOo2vDtjJKV4xZj3yD9Rhc0EljlQ0AfB9qD21Up0v\naAg3aLD7UB0KjjT4R4oNSzbBLckorW7psgLkSVq1iIxkE07UtqKl3QO1SsCsMcm4bmpqj2FZMHO5\nvdh5wDfl9FhFc7f3JUyv7hhZ5OuDM41o64lH8uLNLw5hc0EVhiRGYum8HFgjB15A2Jfv+x7JiyMd\nCy8Ul9pwvCowaDkZ0HReZTHMoMHpt1anUSHGbIApXHtO0ytlRUHR8UZs2luJH0oaexwp2ZkoCEiN\ni/BNHU6zYnBC4LRIWVbQ2OJEQ5MT1khfm8/0/GU1Ldi6vxrb9lejzSkB8K0sGW814lC5HW5Pz5+n\nU2LDkRQTDnurq2MKp8t/DTFmPWbmJmL6yASYTgv7T65IeeiEPSCMdXd8dhcApCVEIDPVFyZmJJm6\nhK7B+nOfQdZFxCDr0gjGexosgvWNjXqPfR/a2P+hi31/eXF7vMg/WIvjVS2wRugCgo/uQqXO/e/1\nyjh0wo6CIw2IsRiQlWZBrPnUh3lZUZB/oBYfbD6Gmo7pSAlRxoDRSTFmA4YkRV6WYcjZKIqCTXsr\n8eYXh+CVFYTp1WhzShAATMmJx4+nDw74YCx5ZTQ2O9HukpAcE97jNKTyulZsLqjEruJaNLV1HSEG\nABFGDaaNTMDM3ETEW40AfFMZD5TaUFRqw8EyGww6tb8AeHqSCTqtCnqjDv/7aRE27DwBl8cLnVaF\nqdnxiI8ynurTjlFjA5FH8uJweROKS22QvHLA6zAqUo/Ket8Ure1F1f5ppeZwLeKtvuuL7vjeSLCG\nYVBseJcRehequrEd0Sb9gK2ZdjHf99udHhwos/uDrerG9i4jiM5ErRL9qzp2Hhl2crVHp9uLLfsq\nsbmgCg3NviBdqxGhVZ/9NerxygEjHw06FYYmm+H1yqizO9FwWj2wGLPeN6KqY1Sj0+31j6gqLrX5\np/2pRAFjhsUgLzcRmWkWiILgG7VW2ezfv9Xh8YfVI1ItXd4jJa+MxhYXnC4JybHnVytNURQ0t7nR\n2OLyB9xnEqw/9xlkXUQMsi6NYLynwSJY39io99j3oY39H7rY9wNTQ5MTbsl7zvVmyutasWlvJbbt\nr0a7S+p2n3CDJmDUT5zFAJVKhFqnwUebjmDj3krUNAb+HhsVqUdWmgWp8RHYXFCJsppWAEB2mgU3\n5aVjcEJk7y/2MnOkogkrPyhEU6sbY4ZG46aZQ5AUE94n53a4pIAV62wtLgxJjMTYYTHnHZp0/t63\nt7jw8dYSbNxbAcnb9X0g2qTHiFSLf7qYKezsQWVzuxvrt5ai8FgDRqRakJebiNT4iHNqW73dgc37\nKrGzqBaifySa3h9Q1dp9NYAOlzcFTGvrrPP0TLVKxIQRMcgbnYShyaaQruEEXNr3/ZOBbecC8o5u\n3qN8IxV9+5xeF6onI1LMyBudhLHDYrpMf+yOrCgor21FUYlvit+hE3b/SCYAMIVp/dOFqxracaK2\ntcdzqUQB6Ukm5GZEYWpOwjl9TwwUwfpzn0HWRcQg69IIxnsaLIL1jY16j30f2tj/oYt9P3BIXt8K\nVZsLKlFUYgMAmMK1yEo9WW/GAnOEDk2t7oApJT+UNAbUVBk3PAajh0ajpd2DOrsD9R0fDmts7QEh\nhSVCh6ToMBwos/m3pydFYnJWPBqanSgusaGsJnB62uCESCzMG4LMNOsluy/BqN3pQVObGwlRYf3d\nlB51971va3HhSEVTwOvr5Guo8+vg5ApqWWkWDBtkhl576vdxh0vChp1l2LDrRMDoFwBIjY9AXm4i\nJmXFwaAL/B1e8srYe7gemwsq8cPxxh6nRXZm1Kn9AZtRr/a/1k/+M+o1mD4qAVOy43u9IublZKC/\n7zs6pt92Dm1PhlweScbEzFjMzE1EXMfowwvlkWSU1bZAr1Eh2mSAThsYkjS3uf11rQ6U2aHTqPx/\nCBiWbO6yf7AY6P3fEwZZFxGdWNDiAAAgAElEQVSDrEsjGO9psAjWNzbqPfZ9aGP/hy72fe8pioLK\n+jZoNSpYI3XntRR9u1NCja0duw7U4rvCKrS0+0YiWCN1MIfrUFLVElD7Ra0Suh0xE281YuZZCoK7\nPV4cqWjqmBbjK+ysAAgzaDA1Ow4zchORfNrIoZZ2Nw6U2XG8shnpSSaMHRYd8qNZLhfn873vn6bY\nMU2q1n5qpTuVKCA9MRKZaVZo1SI+3VHmrwk2a0wSZo1Nwv7jjdhcUImKujYAvhFSRl3gB1KXdGra\nlyVChxmjEjB9VALC9JqAQKPe7oQpXIvswVakxkX02XTAUML3/dAWrP3PIOsiulyCrOnTx5/x8UWL\nluCuu+7p1XM8/vgfoFKp8dhj/3nexwbjPQ0WwfrGRr3Hvg9t7P/Qxb7vnUMn7Hh301EcKfcV5BYF\nAdZInX8q1Om1hRQFaGpzd3wgd/gLBJ88NjcjCnmjk5Az2ApRFNDulHDwhM0/Faal3dOlbkxSdBiG\nJEaed8DU6vCgprEdY7IT0NrsYP+HmN5879fbHSjqCLaKS23+ABYABAGYmu2rCRbdqSaYoig4WtmM\nzXsrsftQHTzewD+ai4KAzFQLZo5OxMgh1vMKhOn88H0/tAVr//cmyGJiECLWr//cPyLrk08+wgcf\nvItVq9b4HzcYejeMk4iIiGig+66wCp9sL0VafIS/4O/JFcjKalrw3qZjKDzWAMBX9DzcoPFPh6lv\ncqK49Mzn16hFJHQU1c5IMmHayIQuS9Ub9WqMGRqDMUNj+vz6wg0aRA4yQ6dRoedKMERdRZsNmGk2\nYGZuImRFQUVdG4pKGmFrcWHGqIRua4IJgoCMJBMykky487rMfmg1EYUqBlkhIioq2h9kGY1GiKKI\nqKjoLvsdPnwQL764Avv3FyAiIhLTp8/E0qW/htHoqwfw+eef4Y03VqOysgJGoxFZWTlYvvw5vPzy\nC/jqqy869vkUAPDKK/9ATs7IS3SFREREFIoampwoKm2EJMmYNjKhx9XYPttRhn9+cwQAUNXQjm0/\n1ADwBVZRJj32H2sE4FvVat6MIZiUFedfZcrt8frDLMnbtVRDhFHjW2Y+TMtpehT0REHAoNhwDIrt\nm4L2RER9jUFWH3JsXAWpZPclez512lgYrljSZ+ez2+144IGluPnmn+Lhh3+HtrZWPPfc/8PTT/8Z\nTzzxX6iursKf//w4HnjgEUydOh0tLS3YsycfAHDHHXehrKwEKpUKv/nNbwEAJpO5z9pGREREBABt\nzpO1fXzToGpsp2r7fL7rBBbNzcSwQad+B1EUBe9vPob120qhVYtYckMWRFHwH1/V0I6qhnaYwrS4\nYVoaZuYmdlktTqtRITE6DInRA7fQNxERUahgkEV+//zn/2LMmLH4xS8W+7c98sjvsWjRbXj00T+i\nrq4WsiwjL28WoqKiER+fgKFDhwHwjfLSarVQqdTdjvQiIiIiuhBujxeHK5pQ3BE8lVYHrrqXFBOG\nzFQLam0O7DvagKff3I0545OxYGY6NGoR//P5QWzcWwmjTo0HfjIKQ5N9IdfJqX22Fhcq69uQkWQK\n2hWriIiIQgmDrD7Ul6Oj+sORI4exc+c2XHnlDP+2k2sBVFaWIzMzGzk5I3HbbQsxadJUTJo0BVdc\nMQdGI+trERERBTuvLGPDzhPY9kN1l2KxoijAGqEPKEpuidChqdXdaSl1BxqanfCeY6HZcIOmU4Fz\nPaJNBsiy4jtX06ml2ctqWgOm81kjdchKtSIzzYLMVAvM4b4aVIqiYOv+arz15WF8mV+OfUcakBBl\nRMHRBpjCtHjoltHdTpWyROi61LEiIiKigYtBFvk5HO2YM+dKLFp0d5fHYmPjoFarsXLlqygo2IMd\nO7bhjTf+gdde+ztWr17LaYRERERBrLyuFa+tL0ZJdUuP+1TUtfX58x7uWBnwTIw6NXLTo5CVZkFm\nmhVxFkO3dagEQcC0kQnIHmzFG58dxN4j9ai1OxBt0uORW0cj1sI/vBEREV0OGGSR37Bhw7F7dz6S\nkpJ7LFQqiiLGjBmHMWPGYdGixZg7dw7y83dhzpwroVZrIElSt8cRERHRpeF0S6isb4Xd1g5ZOTU6\nSqtWwRSu9RcwBwDJK+OT7aX46LsSeGUFGUkm3HHtCMSY9AHnlLwy6pucqLM7Ud/kG31la3HBFK7z\njdIy+UZWRZn00KoD60t1RwHQ3NZ5NJdv9JVKFBBjNiDarEdsx2gtc4QuoM1nYw7X4VcLRmJncS2K\nShoxb8YQjrgiIiK6jDDIIr+FC2/F+vUf4U9/ehw33/xvCAsLQ2lpCbZt24JHHvkDCgr2oLCwAOPH\nT4LZbEZ+/g54PB6kpKQAABISEvH111+gvPwEwsMjEB4eDrWaLzEiIqLz0erwYPehOoiC4J/Kd6Yw\nR/LKOFbZjKKSRhSV2nC8srnH6X1qlXhqeqDJgMMVdpTVtEKrFvGTWRn40bhkiGLX59FqVEjRa5AS\nF9Fn13lyWuHFIAgCJmXFYVJW3EU5PxEREfUfpgzkl5CQiJdeehWvvPIifvWreyDLXiQmJiEvbzYA\nIDw8Avn5O/HWW2vhcDiRlJSEZcuewNChwwEA8+YtRGFhARYt+jc4HA688so/kJMzsj8viYiIKCgo\nioJDJ+zYtLcS+QfrAmpCAYBaJSDKZIBRF1iMXFaA6sZ2uNxe/7ZIowZpiSZIkhedBmTB4ZJQ3+T0\nr9J30rBkExbNzUSclVPviIiIaOATFEU5t4qcIcrj8cJubw/YJkkS6usrEB2dFDQjjtRqEZIkn33H\nfhKM9zRYiKKAqKhwNDS0dineS5c39n1oY//3H68s43hVC0qqms/p3re7JOworkVNo+/3DYNOhclZ\n8QgzqP1T7ursDrS0e7o9XqdRYXiKGVmpFmSlWTEoLhzR0RE99n27U+qYHuiEShQwKiPqvKbu0cDG\n7/3Qxb4PXez70Bas/W82G6HRXNhqwUwMiIiIBiBFUeCVFahVZ6831BsOlwSdVsUgoxdkWUFVQxuK\nSm0oLrHhQJkNzk4jpM5VRpIJM3MTMWFELHTarr/YOd0S3J6uf5Qy6tUBr5Oe6lx23j9FH9Gn0wSJ\niIiILhUGWURERANIc7sbWwursamgEnU2B0alRyFvdCJGDonqtnbR2UheGfWdimmfXly73SUhKlKH\nGbmJmDEqMeSLYiuKgjandMZRUC6Pt+OeOlDX5ERDkzOgJpVWLSJnsBVDB5mhP4e/NAoCMCLVguSY\n8DPup9eqodee/zURERERXU4YZBEREfUzWVFwoNSGTXsrsftQnT8U0WlV2HukHnuP1MMaqcP0kQmY\nmZsIa6S+x3O53F7sLK7BrgO1qO0IW3oqIqBVi4g1G1Brd2Ddt8fx4ZbjyE2PxszRicgZbL3oo8H6\ny6kV+E4Fe/Ung74mBxyu8xtNZQrTIs5iwLAUC7LTLBiSaILmHFbuIyIiIqLzxyCLiIioH9TbHSgq\ntaGopBHFpTb/yB+jTo2pOfGYOToR8VYj9hyux+a9FfihxIZ/fVeCf31XgkGx4chK89VDGpZshk6r\nQml1CzYXVGLbD9X+aW2CAFjCdYg2G3wr1ZkM/pXiYsx6RIZpIQgCahrbsbmgEt8VVvmDM1EQYI3U\nBex/6v8NCNOrzzqFbaCpqGvFpoJKbNtfjTan1O0+Oq0KyTHh/us1hWmB0y5TrRI77qUe0SZDt9MA\niYiIiOjiYJB1AU793h48hdQGPt+9DLLPRERE56zV4UFxqQ3FJY0oKrGh1u7wP6YSBYxIMWP6qASM\nHx4LbafpaBNGxGLCiFjU2trx7b4q7CiqwYnaVpyobcWGnSegEgVEmfSotfnOp1aJmJIdh7zRSZgw\nMhEtzY6zFv6Msxrxk1kZmD9zCPYerseWwiqcqG1FfZMT9U1OFJfauhxj0KkRY9IjJT4CM0YlICPJ\n1O/BlleWIUlKl217DtdjU0EljpQ3AfDd72GDzIi1dA3pIgyafr8OIiIiIuoZg6wLIIoqiKIKdns9\nIiLMUKnU6PLn2gFH6PLL/cCgwOuV0NJi999XIqLLgdvjxeHyJhR1BFdlNS0Bf/5Ijuk0qmqQCXrt\nmX8kx1qMWJCXjgV56ai1O/yBWHGpDbU2B5JiwjAzNxFTsuMRbtBAFIWAQOxcqFUixo+IxfgRsf5r\naGh2BtTU6lxjq6y2FWW1rdiyrwqJ0WHIy03ElBzf818qiqLgWFUzNu2txK7iWrg8PU8LjLUYkJeb\niKkjE3wjrYiIiIgo6DDIugCCICAqKgHNzY2w2Wr7uznnRBRFyHLXlY4GCp3OCIslln8FJ6KgJcsK\nSqpbOoKrRhypaIbkPfW+GxWpQ1aaFVlpVmSmWhDZiyAl1mxA7Ogk5I1OgqwoaG5zw9QxTbAvaTUq\nJESFISEqrMtjiqKgud2DgiP12LS3AserWvDWV4fxzsajGDM0GjlDrMhKtSLK1HM9r95od3qw7Yca\nbNpbifK61o72ikiM7trWQbHhmJmbiOEpZq7OSERERBTkGGRdIJVKBYslBooiQ5blHgvpDgSiKMBi\nMcJmaz/r9JJLTRB8IZsgsCguEQUnyStjy74qfLS1BLYWl397mF6N0RlRvuAqzYJYs+GihPWiIMAc\nfulXGhQEAaYwLWbmJmJmbiLKalqwqaAS23+oxq4Dtdh1wPeHnjiLAZlpVuQMtmJUelSvC8i3OT34\ndHsZvvz+BNweX1CYEheOvNFJmJwVB4OOv9oQERERXc74214vCYII1QBf1UkUBWi1WqjV7gEXZBER\nBStZUbCzuAbrvj3ur0+VmWpB9mArstIsSImNgCiGzuiflLgI3H7VcNx8RQYOlNlQVGJDUWkjKura\nUGOrwMY9FYgwajCtY+XFeKvxvM7v8njxZf4JfLq9DO0uCRq1iLzRicgbnYi0+MiLdFVERERENNAw\nyCIiIuqBoig4UtGEplZ3wHan24sv8k/gRK1vSlv2YCsW5A1hoALfqn+5GdHIzYgGANhbXSgutSH/\nQC0KjjTgsx1l+GxHGUakmDE5Ox7GcxhB1djsxKc7y9DU6oYoCMgbnYgbpw2GJeLSj0QjIiIiov7F\nIIuIiKgbbo8XazccxHf7q3vcZ0hiJBbkpSMz1XIJWxZczOE6TMmOx5TseNhaXNhSWIXNeytxoMyO\nA2X28zrXxMxYzJsx5LxHcxERERHR5YNBFhER0WnqmxxY+f5+lNa0wBKhw4SOVfxOEgRg2CAzRmdE\nc5GK82CJ0OGGqWm4bkqqfzXHc5nyrlIJmDgiDqnxEZeglUREREQ0kDHIIiIi6qS4pBEvf/gDWh0e\nDEs24b75I2HqxQqD1JUoCMgZHIWcwVH93RQiIiIiCjIMsoiIKKR4JBn1TQ7YWlyQT1ty9nhVC9Z9\newyKAvxoXDJunp3R61X2iIiIiIio7zDIIiKioKcoCqob21FUYkN5XStOy6fgkWQ0NDtRZ3fA3uLC\nmSazadQifnHNCEzJib+obSYiIiIiovPHIIuIiLrw2irgKdwARfL06Xk16ZOgTh3dJ+dqdXhQeLTB\nV2up1AZbi+usxwgCYI3UI8asR1SkHqrTRlupVQJm5iYiJY61mIiIiIiIBiIGWUREFMDbWAHHx09B\ncbb0+bmlo9uhn3UPNBmTL+h4RVFw6IQdm/ZWIv9gHSSvDMAXUA1OiERWmgXpiSao1YEF2FWCgCiT\nHtZIPacKEhEREREFMQZZRETkJ9ur4Fj/NBRnCzRZs6G+wMCp23M3VsD13Vo4v/k7IKqgGTLhnI9t\ndXiwZV8VNhVUoqaxHQBg0KkwNScRo9KjMCLFDKNe02dtJSIiIiKigYlBFhHRZUCRXIDshaA1XvA5\n5KYatH/8NBRHMzRZs6GbdjsEQTj7gecqfhgErR7Ob/4O51ev+MKstLFnPexIeRNWritEU6sbAJCe\nFIm83CRMGBELnVbVd+0jIiIiIqIBj0EWEVGQU9ztaPvgSSjNtVAlZfnqUKWNBQzh53wOubnOF2K1\n26EZPhO6aT/r2xCrgyZjCiB74dy4Gs4vV0K46tdQp+R2u6+iKNi4pwL/++VheGUFk7PjMHdyKpJj\nzv26iIiIiIjo8sIgi4goiCmKAufm16E0VQOiCt7y/fCW7we+XQP1oBzoJ14DRGWe8RxyawPa1z8N\npa0R6qHToJv5CwjCxasjpRk2HYrshWvzP+D44gUYrn4Q6uScgH08khdrNxzClsIqqEQBP79mOK4Y\nnXTR2kRERERERMGBQRYRURDzHNwM6dhOCOFRMN70H5DryyAd2wHP8e8hle5FTele6KfeBk3Old0e\nL7fZfCOxWuqhTp8Mfd5dFzXE8kufjoryRiQd+xD2T57HZ1E/hyE6ATFmA8zhWqz79jhKqltgDtdi\n6fyRyEgyXfw2ERERERHRgMcgi4goSHltlXB99yYgiDDMvheiPgJicjbUydnQTfs5vKW74dz4Kpxb\n34QiiNBmzwk4Xm63w/Hx01Caa6EePB76WUsgiBc3xCqvbcWmgkps21+NdpcJ1xtycKVhP3Jr1mHF\n4Wsg49TzD002Yem8HJjCdRe1TUREREREFDwYZBERBSFFcsP51UuA1w3thAVQxQ8NeFxQqaHNmART\nTBSq//kUXN+tBVRqaEfkAQBkRzMc65dDbqqGOnUM9HPuhSBevMLp5XWt+J/PD+HQCTsAQK0SMDEz\nFpkj74Cy92Wk1R/DH3LKUGi6AnV2J+KjjLh2UgrUqkswOoyIiIiIiIIGgywioiDk2v5/kBvLoUrK\ngjb3uh73Mw4ZDeNVv0L7hhVwbX4dgqiCOmU0HOufgWyrhDM6Ewdj50MprMXk7Dho1H0bZkleGZ9u\nL8W/viuBV1YQZzUiLzcRU0fGI9KoBQDIMfeh7b3HEFu5GdfnToB60Mg+bQMREREREV0+GGQREQUJ\nryxj35EGlH//La5o+RpeTTgMV/Q8HdAteXG03I7DjgRIKTdjRMnbcGx8FTYlElahGQc8CVh1aCyk\nQ0cAABv3VuD++SNhjdT3SXvLalrw2ifFKKtphUYtYuEV6bhy/CCIYuBqiGJEDPQz74Tzy5VwblwF\n44InIRrNfdKGviC32yHoIy/6tEsiIiIiIjo7BllERANcvd2Bzfsq8e2+KkQ6KnF/xBeACKxqnIT6\ntcWYmZuI6SMTEGHUorSmBUUljSgqseFweRMkr9xxFg1yNTNwR/hmWIVmHJMT8HXEjRifEolokwEH\nSm04UtGEJ17fhaXzcjA8xXLB7ZW8Mj7eWoL120rhlRUMTTbhzrmZiLMaezxGM2QCvJlXwFO8Ec5v\nVsEw9+FLU3T+DBTJBdeOf8Lzw1dQp42D/spfQhCEsx9IREREREQXDYMsIqIByuGSsHp9MfYcqoMC\nIFnViF+av4Je8aBm0I/gbchEXXkT3tt0DOu+PQ6dRoV2l+Q/3qhXI3doNCzhWkSbDIgxj4TTmYWI\n5mMYNfEm5GpOFVGXvDL+76vD+Hp3BZ55ay9umZ2BH41PPu/gprS6BavXF6O8rhVajYibZ2dgzrhk\niOdwHt2Uf4O3+jC8FT/AXfApdKN7njJ5sXlrjsCxcRWUphoAgFTyPTw/fAltD6s/EhERERHRpcEg\ni4hoAHJ7vHjhvX04UGaHOVyLa4erManiPQhuF7S5c5E+8Sf4vSCgsr4NmwsqsXV/NZxuCZmpFmSl\nWZCVZsXghEjExESgoaEVsqx0nDkGwKQuz6dWifjZVcORFh+JNzYcxFtfHcbhiiYMTTIF7CeKAgYn\nRCItPiJgiqBHkvHR1uP4ZFsZZEXB8EFmLJo7ArGWnkdhnU5Qa6GfsxTtH/wH3N9/CG3OlRDU2gu5\nfRdM8Xrg/n4d3AWfAIoCVUoutFmz4fj8ebi2vw1V/DCoolMvaZuIiIiIiOgUBllERAOM5JXx0rr9\nOFBmR2pcBB65Ng7KF/8PirsNmpFXQzvxJ/6RUonRYbh1zlDcPCsDsqIErPJ3ei2qczF9VAKSYsKw\n8oNC5B+oRf6B2m73M+rUGJ5iRlaaFTFmPd755igq6tug06jwk1npuGJM0jmNwjqdypoE9aBRkEq+\nh7fmCNRJWed9jgvlbSiD85tVkBtPABo99FP+DerhMyAIAnSTboZr21twfPUywm76DwiavqkjRkRE\nRERE54dBFhHRACLLCl79uAj7jjYgIcqI31wbD+XL/4biaIYmaw50k2/tdrqfKAoQ0Tf1mwYnROLx\nX0xA/oFaeCQ54DGnx4vDJ+w4VN6EPYfrsedwvf+xzFQLfnHtCMSYDb16flVSli/IKt9/SYIsRfbC\nXfAJ3N+vA2QvVAkjoL/iLogRMf59NDlXQaoogresAM7v/geGKxZf9HYREREREVFXDLKIiAYIRVHw\nxoYD2FVcg9GmJtyRUQZhw1oorjZoRuRBN+22S1ZsPMKoxayxyT0+7pG8OFLehKJSG8pqWjFmWDTy\nchP7pH3qpGy4AEgVRdCdde/eke3VcGxcBbn2KKDSQDf1Vmiy53QpNC8IAvRXLEb7u/8O6dAWeJKy\noBk69SK3joiIiIiITscgi4hoAFAUBZ98+h0sh7fiCUspTEIbcBiAIPhGYk27rd9X8etMo1YhM82K\nzDRrn59bMMVBCLNCri+F4myFoA/vdj9FckNxt0M0ms/7ORRZgqfoG7h2vAN43RBjhsAwawlEc0KP\nx4j6COhn3wPHx8vh3PIGVDGDz7g/AMhtNghqLQRd2Hm3caCRWxsgtzZ02S5GxEAMu/BVLomIiIiI\nzgeDLCKifiYrCr781wZMr34bokGBAgGqhOFQD5kI9eDxEI2ms5/kMiIIAlRJ2ZAOfQupshiaIRO6\n3c+58VVIx3ZCk3MVdBMXnrUwvCLL8FYdgHR0BzzH8wFXGyCqoB1/E7Sjr4Mgqs7aNnViJrRjb4B7\n97/Q9s4yqJKzoUmfCHXaWAhaX2F7uc0G6dhOeI7u9I30Umuhm3wrNJmzLtmIur4it9kgHd0Jz7Ed\nkGuPdb+TqIJ2/HxoR82FIA6csJWIiIiILk8MsoiI+pHklfG//9qFH9WsgygqcIyYi+hxV4b8CBd1\nsi/I8lb80G2QJbc3QTqeDwDw7P8c3vJC6K9YAlXskID9FEWGt+YIpCM7IB3fBcXR7HtAVEGVMhq6\n8fPPexVC7dgfQ5E8kA5tgffEPnhP7ANENdSDRkJxt8NbdQiAb5VIITwKSpsdri1vQCrZDf3MOyGG\n9/0otr7ku7e7IB3dCW/1YfivJTK24/52CuNkL6TSPXDvfBdS6V4YrlgM0RQfcD5FluCtKIK37vjJ\nU/kJai1UKaOgsiT1ut2KV4J0dAdUcRkQTXG9Ph8RERERDUwMsoiI+onL48XLHxRiet06RGic8AyZ\njtiZN/d3swYEVWImAF+drO5IR7cDigx1xhQorlZ4TxSi/cM/QTvmemjH3Ai5oQyeozsgHdsFpa3R\nd5AgQpWcA036JN8Iqguc7ieIKugn3wJl4kJ4K4s7Rnh9D6l0j+/xMCvU6ROhGTIRYsxgyPWlcG78\nO7zl+9H27v8H/bTboc6YMqBGZynOVnhKvod0dAe8lcWAciqIUw+ZAE36ZIjRqd222WurhHPjKsg1\nR9D23mPQTboZmsxZ8FYdhHR0J6Tj+VBcrT0/+Y63IVqSffcsfdIFhVDexnLfipMNpRAiYhD2kz+f\ndYQeEREREQUnQVEU5ey7hS6Pxwu7vb2/m9EroiggKiocDQ2tkGV2d6hh/w9M7U4PVry7D6n1W3CD\ncQ8UUyIiFvxHn374Dva+b3v33yE3nkDYrc9AjIwJfOy9xyE3lMJ40xMQo1LgObAJrm1vAZILUOt8\n/wWAk9M00zumaRoiL0pbFa8Eb9UBCGodxLj0LvXMFMkNV/778OzbAECBGJUKQau/KG05RYBGo4LH\n40WXoVCd2+aVINeVAIrXd5TBdCqI6+Zauj2HLMG9dz3c3//Ld57OfSCqoErOgXrQSAiqwNe33G6D\ndGwX5MZy/zbRmtw1ZBREqBJGQJM+MaAumSLLcO/7DO789wFZ8j+vdsJC6MZc331bFQXu3R/6ArvT\niOZE6CYs6LkumyLDU/QNvNWHoErKgiZtXLf7yk018BzdAW/VAUD2dnuuLtcXmw51+iTf9Z8WGCoe\nJ6TSvb5Q0Nly9vP10PeCPgLqweOhTh0NQRP4+lMUBXJjOaSjOyC3NkA76pozjlaUKovhLvi00/ca\n9QuNHroxN0AVl+HfdPp7v7fmCNyFG6DJmg11xx8J6PIU7D/36cKx70NbsPa/2WyERnP20h7dYZB1\nFgyyKNix//uP5JVRcKQeuw/Vw+OVAx47UdsKfVMpHjR9BlGlhnH+f0Bl7f30qs6Cve+d296Cp3AD\ndDMXQTsiz7/d21iO9neXQbQkw7jwP/0f+uXmWjg3rYa36iDEuAzfyKvB4wfUNE2p6iCcG1+F0lLX\n300J4As4xkGdPgmq+OEXXOvKW18C5zevQrZXQpWY2dEH4846+s1rq4R0dIcvRGmqPuO+YlQK1OkT\noYobCtfOdyDXHAFUaugmLIA6ZQza3l0GiCLCbn6q22mc7v1fwLX1zR7PLxhM0OfdCXVKbsB2uaUO\nzo2rfeGUvzEq/yg/MSYN3tICeI7ugFxfcsZrOOP1mROh7rhvclO1776UFgBe9wWfswuVFurUXF9w\nZoqHdDwf0tGdkO2VnRqignbsj7vUj1MkN1w734Fn/xd91x7qHUGANvc6aMfNg6BS+9/762vtcO76\nAO6C9b5RlioNDNf8BuqkrP5uMV0kwf5zny4c+z60BWv/M8i6iBhkUbBj/196tbZ2bC6owpbCKjS3\ndf/h0yC48UfrJ4hUmqGb8QtoM6/o83YEe99LZfvg+OxZqIdMhOFHS/3bXTv+CXfBJ9BNuhna3Lld\njlMk94CeVqbI3lPTHS8iURBgtoTBbmuDfJYf9UKYBYLYN9UGFEUBvJ4L6gNFUaC0232jqzpvdzsg\nlezxBV2dwxYAYnQa9Ftif+cAACAASURBVLOW+OtsuXa+A/fe9VBnTIZh9r0B+3rrS9G+7j8BKDBc\n+3DgSD/ZC9ee9ZAOfQsA0IyYCd3knwIaPTwHN/tG/HmcEK3J0I6+Ht7KA/Ac3+VbNOA0oineF0al\njYWgM579uj1uSGV7fWFSQ2nXHTR6qFPH+AKzcwi8e+p7ubHCN+W2dA/g+f/Zu/P4Kus77/+vazlL\nFkJCCGsIhB0CYQdBkFVFcanS0em0trXVdmzvu73vdtT+7pl7Wn7O/Jyl006t0+lUq2M7bUcrYisg\nVkUUkUXZwr6FJewhJIQsJ+dc57p+fwSikS0JSa6cnPfz8chDOddy3idfcuB8+H4/38il12X3xx40\nGcMwqfvoFYjHMHsMJGVW/Y6e8dPFRN75RX2xMZRGeNrnsXoNuWYeaTvOsZ2f+L3Zj/DsrxHIySPd\nOcOJJT/BLTtS//snfyLO3vfBDpJy23exew/zO7q0gUT/c19aTmOf3BJ1/FXIakMqZEmi0/i3nbjr\ncrayjtKKWkorajlzLsKBY+fYfaQCAMOAMYO6c+Po3mR1CX18oRun65ZfETy+GXvgJMJzv9Em/ZIS\nfey9WB1VL3wDI5BC2hefwjBMPNel+rffwas5R9rnf9ShZlt1NIk+/pfjeR5u+dH6RvQn9mDljiI4\n9vZGRTgvFqH6xe/h1VSQctf/we419OPHX/kB3rmThKZ+juDoWy/7HM6hzURWP49XW4nRpTtm117E\nj27/xKyXuzGsQP09LzSyjx3YgFt+DLvvSOyBkzGz81r8M+1WnCRWvB7nSBFmenb9sth+hc0qDF5r\n7D0nilNSVF84qyrDziskMHAKZubHjfrdihPUrnqmfrdKK4idPx7nwAbwXKx+o+s3LtDPX4fQaLag\naREYNJnYgQ/BdbB6DyM86yHMLjlEi16nbt2LEAiTevtfNVqOKJ1DZ3zfl6bR2Ce3RB1/FbLakApZ\nkug0/q0v7rosfreYtz4qwYlf+j3Nzggxo7AP0wt70y2jvg+N57rET+6pXyZ0cCNe5DxGl+6k3buo\nxU3Hr6UzjH3Na08SP7GH1HsXYXXvj3N0O7XLf4jVt4DUBY/6Ha9D6wzj31KxfR8QeecXmNl5pN7z\nAwzTpHbVszh738fKG0PKrf/rqoUmN3KeutUvNOyMaXTtScqshxPmg39rjb3nxoluXU5046v1vb7s\nEKGpnyMwfGaH2qxALvRv2/4WdRt+3zAjMjT5s9gF8xr1uavbvJTohy9DIIXUBY9estOrJLZkft9P\ndhr75Jao4389hSzfdy38zW9+wy9/+UtKS0sZMWIEf/M3f0NhYeFlz33ggQfYsGHDJY/PnDmTX/zi\nFwB873vfY8mSJY2OT58+nV/+8petH15Ekk5lTZSfv7qd3UcqCAUtBvROJ6drCjmZYXIyU+jZLZWB\nvTMwzQt9m2oqiG5ZhnNgA17tufqbGBZWv0JCU+5vsyJWZ2H1LSB+Yg/xYzuwuvcntncNAIGhN/qc\nTDoye/BUzJ0rcU/tJ7b7XYxACGfv+xipmYRnPXTNIowZ7kJ43jdxDn6Ee+4kwVG3YARCV72mMzJM\ni9C4O7HzxuAcWE9g+EzMjB5+x5LLMAyT4OhbsPqNIr5vDTmTb+G8kXnJB5rQuDvAdYhufJWa5T8k\nfNOD2HljOvRybBERkU/ztZC1fPlynnzySRYtWsSYMWN44YUXeOihh1ixYgXdul3aoPWnP/0psVis\n4dcVFRXcfffdzJ8/v9F5s2fP5oknnmj4dTCoP5xF5PodPFHJvy3ZxtnKOvJ6pPPNe0eTk5lyxfNj\nBzYQef+F+h46hoHVt6B+N7gr7HQml7L7jiT60Ss4x3YSGDkH59BGsEPYAyb4HU06MMMwCN/4BWpe\nWUTdhy9f2DnQIDzn65jhLk2+R2DgpLYNmiCs7Dys7Dy/Y0gTWJl9CEy5j2B2OpRVXfac4Pi7Ie4Q\n3bKUyFv/BoEU7AHjCQyajJVb0Gr98kRERNqKr39SPf/889x///0sXLgQgEWLFrFq1SqWLFnCV7/6\n1UvOz8zMbPTrZcuWEQ6HLylkBYNBcnIab9UuInI93i86wa/e2IMTd5la0JMvzh9O6ApTYb1IFZE1\nv8Y5sB6AwMi5BMffhZnatT0jdwpmTj4EU4if2Ets/zpwothDbkzK2THSPFb3AQSGzyS2exVQ/+Hd\n7jPC31AiHYBhGAQnLcTsMRBn3wc4R7bi7FuDs28NhNIIFswjOO5ODEsFLRER6Zh8+xMqGo2yY8cO\nHnnkkYbHTNNk2rRpbNmypUn3WLx4MQsWLCA1tfGOQGvXrmXq1KlkZGQwbdo0vv3tb19SBGuOi0uE\nEtXF/In+OqRlknH8T52tIT01QFo4cF33cV2PogNlvLP5KFv3l2EaBp+/eSjzJuZecWlS7MhWalc9\nh1dTgZHejdRZD2HnFlxXjpbqFGNv2th9RuAc2kT0w8UAhIbdmNivqZ10ivG/TuEpC3GObMHK6kt4\n4t0YSfK90Ngnt6aNv4E1cAKhgRPworXEDm2u39GyZBvRTX/AObKF1Dlfw+qW2z6hpVXoZz95aeyT\nWzKOv2+FrPLycuLxON27d2/0eHZ2NocPX2br6U8pKipi7969/P3f/32jx2fMmMHNN99Mbm4uJSUl\n/OhHP+LrX/86v/vd7zBN8wp3uzLbNsnO7hxLgLKy1IsnmXX28Y/UOby/9Rgr1h1mz+FyumWE+N4X\nJzMi/9JlytdyuryGtzYc4c31hzlzrn6L+m4ZYf7qCxMYPaj7Fa8rX7OYmlW/BSC9cBbdb/4KZtj/\n73uij709bDxlhzbhRc5jdcmmx+iJGGbLGkMmo0Qf/+uTTvdv1/fQTMbfM8k99tL08U+H3rfA1FuI\nlZ+k9LWniZTsomrx9+k283N0nXJnUv78JDL97CcvjX1yS6bxT9g5wy+//DJDhw69pDH8ggULGv5/\n2LBhDBs2jHnz5vHRRx8xefLkZj+P47hUVtZed14/maZBVlYa5eXVCbWLgbSOzj7+JafP886mY6zd\ncZLaujgAmelBzlbW8f/87H0+f/NQZo/ve83mzk7cZev+Mt7dcoxtB8q4+J0ald+NmeP6Mm5Id2zL\npOwKPUfqNi8jsv6l+m3N53wNK38C5dUeVF/+/PbQWcY+nvXxTnH2oBs4W57Y78ntpbOMvzSfxj65\nXd/4pxO87THY9gaRDYs5u/LXnNu5jlDhfGjBPwgDmBk9NLOrnehnP3lp7JNboo5/RkZK4u1amJWV\nhWVZnDlzptHjZWVl1+xvVVNTw7Jly/jWt751zefp168fWVlZHD58uEWFLCChfjNcjet6nea1SPN1\ntvF3PY9law/z6nvFeIBtmUwt6MlNY/owtF8m7249zm/+tJdfvbGH4hOVPHDLUAL2pW+UpRW1rC46\nzuqiE5yrigLQNT3IjMLezCjs06iZ+5W+f9Ftb1C3/iWwg6Tc9h2sXkM71Pc64ce+S0+MtG541Wex\nBk9L7Nfig4Qff2kxjX1ya/n4GwRGz8fsO5rIql8QP7mPmpP7ritL+KavEBh+03XdQ5pOP/vJS2Of\n3JJp/H0rZAWDQQoKCvjggw+YM2cOAK7rsnbtWr70pS9d9doVK1YQjUa56667rvk8J0+epKKigh49\ntF20SGdRW+fw7NKdbN53hlDA4u7p+Uwv7E16ysc9sWaN7UtuTjo/W7KN94tOcKy0ijun5VN+PkJp\nRYTSilpOV9Ry9HQVHmAAhYOymTmmD4WDs7Ga+C/P0R1vUbf2d2AFSZn/v7F7DW2bF53EDMMgPPcv\n8c6fwerW1+84IiJJwerWl9TP/F9iO1fhnjvZspu4DrE9q4m89zxYNoEh01o3pIiIJCVflxY++OCD\nPP744xQUFFBYWMgLL7xAJBLhnnvuAeCxxx6jZ8+efPe732103csvv8y8efPIyspq9Hh1dTVPP/00\nt956K927d6ekpIR//ud/Jj8/n6lTp7bb6xKRtnOirJqnX9nGibIaemSl8D/uHU1uzuX72A3u25Xv\nf3kSP3t1O/uOnuOpxUWXnJPVJdQw+yq7a7hZWaK7VlG35r/Askm59VvaEa0N2b2GgoqEIiLtyjBt\ngqPmXdc9rNzRRN76NyKrngHTIjBoSiulExGRZOVrIev222/n7NmzPPXUU5SWljJixAieffZZunWr\nb8584sSJSxq0FxcXs3HjRp577rlL7mdZFnv37uXVV1/l/Pnz9OjRg+nTp/Ptb3+bYDDYLq9JRNrO\n5r2lPLN0J5FonMJB2XztzpGkXmNnwq7pIR793DhWrD/C6fJacrJSyMkMk5OZQk7XFLqkBq7ZP+ty\nYgc3Urf6BTAtUm7+n9i5o1r6skRERDqtQP4EmPOXRFb+O5GV/1FfzMqf6HcsERFJYIbnecmxiLKF\nYrE4FRU1fse4LqZpkJ2dTllZVdKsmZWPdZbx315cxo9e2grAXTcO4K7p+ZgtKEC1Bs/zqHn5b3DL\njxGe900CAyf5kuNaOsvYS8to/JOXxj65ddTxj+37gMg7z4BhknLL/8DuP87vSJ1ORx17aXsa++SW\nqOOfmZna4mbvLdt+RESkHdVF4/zqjT0APHzHSD4zY6BvRSyA+Ik9uOXHMHPyO2wRS0REpCMJDJlG\neOZXwItT++a/4Ry5dLm/iIhIU6iQJSId3h/WHOTMuQjjh+YwdVQvv+MQ2/k2AMGCuT4nERERSRyB\nYTMIzfgyuA61bz6Fc3SH35FERCQBqZAlIh3akVPn+dOGEsJBi8/f7H+zb7e6HOfgJoxQOvbAyX7H\nERERSSjBEbMI3fgFiDvUvvETnOO7/I4kIiIJRoUsEemwXNfjP1/fjet5LJw5iKwuIb8jEdu1Crw4\ngeE3YdjaREJERKS5ggXzCN3wOYhHqV3xrzgn9/kdSUREEogKWSLSYb296SiHTp5nYJ8MZo/r63cc\nvLhTX8jCIDBitt9xREREElaw8FaCk+8Dp47a1/+F+OkDfkcSEZEEYfsdQETkcs5WRnjlvWIs0+BL\n84djmv41d7/IObQRr/YcVt4YzIwcv+OIiIgktNDY28F1iH70CjWv/SNGWmaj44YdJFh4G/aQaRg+\nbvIiIiIdiwpZItLheJ7Hb97cS100zm035NGvR7rfkQCI7bjY5H2ez0lEREQ6h9D4uwCIbvoDXuXp\nRsc8ILLqGexDGwnN+DJmSoYPCUVEpKNRIUtEfFVVG+PD3ac5dbaG0opaSisinDlXSyQaJyczzF03\n5vsdEYD42RLiJ/diZPTEyi3wO46IiEinERp/F8Gxd1BfuvpY/NR+IquexTm0ifjJfYRmfIlA/kR/\nQoqISIehQpaI+CISdfjThyW8seEItXXxRsfCQYsBvbrwhVuGEQpYPiVsrGE21sg5GIbaC4qIiLQm\nw7z0z1a79zDSPvsEdeteJLbrHSJvPo0zeCp235Gfvhg7bwxGuGPM4BYRkbalQpaItKuY4/LulmMs\n/eAQlTUxLNNg1ri+DM/LpHvXFHIyw6SnBDpULwwvWkNs31qwggSGTfc7joiISNIwAmHCM76EPWA8\nkfeew9m/Fmf/2kvOs/qOJOX2RzvU3x9ERKRtqJAlIu1m39EKfvHHnZRVRjCAqQW9+MyMfHIyU/yO\ndlWxvWvAqSMw/CaMUJrfcURERJKO3W80aZ/9O2J7VuPFIo2OxfatJX5sJ86hjVp6KCKSBFTIEpF2\nUX6+jqdf2cb5mhhjB3fn3psGkttBmrhfjed5DcsKAyPn+pxGREQkeRmhNIKF8y953Oo5mNrlP6Ru\n7e+w+xVi2EEf0omISHtRoxcRaXNx1+U//riD8zUxZo/ry7c+W5gQRSyA+LGduOdOYvYcjNW9v99x\nRERE5FPs3FHYA8bjVZURLXrd7zgiItLGVMgSkTb36uqD7C2pIK9nOn8+d7DfcZoltvNCk/cCzcYS\nERHpqEI3/DlYNtHNy3CryvyOIyIibUiFLBFpU0UHyli29jApIYtvfGYUAbtj7ELYFG5VGc7hzRgp\nGdjquSEiItJhmRk9CBbeBvEodete9DuOiIi0IRWyRKTNnK2M8OzSnQA8eNsIemSl+pyoeWI73wHP\nIzB8JoYV8DuOiIiIXEVw7B0Yad1wijfgHN/ldxwREWkjKmSJSJtw4i4//8MOqmpjzJuYy8ThPfyO\n1CxePEZs97tgGARGzPI7joiIiFyDEQgRuuF+AOo++A2eG/c5kYiItAUVskSkTSxZXcz+Y+fI753B\nfbMTqy8WgFP8IV7kPHb/8Zjp2X7HERERkSawB07G6j0M9+xRYrve8TuOiIi0ARWyRKTV7S2pYMW6\nI6SEbB65uwDbSry3muiO+ibvATV5FxERSRiGYRCa9nkwDOo+WoIbOe93JBERaWWJ9+lSRDq02jqH\nZ5fuxAM+f/MQumem+B2p2eJnDuGePoCZ2Rurzwi/44iIiEgzWNl5BEbMhrpqoh++4nccERFpZSpk\niUireumd/Zw5F2HC0BymFvTyO06LxC7Oxho5F8MwfE4jIiIizRWaeC+E0ojtWkX8zGG/44iISCtS\nIUtEWk3RgTO8u+U4GakBHpg/LCGLQF6kitj+dRAIExh6o99xREREpAWMcDqhSQsBr77xu+f5HUlE\nRFqJClki0iqqamM8v3w3AF+6bTgZqUGfE7VMbO9qiMcIDJmGEUy8ZZEiIiJSLzB8Fma3fsRP7sU5\nsN7vOCIi0kpUyBKR6+Z5Hr96Yw/nqqNML+zNuCE5fkdqsdieNQAERs7xOYmIiIhcD8M06xu/A3Xr\nX8SLRXxOJCIircH2O4CIJL51O0/x0e7TZGeE+dzcIX7HaTH3/Bnc8qP1Td675fodR0RERK6T3Wc4\n9sDJOMUbiG5eSmjyZ9v1+d3qcpziDcSP78Zz440PGiZ2n2HYAydjpme3ay4RkUSmQpaIXJddh8sb\nlhR+dcEIUkKJ+7biHNkKgJU3xuckIiIi0lpCN9yPc3gL0aIVBIbfhJnRo02fz62txDn4Ec6B9cRP\n7AWu3J8rfmQLdetexOo5BHvQFOyBEzFTM9s0n4hIokvcT5wi4rsDx8/x1OIinLjL5+YNYXj/LL8j\nXZeLhSxbhSwREZFOw0zPJjjuDqIfvUL1i98Ds427q8TjXCxeGWndsAdNxh4wASOU2vi8uhqcw5uJ\nHVhP/NQ+4qf2UffBf4HV0o9oBpUNz9z85vZGMBW7/3jsQZOxeg/H+MT3yYvWfpz1+G7w4le5U+sw\nM3piD5xMYNBkzMzejY65laXEijfgHNiAW3GszbN0fFcYe9PG7jOivkjafyxGIOxPPJFWpkKWiLTI\n0dNV/OtLW6mLxrnnpoHcPLGf35Gui+fUET++CwIpWL0Sd3mkiIiIXCpYOJ/48V245e1Q9LBD2Hlj\nsAdNweo5CMO4cuHM6jWE4OQ/wz19gNiBDThHtkCLe3kZmKaB63q0pJDlRaqJ7V5FbPcqjJQM7IGT\nsLoPwDm8BadkK8SdhtdnBFOvfrPr5Xm45ceIblxCdOMSzOw87EGTMcwAseL1uKeLG041QultX5zs\n8C4/9l4sgnN4M87hzWAFsfuPwR44CSO1af/4bGX2xgint1FmkZZTIUtEmu3U2Rp++OIWqiMOt03J\n446p/f2OdN3ix3dBPIadNwbD1FujiIhIZ2LYQVLveNzvGJdlGAZWz8FYPQfDtL9o8X1M0yA7O52y\nsqoLBY3m8SJVxA5txDmwgfjxncR2vE3s4sFACvagKQQGTcHqO7Jd/q7kVpyoL+4dWI9bdoRo2ZGG\nY0ZGDwKDpmAPmoyZlYthGG2epyO70th7cYf40e3EDqyvL2gVf4hT/GHTb2xYWLkF9d/rAePavoAp\n0kT6tCYizVJ2LsIP/3szldVRZo3ry2dnDeoUf3lwjhQBWlYoIiIiyckIpxMcPpPg8JkNfb7cs8ew\ncguwc0dh2MF2zWNm9iY04W6C4+/CLT9aX4BxXez8iZjd+3eKv3+2NcOysfuPxe4/Fs+J4pQUET9S\nhBePXvviuEP8xB7iJUXES4rAsrH7FdYvU8wbixEItf0LuMCL1hDb+wFWz8EaewFUyBKRZvA8j5+9\nup2yyjqmFvTkC7cM7RR/kHie93Gj936FPqcRERER8ZeZkkFw5By/YwAXZqx164fVLbHbWPjNsIME\n8icSyJ/Y5Gs81yF+bFf9zLhDH+Ec2oRzaBPYQey8sfVFrX6j27TI6XketW//vL6YhmbjST0VskSk\nyY6cquLgiUr65qTx4O0jMDvJHxxu+TG8qjLMnHzM1K5+xxERERER8Z1h2tj9RmP3G40X/+KFZYob\nLixT3IBTvAECYewB4wkMmozVdxTGpzYr8DyvoQccTpTg+Dsx07ObnCG27U/ES4owuuRgpHTBPV1M\ndPNrRDe/hpnVh/BNX6lflitJRYUsEWmy94tOAHBTYR9sq/M01dRuhSIiIiIiV2ZYAez+47D7j6tf\npnhkK86B9fX/3fcBzr4PIJRGYMAE7EFTMEJp9b25ijfgVZU13Cd2YD3haX+BPXT6NWdTxUsPUbfh\nJbBsUm7+H1jd+39ix8r63mm1f/opqZ99AjMl44r38TxPM7c6GRWyRKRJYo7Lup0nsUyDKQU9/Y7T\nquIqZImIiIiINIlhBwkMnERg4CS8aC3OkS04BzbglBQR2/MesT3vNTrf6jUUe+BkvOqzRIteJ/Lu\nL7EPbSI048tXXA3hRWupffvfwY0TmvYFrO71m0uZGTmExi4gNHYBkXd/SWzPaiKrniVl/v+67A6h\n8bISav/0FFbvYYRvehDDtFr/GyLtToUsEWmSrfvPUB1xGD80h4zU9m322Za8umrip/ZjpGRgdk/8\n3RdFRERERNqLEUwhMHgqgcFT8eqqcQ5tIlb8IcQi2APGYw+c1Ggpod1/HLWrnsE5vJn4yX2Epj+A\nnT8Jw2xchIq8/yu8ylPY/ccRKJh72ecOTfsC8VP7iZcUEdv2J4KF8xsdj5cfo3bZP+FFzuOcLyXi\nOoRnfe2S55LEo0KWiDTJ+9vqlxVOH93b5yStyynZBp6L1W/MZf8VR0RERERErs0IpREYNoPAsBlX\nPMfqNYS0hU9Qt/5FYjtXEnn73zFSf4c9cBKBgZMxew7C2bcWZ/9ajLRuhGd+9YrLAo1AiPDcb1Dz\n6iLqNvweq/cwrJx8ANyKE9Qu/Ue8yHkCw2bgHNuJs38dEdO6cE/9vT+RqZAlItdUfr6ObcVlZKQF\nGTWwm99xWtXH/bG0W6GIiIiISFszAiHC07+IPWAC0a3LiR/fSWz7m8S2v4mR1g2vrhoMg/Ccr2OE\n0696Lyu7H6EbPkfdml9T+/a/k3bvIrzaSmqW/iNebSWBkXMI3fgAwfOl1Lz2Dzh711BnWoRmfFnF\nrASmQpaIXNPaHSfxPJha0LNTNXn3XJd4yTYwLezcUX7HERERERFJGnZuAXZuAW7NOZyDH+EcWE/8\n5D7AIzjhM9i9hzXpPoGRc4gf24lzaCO1K/8Dt+wIXk0FgWE3EbrxCxiGgZHRg9Q7HqPmtX8gtvs9\nMG1CNz6gJvAJSoUsEbkqz/NY00mXFbqlxXh1VVh9RmAEU/yOIyIiIiKSdMzUrgQL5hIsmItbXY5b\nfgyr78gmX28YBuGbHqS69CDxI1sAsIdMI3RT41lXZtdepNzxGLWv/QOxnSvBdQhN+zyGHWr11yRt\nq/NMrRCRNlF8vJITZTXk9+5C35yrT+1NNI52KxQRERER6TDMtCzs3FHNXvZnhNMJz30EginYQ28k\nPPOhy97DyuxDyoLHMcJdiO1+j+rF3yd++kBrxZd2okKWiFxVZ23y7tVVE9u/DlAhS0REREQk0dm9\nhpD+xZ+SMuvhq+5MaHXrS+rC/xerXyHeuZPU/OHvqNvwMl7cace0cj1UyBKRK6qLxdmw6xS2ZTJ5\nZE+/47QaL1pDzfIf4p0vxcodhdG1l9+RRERERETkOhlm07onmWlZpMz/34RuehDsENEtS6l5dRHx\nspI2Tngpz3WJnz6gQlozqJAlIle0eW8ptXVxxg/tTlo44HecVuFFa6l5/Ue4pQexeg0l5eb/qSaP\nIiIiIiJJxjAMgsNnkrbwCazew3DLSqhZ8gPqtizFc912yeBWnqZ26T9Q8+oT1L7xr3hOtF2eN9Gp\nkCUil+V5HquLOteyQi9WR+2KH+Oe2o/ZczAp8/83RkDNHUVEREREkpWZkUPKHY8TuuFzYBhEN7xM\nzWv/H+65k5ec60WqiO1+j+jOldcsOsXLSqjbsgzn5F48r3FhzPM8ojvfofrl/0v85F4wLeJHt1P7\n5tN48Virvr7OSLsWishlvbXxKLsOl5OdEWbkgG5+x7lunhOl9o1/JX5yL2ZOPqm3fUc7FYqIiIiI\nCIZhEiy8FavfaCKrnsE9tZ/qxX9LaMp9BIZMwzm8hdiB9cSPbgc3DkBsx1uEZ30NK2dAo3t5bpzo\nlmVEN/2h4VwjrRv2wEkEBk3BSM0k8t5z9fcyTILj7yIwYja1y/+ZeEkRkbd+RvjmbzZ5mWQyMjzP\n8/wO0ZHFYnEqKmr8jnFdTNMgOzudsrIqXFfDnWxaMv5FB8r4yctbsUyTx/9iHIP6dm3jlG2v9o2f\n4BzejJmdR+qCxzDCnWsHxsvRz35y0/gnL419ctP4Jy+NffLS2Leu+kLUUqIb/wheHAwDLpZNQmkE\n8ifinj9D/NgOMCyC4+8kOO4ODNPGrThB7TvP4JYWgxUgMHIO7plDxE/sBS7c48L9zMzehGc9jNVj\nIABuTQU1r/0D3rmT2PkTCc99BMO06o9VncUp/hCnpIjAsOkEBk9tyJuo45+ZmUogYLXoWpX4RKSR\nY6VV/PwP2/E8+MrtwztFEStecRzn8GaMLt1JWfBoUhSxRERERESk+QzTIjT+buy8MUTeex73/Bns\n/mMJDJqC1XckhmnjeR6xnSupW/8i0Y2v4hzegj1gPNHNr0E8htljICmzHsbMrG/R4laX4xR/SKx4\nA25ZCYERswhNiS3elgAAIABJREFUWohhBxue10zNJPWOx6l57Umcgx8ReecXWL2G4hxYX7/88AKr\n5+B2/550NCpkiUiDypooP3m5iEg0zp3TBnBDQefYzc8p/giAwNDpmOEuPqcREREREZGOzuo+gLR7\nF+F53iWbQxmGQbBgLnbuKGovLEWMnjkEpkVw0kKCY25vmE0F9bskBkffQnD0LZe93yfPayhmHViP\nc2B9/fOlZ2MPnExg0BTM7v3b7DUnChWyRASAmOPy9CvbOHMuwsThPbh7Rr7fkVqNc3AjAHb+RJ+T\niIiIiIhIIrnaDudm156k3vl/iG1/A+f4bkKTFmJl57X4fgBmejapCx4nsubXmF171hevegzEMLRX\n30UqZIkIAL96Yzf7j55jQK8ufHXBCMxrvMEmCrfyNG7ZYYyuvTCz+vodR0REREREOhHDNAkW3kaw\n8LZWu6eZkUPqbd9ptft1NirpiQj7j55jzbaTZKYH+Z8LCwm1sOleR+QcvLCsMH/iNf/1Q0RERERE\nRDo2FbJEhGVrDwFwz00DyeoS8jVLa4tdKGTZA7WsUEREREREJNGpkCWS5I6ermLrgTKyuoSY2kma\nu1/kVpXhni7G6NIdM1tNEUVERERERBKdClkiSW75usMAzJ+ch211rreETzZ517JCERERERGRxNe5\nPrWKSLOcrqhl/a5TpKcEuGlMH7/jtLpP9scSERERERGRxKdClkgSW7H+CJ4H8ybmEgp2ngbvAG5N\nBfGT+zDSsjB7DPQ7joiIiIiIiLQCFbJEktS5qjreLzpBKGgxd0Ku33FanXNoE+BhD5iAYeitTkRE\nREREpDPQpzuRJPWnj0pw4i6zx/YlLRzwO06LeJ5LdOvrOIc2X3LMaditcFJ7xxIREREREZE2Yvsd\nQETaX00kxjubjmFbBjdP6ud3nBaLH99N3foXAbAH30D4xgcwQmm4kfPEj+/GSMnA6jnE55QiIiIi\nIiLSWlTIEklCKzcdIxKNM3NsH7K6hPyO02Lxo9vr/8e0cPavo/r4bsIzv4JbXQ6eiz1gPIapiaci\nIiIiIiKdhQpZIkki7roUH69k56Fy/rThCIYB86fk+R3rujjHdgKQetdfE935Ns7eNdS+/iOMUDoA\ndr6WFYqIiIiIiHQmKmSJdFKe53HybA27Dpez/3glW/edobbOaTg+e1xfemal+pjw+niRKtwzhzHS\nszFz8kmZ9TCxAeOpe+8/8SLnIZSG1WeY3zFFRERERESkFamQJdKJnKuqY+ehcnYeOsvOw+WUn69r\nOBawTQoGZDFyQDdGDMiif88uPia9fs7xXYCH3XckhmEAEBgwAavnEKJblmH1GIRh6i1ORERERESk\nM9GnPJFOYuOe0/z8DzuIux4AhgH5vTMoyO/GDYV96JERxO5E/aLix3YAYPUtaPS4mZJBeOrn/Igk\nIiIiIiIibUyFLJFO4FR5Db9ctgvX9Zg1ri+j8rsxPC+T1HAA0zTIzk6nrKwK90KRqzO42B/L6jPC\n5yQiIiIiIiLSXlTIEklwMSfOvy/ZTiQa545pA7j3poF+R2pzbmUpXuVpzG79MFO7+h1HRERERERE\n2knnWWckkqR+99Y+jpyuYnheJp+Znu93nHbhNCwrHOlzEhEREREREWlPKmSJJLB1O06yastxMtKC\nfO2uAkzT8DtSu4hfWFZo5xZc40wRERERERHpTFTIEklQJ8qqeWHFHgzg63eOJDM95HekduF5bn0h\ny7Sweg3zO46IiIiIiIi0IxWyRBJQXSzOz17dTl0szt3T8xkxoJvfkdqNW1aCV1eF1XMwRiA5inci\nIiIiIiJSz/dC1m9+8xvmzJnD6NGjue+++ygqKrriuQ888ADDhg275OtrX/tawzme5/GTn/yE6dOn\nU1hYyJe//GUOHz7cHi9FpN2sWH+EY6XVjByQxR3TBvgdp13FG/pjaVmhiIiIiIhIsvG1kLV8+XKe\nfPJJvvnNb7JkyRKGDRvGQw89xNmzZy97/k9/+lPef//9hq+lS5diWRbz589vOOeZZ57h17/+NT/4\nwQ946aWXSElJ4aGHHiIajbbXyxJpUzEnzspNRzEM+NL84UnTF+si52J/LDV6FxERERERSTq+FrKe\nf/557r//fhYuXMjgwYNZtGgRoVCIJUuWXPb8zMxMcnJyGr7WrFlDOBxuKGR5nsevfvUrvvGNbzBv\n3jyGDx/OP/3TP3Hy5ElWrlzZni9NpM2s3XGK8zUxJgzNISczxe847cpzosRP7IVACmZOcuzQKCIi\nIiIiIh+z/XriaDTKjh07eOSRRxoeM02TadOmsWXLlibdY/HixSxYsIDU1FQAjh49SmlpKTfeeGPD\nOV26dGHMmDFs2bKl0cyt5kj0GS8X8yf665D6Yu2fPiwBYP6U/k0a0840/k7pAYhHsfuNx7J9e/tK\nGJ1p7KX5NP7JS2Of3DT+yUtjn7w09sktGcfft0+C5eXlxONxunfv3ujx7OzsJvW0KioqYu/evfz9\n3/99w2OlpaUAl73nxWPNZdsm2dnpLbq2o8nKSvM7glynTbtPc/xMNcP6ZzFlTN9mXdsZxv9s0T4A\nug4bT9dO8nPZHjrD2EvLafyTl8Y+uWn8k5fGPnlp7JNbMo1/wk5pePnllxk6dCiFhYVt+jyO41JZ\nWdumz9HWTNMgKyuN8vJqXNfzO45ch9+/vQeAueP7UlZW1aRrOtP4V+3bDEBd1uAmv/5k1pnGXppP\n45+8NPbJTeOfvDT2yUtjn9wSdfwzMlIIBKwWXetbISsrKwvLsjhz5kyjx8vKysjJybnqtTU1NSxb\ntoxvfetbjR6/eN2ZM2fIzs5udM9Ro0a1OGsi/Wa4Gtf1Os1rSUZHS6vYXnyW7Iww44Z0b/ZYJvr4\ne3XVxEsPYaR1gy49E/q1tLdEH3u5Phr/5KWxT24a/+SlsU9eGvvklkzj71uz92AwSEFBAR988EHD\nY67rsnbtWsaOHXvVa1esWEE0GuWuu+5q9Hhubi45OTmN7llVVcXWrVuveU+Rju7NC72x5k3MxTJ9\n3aeh3XmuQ+Td5wAPO3cUhpE8679FRERERETkY74uLXzwwQd5/PHHKSgooLCwkBdeeIFIJMI999wD\nwGOPPUbPnj357ne/2+i6l19+mXnz5pGVldXoccMw+OIXv8jPfvYz8vLyyM3N5Sc/+Qm9evVizpw5\n7fa6RFrbueooa3ecIhy0mFHYx+847cpz40RW/gLn0EaMjJ4EJ93rdyQRERERERHxia+FrNtvv52z\nZ8/y1FNPUVpayogRI3j22Wfp1q0bACdOnMD81MyT4uJiNm7cyHPPPXfZez788MPU1tbyt3/7t1RW\nVjJhwgSeeeYZgsFgm78ekbbyzqajOHGXOeP7kRpO2NZ2zea5LpFVz+IUb8DokkPqHY9hpmb6HUtE\nRERERER8YnielxyLKFsoFotTUVHjd4zrYpoG2dnplJVVJc2a2c4k5sT5q599QFVtjH/8+lS6Z6Y0\n6/pEHX/Pc4m8+zzO3tUY6dmk3vk9zC5X758njSXq2Evr0PgnL419ctP4Jy+NffLS2Ce3RB3/zMzU\nFjd7T65GOyIJ6J1NxzhfE2PC0JxmF7ESled51K3+VX0RKy2L1DseVxFLREREREREVMgS6cg+2n2a\nF9/Zj2Ua3HZDf7/jtBvn8CZiu1dhpHQldcFjmBk9/I4kIiIiIiIiHYAKWSIdVNGBMv7jjzsAePjO\nkeT3zvA5UfuJn9wPQGjyZzEze/ucRkRERERERDoKFbJEOqA9R8r5tyXbiLseX54/nMkjevodqV25\nZ0sAMLPzfE4iIiIiIiIiHYkKWSIdzMETlfzk5SJijsufzx3CjDF9/I7U7tyyI2BYmFnJ99pFRERE\nRETkylTIEulAjp2p5kcvbiESjfOZGfncMqmf35HanVtzDq+2EjOrN4YV8DuOiIiIiIiIdCAqZIl0\nEJ7n8Z/Ld1EdcZg/OY87pw3wO5IvGpYVdku+Ip6IiIiIiIhcnQpZIh3EtuKzHDheSW5OOp+dPQjD\nMPyO5Au37AgAlvpjiYiIiIiIyKeokCXSAXiexx/eLwbg7un5mElaxAKIl11s9K4ZWSIiIiIiItKY\nClkiHUDRgTIOnjhPXo90xg/t7nccX2lpoYiIiIiIiFyJClkiPqufjXUQqJ+NlaxLCgG8eAy3/ARG\nSgZmale/44iIiIiIiEgHo0KWiM+27i/j0Mnz9O/ZhbFDknw2Vvlx8OKY6o8lIiIiIiIil6FCloiP\nNBurMS0rFBERERERkatRIUvER1v2neHwqfMM6NWFMYOz/Y7ju4uN3i01ehcREREREZHLUCFLxCea\njXUpt+wIgJYWioiIiIiIyGWpkCXik417Sjlyuor83hkUDtJsLM/zcMtKwLQxM3v5HUdEREREREQ6\nINvvACLJpqKqjtfWHOK9rccBzca6yKupwKurwszuj2HqrUlEREREREQupU+LIu2kOhLj9XVHeOuj\nEqKOS0rI4s5p+Ywe2M3vaB2Ce6E/lpmd63MSERERERER6ahUyBJpB2u2neB3b+2jps4hYJvMn5LH\n7Tf0Jz0l4He0DiN+tr4/ltVN/bFERERERETk8lTIEmljZ87V8sKK3bguzBzbh7tuzCerS8jvWB3O\nxzOytGOhiIiIiIiIXJ4KWSJtbMl7B3HiHnfdOIDPzBjod5wO62Ihy9KOhSIiIiIiInIF2rVQpA0d\nOXWedTtO0iU1wK2TVaC5Es+J4p47gZGWhRFO9zuOiIiIiIiIdFAqZIm0ocXvFuMBd92YT0pIEyCv\nxC0/Dp6H2U3LCkVEREREROTKVMgSaSO7DpezrbiMHpkpzBzbx+84HZpbdqHRu5YVioiIiIiIyFWo\nkCXSBjzP4+VV+wG4d+ZAbEs/alcTP3uh0Xu3XJ+TiIiIiIiISEemT9cibeCjPaUcPHGe/r26MHF4\nD7/jdHgXZ2SZmpElIiIiIiIiV6FClkgrc+Iui989AMCfzRqEaRg+J+rYPM8jXlYCVgCza0+/44iI\niIiIiEgHpkKWSCtbvfU4p8trKcjvxsgB3fyO0+F51WchWoPZLRfDtPyOIyIiIiIiIh2YtlETaSWu\n57Fq8zF+/87Hs7Hk2tyy+v5YlnYsFBERERERkWtQIUukFZwur+E/X9/N7iMVmIbBn80aRF7PLn7H\n6vA8zyO2ZzUAZnf1xxIREREREZGrUyFL5Dq4nsfbG4+y+N0DRGMuuTnpfHXBCPr3UhGrKWK7VuEc\n2ojRJYfAkGl+xxEREREREZEOToUskRZyXY9//f1Wth88i2Ua3HXjAO6YNgDbUuu5poifPUrd2t+C\nYZEy9xGMYKrfkURERERERKSDUyFLpIW2FZex/eBZemal8MhnRmkpYTN4Th2Rt38G8RihKfdh9Rjo\ndyQRERERERFJAJo6ItJCKzcdA2DhTPXDaq66D36LW34cK3cUgcL5fscRERERERGRBKFClkgLnCqv\nYVtxGVldQowd0t3vOAkldmADsd3vYqRkEJ71MIahtyERERERERFpGn2CFGmBdy7Mxpo5to96YjWD\nW1lK5L3nAYPw7K9jpnb1O5KIiIiIiIgkEH0CF2mmulic94tOYJkGM8f08TtOQqlb998QqyU49nbs\n3AK/44iIiIiIiEiCUSFLpJnW7zxFTZ3DxOE96Joe8jtOwoifLcE5tBEjNZPg+Lv9jiMiIiIiIiIJ\nSIUskWbwPI+3Nx4FYO74XJ/TJJboptcACI65HcMO+pxGREREREREEpEKWSLNsP/YOUpOV5HXI51B\nfTP8jpMw4hXHcYo/xEjJIDBipt9xREREREREJEGpkCXSDBdnY82ZkIthGD6nSRzRzcsAj2DhfAxb\nyzFFRERERESkZVTIEmmiiqo6Nu4pJS1sM2VkT7/jJAy38jTO/rUYoXQCI+f4HUdEREREREQSmApZ\nIk303pbjxF2P6YW9CQUsv+MkjOiWpeC5BEbfghEI+x1HREREREREEpgKWSJN4MRdVm05hgHMHtfX\n7zgJw60qI7Z3DQRTCY6a53ccERERERERSXC23wFEEsGGXaeoqIpSOCibHlmpfse5Ll7coe79F3DP\nn7nkmD1wMsGRs1vtuaJbloEbJzjqZoxgYn/fRERERERExH8qZIlcg+t5LF93BIDbpuT5nOb6OYc3\nE9uz+rLH4if2YvUehpXV57qfx60uJ7bnPQiECY66+brvJyIiIiIiIqJClsg1bN13huNnqhnctytD\n+2X6Hee6xfauASA886tYvYd9/Pi+tUQ3LqFu7W9Jue2717Uro+e51K1/EeIOwdG3YoTTrzu3iIiI\niIiIiHpkiVyF53ksW3cYgNun9r+u4k5H4NZWEi/ZhhHugj14KmZGj4av4NgFmF17ET+6Hefw5hY/\nh+d51K35L5z96zBSMwmMvrUVX4GIiIiIiIgkMxWyRK5i95EKio9XkpuTxphB2X7HuW7OgfXgxbEH\nTcGwGk/INCyb0LS/AKBu7e/wnGiz7+95HnVrf0ts50qMlAxS7ngMMyWjVbKLiIiIiIiIqJAlchXL\n1x4C4PYbEn82FkBs3wcABIbeeNnjdr9C7P7j8M6XEi1a0ax7e55H3foXiW1/EyPchZQFj2NlXn+v\nLREREREREZGLVMgSuYJDJyvZcaic7l3DTBrRw+841y1efhy39CBmZm/M7gOueF5o6ufAtIluWYpb\nVdake3ueR/TDxcSKVkAojZQFj2J169tKyUVERERERETqqdm7yBUsX1vfG+u2KXlY5qU134u9oOJl\nhy85ZvcdSWjivW2esTkuNnm3h9x41dllZkYPgoXziW5ZSt36l0iZ+8i17130OtEtSyGYQurtj2Jl\nJ/7ujiIiIiIiItLxqJAlchknyqrZuKeUjLQg0wt7X/Yc92wJsZ1vX/ZY9NR+7ME3dJildZ7nEt37\nAWAQGDL1mucHx91BbN8anAPrcUbOwf7E7oaf5p47Sd2Hr4AVJPX2v8LKGdB6wUVEREREREQ+QUsL\nRS7j9fVH8IBbJvUjYFuXPcc5shWA4Ng7SPv8jxu+guPvBiC2Y2V7xb2myOEdeNVnsfoMx0y/dtN6\nIxAmNOV+AOre//VVG79H1v4OXIfg+DuxegxqtcwiIiIiIiIin6ZClsinnK2MsHb7SVJCNrPHXbnP\n08VClj1wEmZaVsNXYNQ8sGxie9fgxSLtFfuqzm9bBVy5yfvl2IOmYPUdiVt+lLr1L172HOdIEfEj\nWzG65BAcfWtrRBURERERERG5IhWyRD5l2drDxF2PuRP6khK6/OpbL1KFe/oARloW5qf6QZnhLtgD\np0CstmGXQD95sTqqd60DO4g9YEKTrzMMg/CshzHCXYjteJvYoU2N7xt3iKz9LQDhqX+BYQdbNbeI\niIiIiIjIp6mQJfIJZecivLf1OCkhi1smXblhuVNSBJ6H3a/wso3TgwVzgfrlhZ7ntVnepogd3IgX\nixDIn4ARTGnWtWZaFuFZDwEQefeXjXYxjG1/E+/cSazcUVj9x7ZqZhEREREREZHLUSFL5BOWrT1E\n3PWYN6Ef6SmBK57nHCkCwMobc9njVo+BmDn5uOVHiZ/c2xZRm+ziboXNWVb4SXbeGAKjb4W6aiIr\n/wPPjePWVFC36Q9gWISm/cVVd0EUERERERERaS0qZIlccOZcLauLTpASsrllcr8rnue5cZyj28C0\nsfuOvOJ5H8/KeqvVszaVW12Oc2wHVnoWdt+CFt8nNPmzmN0HED+5l+imP1K34fcQixAYfXOH2ZlR\nREREREREOj8VskQuWPpBfW+smyfmkha+8mys+OkDUFeN1XsYRiB8xfPsgZMxQuk4BzfhVpe3ReRr\niu1+FzyP9NEzMcyW/7gbVoCUuY9AIEx08x9x9q7BSMkgdGGHRhEREREREZH2oEKWCFBaUcuabRdm\nY0268mwsgPjF3QqvsKzwIsMOEhh+E3hxYrtWtVbUJvNc58LzGmSMu/m672d27Ul4+hfhQs+v0OQ/\na3bPLREREREREZHr0exC1sqVK3Fdty2yiPhm6Qf1vbFundSP1KvMxgJwmljIAgiMmA0YxHa/i+c6\nrRG1yZxDm/BqKrDzCglk9WqVewaGTCM4/m4Cw2dht7DnloiIiIiIiEhL2c294Jvf/CbZ2dncfffd\n3HvvvQwaNKgtcom0m9MVtazZdpLUkM28iVefjeVWleGePYrRtRdm157XvLeZkYOVV0j8yFacg5sI\nDJrcWrGvKbbjbQCCo+a26n1DE+9p1fuJiIiIiIiINFWzZ2S9+eab3Hfffbz++uvccccd3H///bz0\n0ktUVVW1RT6RNrd0zSFcz+PWyf1IDV+9tntxt8KmzMa66GLT9+jW5cTLj7U8aDPEzx4lfmIPRkYP\n7H6j2+U5RURERERERNpaswtZubm5fOtb32LlypU899xz5OXl8eSTTzJ9+nQeffRR1q1b1xY5RdrE\n6YpaPth+krTwtWdjQfOWFV5k5Y7CzOqLe+YQNb//a6pf/hvqNv0R99ypFue+ltjOlQAER87GMNQK\nT0RERERERDqHZi8t/KSpU6cydepUTp06xXe+8x1ee+01li5dSp8+fXjggQf4whe+gG1f11OItKmN\ne07jeh6zx+eSErr671XPiRI/vhMCYaxeQ5v8HIZhkrLgMWK738Up3oB79ijRs0eJfvQKVu/hpNz6\nLYxgapPv53kezt73ie1ZTXDiPdh9RjQ+Hq0ltu8DsAIEhs5o8n1FREREREREOrrrqjJt2LCBV155\nhTfeeINAIMDnP/955s2bx+rVq3nqqafYtm0b//Iv/9JaWUVa3Z4jFQAUDsy+5rnxE7vBiWIPmIBh\nNe9Hx0ztSmj8XYTG30W8/BjOgQ3E9n1A/MRuIqtfIDznLzEM45r3cWvOUbf6P3EObwagdsWPSbnt\nu9i9hzWcE9u7BmIRAsNmYITTm5VTREREREREpCNrdiHr2LFjLFmyhFdffZVjx44xefJknnjiCW65\n5RaCwSBQP1Nr3LhxPProo60eWKS1xF2XvSUVBAMmA3p3ueb5LVlWeDlWVl+sifcQHH0L1Yv/FufA\nepy+BQSG33TV62LFH1L3/q/wIucxMnpi5xUS2/4mtSt+TOrtf4XVczCe5zUsKwwUtG6TdxERERER\nERG/NbuQNW/ePHr06ME999zDwoUL6dfv8n2FBg8ezOjRajItHdfhk1VEonEK8rthW1fvI+V5XkOj\ndyuvsFWe3wilkTLnL6l57Ukia/4Ls+dgrKw+lz53XTWRNb/G2V/ffy4wci6hKfdhBEIYKRlEP1xM\nzfJ/IXXBo3ixCG7Fccweg7C6D2iVnCIiIiIiIiIdRbMLWT//+c+ZMWMGpnn1D/75+fn8+te/bnEw\nkba2+0g5AMPzMq95rne+FO98KWZ2f8zUa5/fVFavIQQn3kP0w8VE3v53Uj/zfzHsYMNxp2QbkXd/\niVdTgZHWjfDMr2Dnjmo4Hhp3J8Qdopv+QM3yH2Jm9gI+3ilRREREREREpDNp9nZmEyZM4MyZM5c9\ndvr0aaqrq5t1v9/85jfMmTOH0aNHc99991FUVHTV88+dO8f3v/99pk2bxujRo7ntttvYsGFDw/Hv\nfe97DBs2rNHXV7/61WZlkuTwcSEr65rnuhUnALC657V6juCYBVh9RuCeLaFu3X8D4MUiRFb/J7Wv\n/wteTQX2kBtJ++wTjYpYDddP+AzBsXdAtAb3dDFGuAv2wEmtnlNERERERETEb82ekfXXf/3XdOnS\nhb/7u7+75NjTTz/N+fPn+fGPf9ykey1fvpwnn3ySRYsWMWbMGF544QUeeughVqxYQbdu3S45PxqN\n8uCDD5KTk8PTTz9Njx49KCkpITu7caPu2bNn88QTTzT8+mLvLpGLnLjLvpJzhAIW/Xtduz+We+4U\nAEbXXq2exTBNwrO/Rs3ivyW2cyVGSldie9/HO1+KEe5C6KYvExgw4crXGwbBSQvxXIdY0QoCI+dg\nWIFWzykiIiIiIiLit2YXsj766CMWLVp02WM33XQTP/jBD5p8r+eff57777+fhQsXArBo0SJWrVrF\nkiVLLjuLavHixVRWVvLiiy8SCNR/UM/Nzb3kvGAwSE5OTpNzSPI5fPI8dbE4o5rQHwvAPXcSALNr\nzzbJY6ZlEZ71MLUrfkR04xIA7AETCM34EmZKxjWvNwyD0JT7CQybgdkGxTYRERERERGRjqDZhazz\n588TDocveywUClFZWdmk+0SjUXbs2MEjjzzS8JhpmkybNo0tW7Zc9pqVK1cyduxYfvCDH/DOO++Q\nnZ3NwoUL+dKXvoRhGA3nrV27lqlTp5KRkcG0af8/e/ceHVV97///teeW+/3GHREkYAwQsSoIXiKt\nFv22h2LxR9GqLT1eW7qq1XO6VhG0Fpf91lZr9RyFeqiHeqktXdVS61fxUiv1UotUFJFbDJCQeyb3\nyczevz9CBmICmUlmMjPZz8darFX27L3zHt6ZIK9+Pu89X6tWrVJ29tDnGjkcxuAnxbHe+hP9fUTS\nx5VNkqSZp+SE9OdieXtWZLlyxkbtz9FzymyZn/uKfB++quRzrpD7tPl9vq8HZ8iZ1z/Ypf/2Re/t\njf7bF723N/pvX/Tevui9vdmx/2EHWZMnT9arr76qBQsW9Hvttdde06RJoc0QamxsVCAQUH5+fp/j\neXl5qqioGPCayspKbdu2TUuWLNFjjz2mPXv26K677pJhGLrmmmskSQsXLtTnP/95TZgwQZWVlbr/\n/vt1/fXX68knnxx0QP1AXC6H8vLSw74uHuXkpMW6hLix93BP4HpO6biQ+tvWUiNJyj9lihzupOgV\n9oUVPb+igP7bF723N/pvX/Te3ui/fdF7+6L39man/ocdZF199dW688475Xa79ZWvfEUFBQWqra3V\n5s2b9Zvf/CasrYXhsixLBQUFWrNmjZxOp0pKSlRZWamnnnoqGGRddtllwfN7h70vWrRI7777rs4+\n++ywv6bq520KAAAgAElEQVTfb8rr7YjYe4gFh8NQTk6aGhvbZJpWrMuJOX/A1M799Ur2OJWT6lJ9\nfetJz7f8Pvmb62Sk56nR2y2pe2QKjRD6b1/03t7ov33Re3uj//ZF7+2L3ttbovY/MzNFbrdzSNeG\nHWQtW7ZMdXV1evTRR/U///M/weNJSUn67ne/q2XLloV0n5ycHDmdzn5PQKyvrz/hfKv8/Hy53W45\nncfe7NSpU1VVVXXCrzNx4kTl5OSooqJiSEGWpIT6ZjgZ07RGzXsZjn2HvPJ1myo9NU+GjEH/TAJN\nNZIsObKKEvrPj/7bF723N/pvX/Te3ui/fdF7+6L39man/ocdZEnSTTfdpKuvvlr//Oc/1dTUpOzs\nbJWVlSkjY/Cnv/XyeDwqKSnRm2++qfLyckmSaZratm1bcHXVZ5WVlWnLli0yTTO4TfDAgQMaO3bs\nCb9OdXW1mpqaVFhYGMY7xGi269NGSdKMyaHNTTO9vYPeGaIOAAAAAEAsDSnIkqSMjAydf/75w/ri\n1113ne644w6VlJRo1qxZ2rhxozo7O7VkyRJJ0u23366ioiLdeuutkqTly5dr06ZNuvfee7V8+XLt\n3btXjz/+uL797W9Lktra2vTQQw/pkksuUX5+viorK/WTn/xEU6ZM0bx584ZVK0aPYJA1KSek882m\nnkHvjszoPLEQAAAAAACEZshB1rvvvqsDBw6oq6ur32srVoQ2rHrx4sVqaGjQgw8+qNraWs2cOVPr\n169Xbm6uJKmqqqrPgPbx48dr/fr1WrdunZ588kmNHTtWN9xwQ/DrOZ1O7d69W3/4wx/U0tKiwsJC\nLViwQKtWrZLH4xnqW8Uo4g+Y2nOwWSlJTk0qCm2Iv9W7IiubIAsAAAAAgFgKO8iqq6vTtddeqz17\n9sgwDFlWzx5Mwzj2qMdQgyxJuuqqq3TVVVcN+NoTTzzR79jcuXP17LPPDnh+cnKyNmzYEPLXhv3s\nO+yVz29q1uQ8OUN8iqXZ3Lsii62FAAAAAADEUmj/kj/Ovffeq/T0dL322muyLEvPPPOMtm7dqlWr\nVmny5Mn6y1/+Eo06gYj4OMxthdLRIMtwyMjMj1ZZAAAAAAAgBGEHWe+8846+8Y1v9Hmy4Lhx43TD\nDTfoS1/6ktauXRvRAoFI2vVpk6TQB71b3Z2y2ptkZBTIcAx5Jy4AAAAAAIiAsIMsr9er3NxcORwO\npaenq76+PvhaWVmZ3nvvvYgWCERKt9/UnkPNSklyaVJhaE/YDG4rzGI+FgAAAAAAsRZ2kDVhwgTV\n1NRIkqZNm6bnnnsu+Norr7yi7OzQVroAI23f4WZ1+00VT8yWw2EMfoEIsgAAAAAAiCdhB1kXXHCB\n/va3v0mSbrzxRr344os6//zzVV5erieeeOKEg9uBWPuoomc+VvGk0MNWs/noEwuzGPQOAAAAAECs\nhT3057bbbgv+7wsuuEBPPvmkXnrpJXV2dmr+/Pm64IILIlogECkf7G+QJJ0xJTfka1iRBQAAAABA\n/AgryPL5fNqwYYMuuugizZgxQ5JUWlqq0tLSqBQHREprR7f2H/YqJyNJ4/LTQr6OFVkAAAAAAMSP\nsLYWejwe/dd//Ze8Xm+06gGi4sMDDbIklZ6aK8MIbT6WJFnNRySnS0Z66Ku4AAAAAABAdIQ9I2vW\nrFn68MMPo1ELEDX/2tfzdM0zpuSFfI3V2Sqrq1WOzCIZRtgfFQAAAAAAEGFhz8j6/ve/r9tuu00u\nl0sXXHCB8vLy+q1wSUlJiViBwHBZlqUP9jfIYRg6/ZSckK8zvczHAgAAAAAgnoQdZC1btkyS9KMf\n/Uj33HPPgOd89NFHw6sKiKCDtW1qbvVp2oQspSa7Q77ObGI+FgAAAAAA8STsIOvHP/5xWDOGgFj7\nILitMLw5V70rsgxWZAEAAAAAEBfCDrK+8pWvRKMOIGp652OVnhr6fCyJFVkAAAAAAMQbJlhjVOv0\n+fXJwWalp7g1eUxGWNcyIwsAAAAAgPgS9oqsc889d9Cthdu2bRtyQUAk7apoUsC0VDIlV44wtsRa\nliWz+YjkTpaRkhXFCgEAAAAAQKjCDrJWrFjRL8hqbm7W3//+d7W2tmrp0qURKw4Yrn/tH9p8LKuj\nWerulCNvMjPhAAAAAACIE2EHWd/+9rcHPG5ZllatWiWXK+xbAlGzc1+DpCEMem9mWyEAAAAAAPEm\nYjOyDMPQV7/6Vf3v//5vpG4JDMuRxnbVNHVoUmG6stKTwrrWbO4d9E6QBQAAAABAvIjosPfKykp1\nd3dH8pbAkH3QuxorzKcVSpIVXJHFEwsBAAAAAIgXYe8D3LRpU79j3d3d2rdvn5577jldeumlESkM\nGK4P9vXMxyo9NbxthRJbCwEAAAAAiEdhB1l33313v2Mej0djxozR8uXLdcstt0SkMGA4uv2mPvq0\nUUkep6aOD/+pg8e2FrIiCwAAAACAeBF2kLVr165o1AFE1J6DTfJ1myo7LV8uZ3g7aC3LlOk9IiWl\nyUhOj1KFAAAAAAAgXBGdkQXEi/d210kK/2mFkmS1NkgBP6uxAAAAAACIM2EHWT/72c+0evXqAV9b\nvXq1fv7znw+7KGA4On1+vbmzSi6nQ3NnFIZ1rRXwq/NvT0iSnHkTo1EeAAAAAAAYorCDrOeff15z\n584d8LWzzjpLzz///LCLAoZj284j6ugK6OyZhcpM9YR8nWX61fnyIwp8+r4c2WPlmbskilUCAAAA\nAIBwhR1k1dTUqKho4Ce5FRYWqqamZthFAUNlWZa2/uOgJOniuRNCv84MqHPro/If+IeMrCKlXH6H\nHKnhD4kHAAAAAADRE3aQVVBQoA8//HDA1z788EPl5oY/kwiIlN2VTTpU16YpYzM0ZWxmSNdYpqnO\nV9fLv+9tGRkFSr3sDjlSs6NcKQAAAAAACFfYQdall16qX/7yl3r11Vf7HH/ttdf08MMPa/HixZGq\nDQjby0dXY5WfGdpqLMsy1fn64/Lv2SYjPU+pl98hRzphLAAAAAAA8cgV7gWrVq3Srl27dMMNNyg7\nO1sFBQWqra1Vc3OzzjvvPH33u9+NRp3AoBpbuvTe7jqlp7h19szQhrz7P35D/t1/lZGW0xNiZeRH\nuUoAAAAAADBUYQdZSUlJ+tWvfqW//vWveuutt9TU1KTs7GzNmzdP5513XjRqBELy6j8PybQsnT97\nnNwu56DnW2ZAXf98TpKUsuhmOTLDe8IhAAAAAAAYWWEHWb0WLlyohQsXRrIWYMj8AVOvvX9YhiFd\nWDYutGv2/F1WS62c40vkLJoW5QoBAAAAAMBwhT0j609/+pPWr18/4GsbNmzQli1bhl0UEK53P66R\nt82nOdPylZ+VMuj5lmnKd3Q1lufML0W7PAAAAAAAEAFhB1mPPvqokpKSBnwtOTlZjz766LCLAsK1\n9R+HJIU+5N2//x2ZzdVyji2Wa2xxNEsDAAAAAAAREnaQVVFRodNOO23A16ZOnaqKiophFwWEo6K6\nRXsONWtMbqpmnpIz6PmWZcr3Xu9qrC9HuzwAAAAAABAhYQdZycnJqq6uHvC16upqeTyeYRcFhOPl\n9w5Kki46c7wchjHo+f4D/5TZeFCOomlyjpsZ7fIAAAAAAECEhB1kzZ8/X4888ojq6+v7HG9oaNAj\njzzCkwsxoprbfPr7ziNK9jh13hljBz3fsiz53vujJCmp7EsyQgi+AAAAAABAfAj7qYW33Xabli1b\npkWLFmnhwoUqLCxUTU2N3njjDWVmZur73/9+NOoEBvTKewflD5gqP3OiUpMH/3YOVL4vs75CjoIp\nck4sHYEKAQAAAABApIS9ImvcuHH64x//qKuuukrV1dV6/fXXVV1drauvvlq///3vNXbs4KtigEjo\n9gf0yj8PyTCkRXMHH/JuWZa6jq7G8pT9H1ZjAQAAAACQYMJekSVJubm5uvXWWyNdCxCWbTuPqKW9\nW2cVFyg/O2XQ8wOHPpRZs0+O3IlyTS4bgQoBAAAAAEAkDSnI2rJli5555hkdOHBAXV1d/V7ftm3b\nsAsDTsayLL34TqUk6QtnTwrpGt8HL0qSPHMuYzUWAAAAAAAJKOythc8995zuuOMOTZo0SdXV1Sov\nL9eFF14o0zSVnp6uFStWRKNOoI+d+xt0uK5NU8dlatr4rEHPN721Cny6Q0ZqtlynnjUCFQIAAAAA\ngEgLO8jasGGDbrrpJt15552SpK997Wtat26dXn75ZeXk5CglZfAtXsBw/SXc1VgfbpVkyT3jAhmO\nIS1EBAAAAAAAMRZ2kFVRUaEzzzxTTqdTTqdTra2tkqT09HR961vf0qZNmyJeJHC8g7Wt2rm/QXmZ\nyTpzev6g51t+n7o/fl0ynHLPvDD6BQIAAAAAgKgIO8hKS0uTz+eTJBUVFWnv3r3B1yzLUmNjY+Sq\nAwbw/46uxlp01gQ5HYN/C/v3viV1tck15Uw50nKiXR4AAAAAAIiSsPdYlZaW6uOPP9bChQtVXl6u\nhx9+WC6XS263W7/85S81Z86caNQJGzrS2K4DVS3Kz0pWQXaKMlLd8rZ3a9vOI0r2OLVw1riQ7tOz\nrVByn35xNMsFAAAAAABRFnaQdf311+vw4cOSpO985zs6dOiQ1qxZI9M0VVpaqrvuuiviRcJ+TNPS\nT5/arrrmzuCxJLdTKUlO+QOmys+cqNTkwb99AzX7ZNbulyNnvJxji6NZMgAAAAAAiLKwg6w5c+YE\nV11lZmbqkUcekc/nk8/nU3p6esQLhD29v6dOdc2dGpuXqnH5aapt6lBtU4eaWn1Kcju1aO6EkO7j\n2/myJMldcrEMw4hmyQAAAAAAIMoi8vg2j8cjj8cTiVsBkqSX3zsoSbriwqkqO61AUs8MtrZOvyQp\nPcU96D3Mzhb5970luVPkPm1+9IoFAAAAAAAjIiJBFhBJVfVt+vBAo/IykzV76rGnEhqGEVKA1at7\n1+tSwC/3jAtluJOjUSoAAAAAABhBYT+1EIi2re8dkiRddOZ4ORxD2w5omaa6e4e8l5RHrDYAAAAA\nABA7BFmIKx1dfr35QZVcTocWzho75PsEKt+X1Vov5/jT5cwO7emGAAAAAAAgvhFkIa78fWe1OroC\nOmdmoTJShz53rXv/u5Ik94wLI1QZAAAAAACINYIsxA3LsoLbCstDfCrhiQSqdkuSXONPH3ZdAAAA\nAAAgPhBkIW58/GmTDtW1acrYTE0Zmznk+5it9bJaauXInSAjOT2CFQIAAAAAgFgiyELc2PreQUlS\n+Znjh3WfQNXHkiTnmOJh1wQAAAAAAOIHQRbiQoO3U+/trlN6iltnzywc1r16txU6xxFkAQAAAAAw\nmhBkIS68tv2wTMvS+bPHye1yDutegapdkiTnmOmRKA0AAAAAAMQJgizEnGlZen3HYRmGdGHZuOHd\nq71ZZnO1HFlj5EjNjlCFAAAAAAAgHhBkIeYqqlvU3OrTaROylZ+VMqx7BaqPzscay7ZCAAAAAABG\nG4IsxNy/9tZLkmZPzRv2vYKD3gmyAAAAAAAYdQiyEHPvHw2ySgmyAAAAAADASRBkIaa8bT4dqPIq\nLzNJ4/PThnUvq7NVZsNBGRkFcqQPPxQDAAAAAADxhSALMfXB/npZkkqn5sswjGHdy1+9W5LkHMvT\nCgEAAAAAGI0IshBTO45uK5x1auS2FbrGzhj2vQAAAAAAQPwhyELMBExTH+xrkMvp0MzJOcO/H/Ox\nAAAAAAAY1QiyEDN7D3nV3uXXjEnZSvI4h3Uvy9chs75CRlqOjIyCCFUIAAAAAADiCUEWYia4rTAS\nTyus/kSyLDnHFA971hYAAAAAAIhPBFmImd4gqzQiQRbbCgEAAAAAGO0IshATDd5OHaxtVVFuqopy\nUod9Pz/zsQAAAAAAGPUIshATO/b1rMaaHYHVWJa/S2btfhnJGXJkjx32/QAAAAAAQHyKeZC1adMm\nlZeXq7S0VMuWLdOOHTtOen5zc7PuvPNOzZ8/X6WlpfriF7+ot99+O/i6ZVl64IEHtGDBAs2aNUvX\nXnutKioqov02EKYdeyK4rfDIXskMyDmW+VgAAAAAAIxmMQ2ytmzZonXr1unmm2/W5s2bVVxcrJUr\nV6qhoWHA830+n6677jpVV1froYce0p///GetXr1aeXnHwpDHHntMTzzxhNasWaNnnnlGKSkpWrly\npXw+30i9LQyi22/qw4oGJXmcmj4he8j3sSxT/qqP5Xt/iyS2FQIAAAAAMNq5YvnFH3/8cV155ZVa\nunSpJGnt2rV69dVXtXnzZn3zm9/sd/7vfvc7eb1ePf3003K73ZKkCRMmBF+3LEu//vWvddNNN2nR\nokWSpPvuu0/z58/X1q1bdemll47Au8JgPq5slK/bVNlp+XK7wstSLcuSWbtP3Xvfln/f27LaGnte\ncHrkmjgrCtUCAAAAAIB4EbMgy+fzaefOnbrxxhuDxxwOh+bPn6/t27cPeM3WrVs1Z84crVmzRq+8\n8ory8vK0dOlSXXPNNTIMQwcPHlRtba3OO++84DUZGRmaPXu2tm/fPuQgy+FI7O1qvfXHy/v4176e\nFXezp+WHXVPHX5+Qb+fLR39nyDluptzTzpF7yllypGREuNLRId76j5FD7+2N/tsXvbc3+m9f9N6+\n6L292bH/MQuyGhsbFQgElJ+f3+d4Xl7eCWdaVVZWatu2bVqyZIkee+wx7dmzR3fddZcMw9A111yj\n2tpaSRrwnr2vhcvlcigvL31I18abnJy0WJcgy7L0wf6eIOvCz01SXlZKyNf66g+peedWOVLSlbNw\nmdJmzJcrIydapY468dB/xAa9tzf6b1/03t7ov33Re/ui9/Zmp/7HdGthuCzLUkFBgdasWSOn06mS\nkhJVVlbqqaee0jXXXBOVr+n3m/J6O6Jy75HicBjKyUlTY2ObTNOKaS27K5tUVdemU8ZkSP6A6utb\nQ762fevTkix5Zi2W/9QL1OyTFMb1dhVP/cfIovf2Rv/ti97bG/23L3pvX/Te3hK1/5mZKXK7nUO6\nNmZBVk5OjpxOp+rq6vocr6+vV0FBwYDX5Ofny+12y+k89manTp2qqqoqSQpeV1dX12cAfH19vc44\n44wh15pI3wwnY5pWzN/LC299Kkm6eO6EsGoxvTXq/mSblJQm18yLYv4+ElE89B+xQe/tjf7bF723\nN/pvX/Tevui9vdmp/zF7aqHH41FJSYnefPPN4DHTNLVt2zbNmTNnwGvKysr06aefyjTN4LEDBw5o\n7NixknoGvxcUFPS5Z2trq95///0T3hMjp6axXf/cXausNI/OnlkU1rW+7c9LlilP6RdkeELfjggA\nAAAAAEaPmAVZknTdddfp6aef1ubNm7V3716tWbNGnZ2dWrJkiSTp9ttv109/+tPg+cuXL1djY6Pu\nvfde7d+/Xy+99JIef/xxfe1rX5MkGYahr3/963r44Yf18ssv6+OPP9btt9+uMWPGqLy8PCbvEcf8\nv3cPypJUfub4sJ5WaLbWq3v33yR3ijwli6JXIAAAAAAAiGsxnZG1ePFiNTQ06MEHH1Rtba1mzpyp\n9evXKzc3V5JUVVUlh+NY4DF+/HitX79e69at05NPPqmxY8fqhhtu0IoVK4LnfOtb31JHR4dWr14t\nr9eruXPn6rHHHpPH4xnx94dj2ju79caOKrldDl1YNj6sa33bt0hmQJ7Zi2Qk2WeAHQAAAAAA6Muw\nLMsemyiHqLs7oKam9liXMSwOh6G8vHTV17fGbM/sn9+q0G9f2asL54zT1y+dEfJ1ZnuT2p68TXK4\nlL78/8pIHh1PkBxJ8dB/xAa9tzf6b1/03t7ov33Re/ui9/aWqP3Pzk4d8rD3mG4thD34A6Zeeveg\nJOnzn5sY1rW+9/8sBfzynF5OiAUAAAAAgM0RZCHq3v24Ro0tXZo1NU9j80LfGmh2eNX94SuS0yP3\nrEujWCEAAAAAAEgEBFmIKsuy9OLblZKkL4S5Gqt7xwtSwCf3zAvlSMmMRnkAAAAAACCBEGQhqj45\n2KwD1S2aUJCumZNzQr7OMgPq3vW6ZDjlmf3FKFYIAAAAAAASBUEWourFd3pWY11y9kQZhhHydYGq\nj2V1tco54XQ50kIPwAAAAAAAwOjlinUBGJ26fAH97vW9em93rbLSPDp7ZlFY1/v3vytJck05Kxrl\nAQAAAACABESQhYj7+NNGPb5ll2qaOpSa5NK1X5whtyv0xX+Wacq//x+SYcg1uSyKlQIAAAAAgERC\nkIWI6fT59eyre7X1vUOSpLLT8nX1JcXKTk8K6z6BI5/I6miWc9xMhrwDAAAAAIAggixERGNLl+7d\n9A/VNnUqLdmlFV+YrnNmFoU1F6sX2woBAAAAAMBACLIwbJZl6fEtH6m2qVOzpubpusUzlZXmGeK9\njm4rlCHXlLmRLRQAAAAAACQ0giwM26vbD+uD/Q0am5eqm/7tDHncziHfy6zdL6utQc4x0+VIzY5g\nlQAAAAAAINGFPoEbGMCRxnY9vfUTOQxDKy8/fVghliR17+vdVshqLAAAAAAA0BdBFobMNC1teP4j\n+bpN/Z/zTtGUscMbzG5ZFvOxAAAAAADACRFkYcheePtT7TnUrFPGZOiyeZOHfT+z/lNZLbVyFJwq\nR3peBCoEAAAAAACjCUEWhqSyplWbX98nl9OhlZefLpdz+N9KrMYCAAAAAAAnQ5CFsHX7TT323IcK\nmJauuHCqxuWnReS+vUGW+1SCLAAAAAAA0B9BFsL2+vuHdbC2VTMmZWvRWRMics9A4yGZTVVy5E2U\nI7MwIvcEAAAAAACjC0EWwuIPmHrhrQpJ0v938WlyGEZk7ht8WuHnInI/AAAAAAAw+hBkISxvfXhE\n9d4uzZqap0lFGRG7L/OxAAAAAADAYAiyEDLTsrTl7z2rsRafO/ynFAbv21ovs6FSjqwxcuaMi9h9\nAQAAAADA6EKQhZBt/6ROVfXtmjYhS9MnZkfsvv5P35ckOSfNjtg9AQAAAADA6EOQhZBYlqU/betZ\njXVZBFdjSceCLBdBFgAAAAAAOAmCLIRkV0Wj9ld5NaEgXbOm5kXsvpbfp8ChjyR3spxjpkfsvgAA\nAAAAYPQhyEJI/tQ7G2veJBkRelKhJAUOfyQFfHJNOEOG0xWx+wIAAAAAgNGHIAuD2l/l1YcHGlWQ\nnazPzSiM6L3ZVggAAAAAAEJFkIVBbTk6G+uL50yW0xG5bxnLso4Nep9YGrH7AgAAAACA0YkgCydV\nVd+m93bXKivNo/NKx0T03mbjYVmt9XIUTJEjNXJPQQQAAAAAAKMTQRZO6l/7GmRJumDOOLldzoje\nO7itcOKsiN4XAAAAAACMTgRZOKnm1i5J0pi81IjfO1DJfCwAAAAAABA6giycVHObT5KUleqJ6H2t\nrjYFqj+RkZIpR8EpEb03AAAAAAAYnQiycFLeo0FWZnpSRO/rP/iBZJlyTpwlw+DbEAAAAAAADI4E\nAScVXJGVFtkVWcH5WGwrBAAAAAAAISLIwkk1t/nkdBhKS3ZF7J6WaSpQ+S/JcMo1oSRi9wUAAAAA\nAKMbQRZOyDQttbT7lJXukWEYkbtv7T5ZnS1yjp0uwxP5IfIAAAAAAGB0IsjCCbW0+2RZUdhWWLlD\nkuSaNCui9wUAAAAAAKMbQRZO6Nh8rAgPej86H8vJfCwAAAAAABAGgiycUG+QlRnBFVn+6t0y6ypk\nZBbJkTU2YvcFAAAAAACjH0EWTqi5NbJPLLRMU11/+19JUtLcL0d07hYAAAAAABj9CLJwQs1tXZIi\ntyKre9drMus/laNomlzT5kXkngAAAAAAwD4IsnBC3rZuSZFZkWV1tsr3zu8kGUo+7ypWYwEAAAAA\ngLARZOGEeldkZaUPP8jq+sdmWV2tcs+4QM78U4Z9PwAAAAAAYD8EWTghb1tkZmQFGirV/eFWyZMq\nz+e+EonSAAAAAACADRFk4YQi8dRCy7LU9bdNkmUp6awlcqRkRqo8AAAAAABgMwRZOKHmVp+SPE4l\ne1xDvod//zsKVO2SI2e83KeXR7A6AAAAAABgN0NPKDCqdfsDau/yqzAnZdBzTW+Nuve+LX/FP6Xu\njr6vtTZIkpLmr5DhcEalVgAAAAAAYA8EWRhQ8yDzsczWBvn3va3uvW/JrN1/0nu5pp8n1/jTI14j\nAAAAAACwF4IsDOhkQVag7oDaN98tWQFJkpFZJPfUs+WaerYcGYV9TzYkw5UU9XoBAAAAAMDoR5CF\nAXlbe4Os/iFUoPoTyQrIOWl2zwD3vMkyDGOkSwQAAAAAADZDkIUBNbf3PrHQ3e81s7lakuSevkDO\n/FNGsiwAAAAAAGBjPLUQAwquyErvvyLLbD4iSXJkjRnRmgAAAAAAgL0RZGFAvTOyMgeYkXUsyCrs\n9xoAAAAAAEC0EGRhQCca9m4FumW11slIy2WIOwAAAAAAGFEEWRhQc1uXpP5BlumtlSxLjqyiWJQF\nAAAAAABsjCALA2puHXhroRXcVkiQBQAAAAAARhZBFvqxLEveNp/SU9xyOft+i/Q+sZBB7wAAAAAA\nYKQRZKGfTl9APr/Zb1uhdPygd1ZkAQAAAACAkUWQhX5O/sRCVmQBAAAAAIDYIMhCP97eJxamDxBk\neY9IhiEjo2CkywIAAAAAADZHkIV+giuyUj8z6L27S1Zbo4yMAhlOVyxKAwAAAAAANkaQhX6aW7sk\n9V+RZXqZjwUAAAAAAGKHIAv99K7I+uywd+ZjAQAAAACAWCLIQj/HgqykPseDTyzMZEUWAAAAAAAY\neQRZ6Mc72IqsbFZkAQAAAACAkUeQhX6aW48Oe//sjCxWZAEAAAAAgBgiyEI/zW1dchiG0lPcfY5b\nzUckh0tGel6MKgMAAAAAAHZGkIU+TMtSS3u3MtPcchhG8LjV1Sars0WOzEIZDr5tAAAAAADAyCOR\nQB9tHd0KmNaJB71nsa0QAAAAAADEBkEW+uh9YmHmCQa9GwRZAAAAAAAgRgiy0EfzCZ9Y2LsiiycW\nAnjiF8YAACAASURBVAAAAACA2CDIQh/eo08szDrREwtZkQUAAAAAAGIk5kHWpk2bVF5ertLSUi1b\ntkw7duw44bm///3vVVxc3OdXaWlpn3P+4z/+o9853/zmN6P9NkaNE24t9LIiCwAAAAAAxJYrll98\ny5YtWrdundauXavZs2dr48aNWrlypV544QXl5uYOeE12draef/754O+N456s1+uiiy7S3XffHfy9\nx+Ppdw4G1tzWJanv1kLLsmQ2VUuuJBmp2bEqDQAAAAAA2FxMV2Q9/vjjuvLKK7V06VJNmzZNa9eu\nVVJSkjZv3nzS6woKCoK/8vPz+73u8Xj6nJOVlRWttzDqDDQjy+pskbo75MgqHDA4BAAAAAAAGAkx\nW5Hl8/m0c+dO3XjjjcFjDodD8+fP1/bt2094XWtrqy688EJZlqWSkhJ973vf07Rp0/qcs23bNs2b\nN0+ZmZmaP3++Vq1apezsoa8kcjgSO7zprT+U9+E9GmTlZCQHz+/dVujMGpPwfxZ2FE7/MbrQe3uj\n//ZF7+2N/tsXvbcvem9vdux/zIKsxsZGBQKBfiuq8vLyVFFRMeA1U6ZM0T333KPi4mK1tLToV7/6\nlZYvX67nn39eRUU9Q8gXLlyoz3/+85owYYIqKyt1//336/rrr9eTTz4phyP8BWgul0N5eenhv8E4\nlJOTNug5bZ1+SdKUSTlKTXZLkloONqlNUtqYicodJX8WdhRK/zE60Xt7o//2Re/tjf7bF723L3pv\nb3bqf0xnZIWrrKxMZWVlfX6/ePFi/fa3v9Utt9wiSbrsssuCr/cOe1+0aJHeffddnX322WF/Tb/f\nlNfbMfziY8jhMJSTk6bGxjaZpnXScxuaO+VxOdTe2qmOo/OyOg8dkCR1JeWpvr412uUiwsLpP0YX\nem9v9N++6L290X/7ovf2Re/tLVH7n5mZIrfbOaRrYxZk5eTkyOl0qq6urs/x+vp6FRQUhHQPt9ut\nmTNnnnAFlyRNnDhROTk5qqioGFKQJSmhvhlOxjStk74Xf8BUS0e38rOSZVk9Q94lKdDUs7XQyCgc\nNX8WdjRY/zF60Xt7o//2Re/tjf7bF723L3pvb3bqf8yGvXs8HpWUlOjNN98MHjNNU9u2bdOcOXNC\nukcgENDu3btPGnxVV1erqalJhYWFw655tGtp75bUd9C7JJnN1ZIkI3vMiNcEAAAAAADQK6ZbC6+7\n7jrdcccdKikp0axZs7Rx40Z1dnZqyZIlkqTbb79dRUVFuvXWWyVJDz30kObMmaPJkyfL6/Vqw4YN\nqqqq0hVXXCFJamtr00MPPaRLLrlE+fn5qqys1E9+8hNNmTJF8+bNi9n7TBTNR7cSZh7/xELLlNlc\nI3lSZSQxHwsAAAAAAMROTIOsxYsXq6GhQQ8++KBqa2s1c+ZMrV+/Xrm5uZKkqqqqPgPavV6vfvjD\nH6q2tlZZWVk644wz9PTTT+vUU0+VJDmdTu3evVt/+MMf1NLSosLCQi1YsECrVq2Sx+MZsAYc09za\n88TCrPSk4DGrrVEK+OTInSDDsM9TEAAAAAAAQPyJ+bD3q666SlddddWArz3xxBN9fv+DH/xAP/jB\nD054r+TkZG3YsCGi9dlJc9vRIOu4FVlmc898LEdWUUxqAgAAAAAA6BWzGVmIPwMGWd4aSQRZAAAA\nAAAg9giyENTU2jMjKyv9uBlZbY2SJEdabkxqAgAAAAAA6EWQhaCmlp4gKyfjuBlZ7c2SJCM1KyY1\nAQAAAAAA9CLIQlBDMMhKDh6zOgiyAAAAAABAfCDIQlBTS5ecDkMZqe7gMbN3RVYKQRYAAAAAAIgt\ngixIkvwBU942n7LTPXIYRvB4z4osQ0ZKZuyKAwAAAAAAEEEWjvK2+WRJyj5+PpZlyWpvlpGSIcPh\njF1xAAAAAAAAIsjCUQPNx5KvXTL9bCsEAAAAAABxgSALko57YmH6sRVZZnuTJAa9AwAAAACA+ECQ\nBUlSY3BF1nFbCxn0DgAAAAAA4ghBFiRJja09QVZ2hid4rGfQu+RgRRYAAAAAAIgDBFmQdGxFVu5x\nM7KCK7IIsgAAAAAAQBwgyIKkY0HW8U8tNNlaCAAAAAAA4ghBFiQdP+z9uK2FDHsHAAAAAABxhCAL\nsixLja1dSk9xy+1yHjve4ZVEkAUAAAAAAOIDQRbU1ulXt99UdnpSn+O9M7IcqdmxKAsAAAAAAKAP\ngiwcG/Se+Zkgq6NZcrold0osygIAAAAAAOiDIAvHBr0ftyLLMv2yOltkpGbJMIxYlQYAAAAAABBE\nkAU1tR4d9H7cEwutjhZJPLEQAAAAAADED4IsqMHbKekzQdbRJxY6GPQOAAAAAADiBEEWBl6RdXTQ\nu8GgdwAAAAAAECcIsqDGFp8kKee4GVlmx9Egi62FAAAAAAAgThBkQY0tPVsLswdckUWQBQAAAAAA\n4gNBFtTY0iW3y6G0ZFfwWG+Q5WBFFgAAAAAAiBMEWTbn6w6ordOvnIwkGYYRPG51sCILAAAAAADE\nF4IsmwsOej9uPpYkmUefWkiQBQAAAAAA4gVBls01tvR/YqF03IyslMwRrwkAAAAAAGAgBFk21xtk\n9Rn0blk9WwuT0mQ43bEqDQAAAAAAoA+CLJtrbB1gRVZ3p+T3ycG2QgAAAAAAEEcIsmwuuLXwuBlZ\nwUHvPLEQAAAAAADEEYIsmxtoRpbZzhMLAQAAAABA/CHIsrmmAYKs4KD31OyY1AQAAAAAADAQgiyb\na2ztkiEpM80TPNa7tdDB1kIAAAAAABBHCLJszDQtNbf6lJnukct57FvBYmshAAAAAACIQwRZNuZt\n9ylgWn0GvUvHzchiRRYAAAAAAIgjBFk2NtCgd+m4pxayIgsAAAAAAMQRgiwb6x30nv3ZIKu9SZLk\nYNg7AAAAAACIIwRZNtbY2hNk5fYLspolh1NKSo1FWQAAAAAAAAMiyLKx3q2F2cfNyLJMU1anV0ZK\nlgyDbw8AAAAAABA/SCpsbKAZWVZni2RZzMcCAAAAAABxhyDLxgYMsnoHvadkxqQmAAAAAACAEyHI\nsrGm1gG2Frb3BFkMegcAAAAAAPGGIMvGGlq6lJLkVEqSK3is94mFbC0EAAAAAADxhiDLpjq6/Ory\nBfqsxpIkM7i1kCALAAAAAADEF4IsmxpoPpZ0bGshK7IAAAAAAEC8Iciyqcaj87Fy0gcOshysyAIA\nAAAAAHGGIMumGr1Hg6zMzwRZHazIAgAAAAAA8Ykgy6YGW5FFkAUAAAAAAOINQZZNNR2dkZX9mRlZ\nZnuz5E6R4Uoa6DIAAAAAAICYIciyqdqmDklSQVZK8Jjl75K6O1iNBQAAAAAA4hJBlk3VNB4NsrKP\nC7LavZIkB0EWAAAAAACIQwRZNuQPmKpr7lRWukdJHmfweHDQO08sBAAAAAAAcYggy4bqvZ0yLUtF\nx63Gko7OxxKD3gEAAAAAQHwiyLKh2qPbCgtzUvsct9qbJBFkAQAAAACA+ESQZUNHgkHWcfOxLEuB\nwx9JkhypOTGpCwAAAAAA4GQIsmyoZoAgy7/7Dfn3vysjLVeuyXNiVRoAAAAAAMAJEWTZUE1ju6Rj\nQZbZVKXOvz0hGYaSL75BRlJaLMsDAAAAAAAYEEGWDdU0HV2RlZ0iy+9Tx8sPS36fPHOXyDVmeoyr\nAwAAAAAAGBhBls2YpqXapg6lp7iVmuxW11vPyKyvlHPsDHnmXB7r8gAAAAAAAE6IIMtmGlu65A9Y\nKspJUfeB99S98yUZSelKLr9ehoNvBwAAAAAAEL9csS4AI6t3PtbkTL86X3tCkpR80Uo50nhSIQAA\nAAAAiG8swbGZI0fnY83tflfqapP7jC/INYmnFAIAAAAAgPhHkGUzNY09QVZOoE6S5Cm5OJblAAAA\nAAAAhIwgy2Z6g6zkrgbJcMrIyI9xRQAAAAAAAKEhyLKZmsZ2JcknZ5dXRmaBDIcz1iUBAAAAAACE\nhCDLRizLUk1Thyak9Ax8d2QVxbgiAAAAAACA0BFk2Uhzm0++blNT0zslSY6sMTGuCAAAAAAAIHQE\nWTZypKFnJdaE5DZJrMgCAAAAAACJhSDLRnoHvRc4vZJYkQUAAAAAABILQZaNHDkaZGWZTZJYkQUA\nAAAAABJLzIOsTZs2qby8XKWlpVq2bJl27NhxwnN///vfq7i4uM+v0tLSPudYlqUHHnhACxYs0KxZ\ns3TttdeqoqIi2m8jIdQ09mwtTOmql5xuGWk5Ma4IAAAAAAAgdDENsrZs2aJ169bp5ptv1ubNm1Vc\nXKyVK1eqoaHhhNdkZ2frjTfeCP565ZVX+rz+2GOP6YknntCaNWv0zDPPKCUlRStXrpTP54v224l7\nNY0dSjU65ehulyOrSIYR8xwTAAAAAAAgZDFNMh5//HFdeeWVWrp0qaZNm6a1a9cqKSlJmzdvPul1\nBQUFwV/5+fnB45Zl6de//rVuuukmLVq0SDNmzNB9992n6upqbd26NdpvJ65ZlqUjjR0a52mVJDky\n2VYIAAAAAAASiytWX9jn82nnzp268cYbg8ccDofmz5+v7du3n/C61tZWXXjhhbIsSyUlJfre976n\nadOmSZIOHjyo2tpanXfeecHzMzIyNHv2bG3fvl2XXnrpkGp1OIwhXRcvHA5D3jafOrr8mprfKZmS\nM3tMwr8vhKa3z/Tbfui9vdF/+6L39kb/7Yve2xe9tzc79j9mQVZjY6MCgUCfFVWSlJeXd8KZVlOm\nTNE999yj4uJitbS06Fe/+pWWL1+u559/XkVFRaqtrZWkAe/Z+1q4XC6H8vLSh3RtPNlV0bNdc3Ja\nh9QiZYyfrMxR8L4QupyctFiXgBih9/ZG/+2L3tsb/bcvem9f9N7e7NT/mAVZQ1FWVqaysrI+v1+8\neLF++9vf6pZbbonK1/T7TXm9HVG590hxOAxV1bVJknKtZklShytb3fWtsSwLI8ThMJSTk6bGxjaZ\nphXrcjCC6L290X/7ovf2Rv/ti97bF723t0Ttf2Zmitxu55CujVmQlZOTI6fTqbq6uj7H6+vrVVBQ\nENI93G63Zs6cGVzB1XtdXV2d8vLy+tzzjDPOGHKtifTNcCK9QVaW2dhzIKNoVLwvhM40LXpuU/Te\n3ui/fdF7e6P/9kXv7Yve25ud+h+zYe8ej0clJSV68803g8dM09S2bds0Z86ckO4RCAS0e/fuYIA1\nYcIEFRQU9Llna2ur3n///ZDvOVpV1bdJspTcVS+5k2WkZMa6JAAAAAAAgLDEdGvhddddpzvuuEMl\nJSWaNWuWNm7cqM7OTi1ZskSSdPvtt6uoqEi33nqrJOmhhx7SnDlzNHnyZHm9Xm3YsEFVVVW64oor\nJEmGYejrX/+6Hn74YU2aNEkTJkzQAw88oDFjxqi8vDxm7zMeVNW1KdPokCPgkyPnFBmGfQbBAQAA\nAACA0SGmQdbixYvV0NCgBx98ULW1tZo5c6bWr1+v3NxcSVJVVZUcjmOLxrxer374wx+qtrZWWVlZ\nOuOMM/T000/r1FNPDZ7zrW99Sx0dHVq9erW8Xq/mzp2rxx57TB6PZ8TfXzypqmvTGHfPTCxHVlGM\nqwEAAAAAAAifYVmWPTZRDlF3d0BNTe2xLmNYOnx+3Xz/67okt0KL9Zo8Z35ZSWctiXVZGCEOh6G8\nvHTV17faZs80etB7e6P/9kXv7Y3+2xe9ty96b2+J2v/s7NQhD3uP2YwsjJzaxp6nLk5I7hn4zoos\nAAAAAACQiAiybODI0SCr0OmVJDmyxsSyHAAAAAAAgCEhyLKBmqNBVlagSRIrsgAAAAAAQGIiyLKB\nmsZ2GTKV7GuQkZwhIykt1iUBAAAAAACEjSDLBjxupwrdHTJMvwxWYwEAAAAAgATlinUBiL6vff40\nGbMttfyBbYUAAAAAACBxEWTZgNPhkKujVpLkyCTIAgAAAAAAiYmthTbR3VAlSXJk88RCAAAAAACQ\nmAiybKK74bAkVmQBAAAAAIDERZBlE8EVWczIAgAAAAAACYogywasgF/+phoZqdky3MmxLgcAAAAA\nAGBICLJswGypkyxTjizmYwEAAAAAgMRFkGUDZnO1JMmRzbZCAAAAAACQuAiybMBs6gmynKzIAgAA\nAAAACYwgywbM5iOSxNZCAAAAAACQ0AiybMDyd0ky5MybGOtSAAAAAAAAhswV6wIQfcnzv6aC+Zer\nLblApmnFuhwAAAAAAIAhYUWWDTiS05U8fnqsywAAAAAAABgWgiwAAAAAAAAkBIIsAAAAAAAAJASC\nLAAAAAAAACQEgiwAAAAAAAAkBIIsAAAAAAAAJASCLAAAAAAAACQEgiwAAAAAAAAkBIIsAAAAAAAA\nJASCLAAAAAAAACQEgiwAAAAAAAAkBIIsAAAAAAAAJASCLAAAAAAAACQEgiwAAAAAAAAkBIIsAAAA\nAAAAJASCLAAAAAAAACQEgiwAAAAAAAAkBIIsAAAAAAAAJASCLAAAAAAAACQEgiwAAAAAAAAkBIIs\nAAAAAAAAJASCLAAAAAAAACQEw7IsK9ZFxDPTtBQImLEuY9jcbqe6uwOxLgMxQv/ti97bG/23L3pv\nb/Tfvui9fdF7e0vE/judDjkcxpCuJcgCAAAAAABAQmBrIQAAAAAAABICQRYAAAAAAAASAkEWAAAA\nAAAAEgJBFgAAAAAAABICQRYAAAAAAAASAkEWAAAAAAAAEgJBFgAAAAAAABICQRYAAAAAAAASAkEW\nAAAAAAAAEgJBFgAAAAAAABICQRYAAAAAAAASAkEWAAAAAAAAEgJBFgAAAAAAABICQZYNbNq0SeXl\n5SotLdWyZcu0Y8eOWJeECPvv//5vLV26VGVlZZo3b55uueUWHThwoM85V199tYqLi/v8Wr16dWwK\nRsT84he/6NfXSy+9NPh6V1eX1q5dq3POOUdlZWX69re/rfr6+hhWjEgqLy/v1//i4mKtXbtWEp/7\n0eSdd97RDTfcoAULFqi4uFivvPJKn9dD+awfPnxY//7v/67Zs2dr3rx5uu+++xQIBEbybWCITtb/\npqYm3X333brkkks0a9YsXXTRRbrnnnvU2tra5x4D/az405/+NNJvBWEa7LMfys95PvuJ62T9P3jw\n4ICf6+LiYv35z38OnsdnPzGF8u87O//d74p1AYiuLVu2aN26dVq7dq1mz56tjRs3auXKlXrhhReU\nm5sb6/IQIW+//bZWrFih0tJSBQIB3X///frGN76hLVu2KDk5OXje8uXLdfPNNwd/n5KSEotyEWEz\nZszQ+vXrg793Op3B//3jH/9Yr732mn7+858rIyNDd999t77zne9o06ZNsSgVEfbss8/2+Y+RTz75\nRNddd12fMJPP/ejQ3t6u4uJiLV26VLfccku/1wf7rAcCAV1//fXKz8/XU089pZqaGt1xxx1KSkrS\nqlWrRvrtIEwn639NTU2wn9OmTdOhQ4e0Zs0a1dXV6Wc/+1mfc++77z7Nnz8/+PvMzMwRqR9DN9hn\nXzr5z3k++4ntZP0fO3as3njjjT7Hnn76aW3YsEHnn39+n+N89hNPKP++s/Xf/RZGtSuuuMK66667\ngr8PBALWggULrPXr18ewKkRbfX29NX36dOsf//hH8NhVV11l3XvvvTGsCtHw4IMPWkuWLBnwNa/X\na5WUlFgvvPBC8NiePXus6dOnWzt27BipEjGCfvSjH1mLFi2yTNO0LIvP/Wg1ffp0a+vWrcHfh/JZ\nf/XVV62ZM2datbW1wXN+85vfWGeddZbl8/lGrngM22f7P5AtW7ZYpaWlViAQCOs6xLeBejjYz3k+\n+6NHKJ/hL3/5y9Z//ud/hn0d4t9n/31n97/72Vo4ivl8Pu3cuVPnnXde8JjD4dD8+fO1ffv2GFaG\naGtpaZEkZWVl9Tm+efNmnXPOObr88sv1s5/9TJ2dnbEoDxG2b98+LViwQBdffLG+//3vq7q6WpL0\nwQcfqLu7u8/PgKlTp2rcuHH8DBiFfD6f/vjHP2rp0qUyDCN4nM/96BfKZ3379u2aMWOG8vPzg+cs\nWLBAXq9X+/btG/GaEV2tra3KyMiQw9H3P/XvvPNOnXvuufrqV7+qzZs3x6g6RNrJfs7z2bePDz74\nQB999JGuuOKKfq/x2U98n/33nd3/7mdr4SjW2NioQCDQ5xtXkvLy8lRRURGjqhBtlmVp3bp1Ovvs\nszV16tTg8csvv1zjxo1TYWGhdu3apZ/+9Kc6cOCAHnjggRhWi+GaNWuW1q1bpylTpqi2tla//OUv\ntWLFCj333HOqq6tTcnKy0tPT+1yTl5enurq6GFWMaHnppZfU0tKiJUuWBI/xubeHUD7rdXV1ysvL\n6/N6738f1NXVqbi4eGSKRdQ1Njbq4Ycf1pVXXtnn+He+8x2de+65SklJ0RtvvKHVq1ervb1dK1as\niFGliITBfs7z2bePZ599VlOnTtWZZ57Z5zif/cQ30L/v7P53P0EWMMrcdddd2r17t5588sk+x4//\nD9ri4mIVFhbq2muv1aFDhzR+/PiRLhMRcsEFFwT/94wZMzR79mxddNFF+stf/iKXix/xdvK73/1O\n559/voqKioLH+NwD9tLa2qrrr79ep512mm666aY+rx0/Q+n0009XR0eHNmzYwD9mExw/5yFJnZ2d\nev755/t97iU++6PBif59Z2dsLRzFcnJy5HQ6+628qK+vV0FBQYyqQjTdfffd2rp1qzZu3NjnH7MD\nmT17tiTp008/HYnSMEIyMzN1yimnqKKiQvn5+ers7Oz35Kr6+vp+KzWR2A4dOqQ333xzwO0Ex+Nz\nPzqF8lnPz8/v9ySj3v8+4OfB6NDa2qqVK1cqNTVVv/jFLwb9PzNmz56tw4cPy+/3j1CFGAmf/TnP\nZ98eXnjhBXV2durf/u3fBj2Xz35iOdG/7+z+dz9B1ijm8XhUUlKiN998M3jMNE1t27ZNc+bMiWFl\niDTLsnTXXXfpxRdf1MaNGzVx4sRBr/noo4/+//buPCTK7Y/j+Ke8g5pp45YVES2WbWqSWFHSIpbZ\nP2UalFGmBFJaGFnabkUa4UK2KQQtatDiHy0oRSsZlRUhlFpUtBqKrZYZpb8/fty5d37+bnpvpfex\n9wuGcZ5znsdz5vF7nPnOmfNIEknNDubDhw96+vSp3N3dNXz4cJlMJqsx4OHDh3rx4gVjQAdTWFgo\nV1dXTZgw4Zv1iPuOqTWxPmLECFVUVOjVq1eWOleuXJGTk5P69+/f5m3Gj1VXV6eYmBiZTCbt3r1b\ntra2Le5TXl4uZ2dnZu92MP87zhP7v4Zjx45p0qRJrboqPbFvDC29v/vV//fz19vBLViwQCtXrtSw\nYcPk4+Oj/fv369OnT1ZrqMD4UlJSdPLkSe3atUsODg6qqamRJDk6OsrOzk5PnjzRiRMnNH78eJnN\nZlVWVio1NVWjR4+Wp6dnO7ce32Pr1q2aOHGievXqperqamVnZ8vGxkahoaFydHTUzJkzlZqaKicn\nJ3Xt2lWbN2+Wv7+/vL2927vp+EEaGxtVWFio6dOnW70oJe47lg8fPljNpHv27JnKy8vl5uYmd3f3\nFmN93LhxGjBggBITE5WYmKiamhplZWUpMjJSJpOpvbqFVvrW+be3t1d0dLTq6+u1bds21dXVWT6h\nd3FxkY2Njc6dO6fa2lr5+vrK1tZWJSUlysnJ0cKFC9urS2ilb537+vr6Fsd5Yt/YWhr7Jenx48cq\nLS1Vbm5us/2JfeNq6f1da17nd+T479TU1NTU3o3Az5WXl6e9e/eqpqZGQ4YM0dq1a+Xj49PezcIP\n9FcL9aWmpiosLExVVVVKTEzU/fv39fHjR/Xs2VOTJ09WbGxsswUCYSwJCQkqLS3Vmzdv5OLiIn9/\nfyUkJFg+tWloaFBaWppOnTqlz58/KzAwUOvXrzf8dGL84fLly4qJiVFxcbH69etn2U7cdyzXrl3T\nvHnzmm2Pi4tTfHx8q2L9+fPn2rBhg65fvy57e3vNmDFDy5cvl42NTVt2Bf/At85/QEDA/y2TpLNn\nz6p37966dOmSMjIyLBf76dOnjyIjIxUREWF1lVP8+3zr3IeHh7dqnCf2jaulsV+SMjIydPz4cZ07\nd67ZlUqJfeNq6f2d1LrX+R01/klkAQAAAAAAwBBYIwsAAAAAAACGQCILAAAAAAAAhkAiCwAAAAAA\nAIZAIgsAAAAAAACGQCILAAAAAAAAhkAiCwAAAAAAAIZAIgsAAAAAAACGQCILAADgF3bt2jV5eXnp\n3r177d0UAACAFpHIAgAAAAAAgCGQyAIAAAAAAIAhkMgCAABoBzdu3NDcuXPl6+urUaNGac2aNaqr\nq5MkFRYWysvLS2VlZZozZ458fHw0ZcoUnTlzptlx8vLyNHnyZA0fPlzBwcHat29fszoVFRWKjY2V\nv7+//Pz8FB4erpKSEqs6r1+/1pIlS+Tn56egoCDl5+f/lH4DAAB8DxJZAAAAbezmzZuKioqSm5ub\ntm/fruTkZF28eFGrVq2yqpeQkKCgoCBlZ2dr0KBBWrp0qSoqKizlhw8f1qZNmzRp0iTt2bNHISEh\nSktLU25urqXOgwcPNHv2bFVXVyslJUU7duxQcHCwqqqqrH7X2rVrNXjwYO3YsUMBAQHauHGjysrK\nfu4TAQAA8Df91t4NAAAA+NWkp6fLz89PWVlZlm0eHh6KioqyWnQ9IiJCMTExkqTAwECFhoYqJydH\nmZmZamxsVHZ2tsLCwpSUlCRJGjdunN6/f6+cnBzNnz9ftra22rlzpxwdHVVQUCA7OztJ0tixY5u1\nadq0aVq0aJEkKSAgQOfPn9fp06fl4+Pz054HAACAv4sZWQAAAG2ovr5et2/f1tSpU/XlyxfLbeTI\nkTKZTLpz546lbnBwsOXnzp07KygoyDJL6uXLl6qurlZISIjV8UNDQ1VXV6fKykpJ0tWrVxUaGmpJ\nYv2VPye3TCaT+vbtq5cvX353fwEAAH4kZmQBAAC0oXfv3unr169KSUlRSkpKs/Kqqir16NFDxnjj\nugAAAlJJREFUkuTi4mJV5urqqpqaGkmy3Lu6ujarI0lv376VJL1580bu7u4ttsvJycnqsclk0ufP\nn1vTJQAAgDZDIgsAAKANOTo6qlOnToqLi9P48eOblXfv3t2yEPurV6/k7OxsKautrbUkpX6/r62t\ntdr/98fdunWTJJnNZkvSCwAAwOj4aiEAAEAb6tKli0aMGKFHjx7J29u72c3Dw8NS989XKWxsbNTZ\ns2cta1b16NFD3bt3V3FxsdXxi4qK1LVrV3l5eUmSxowZo6KiIjU0NLRB7wAAAH4uZmQBAAC0seXL\nlysqKkqdO3fWlClT5ODgoKqqKl24cEEJCQmWekeOHJHJZNLAgQN19OhRPXnyRBkZGZL+u2ZWfHy8\n1q1bJ7PZrLFjx6q0tFSHDh3SsmXLZGtrK0lavHixwsPDFRkZqejoaJnNZt29e1dms1nh4eHt0n8A\nAIB/ikQWAABAG/P391d+fr62b9+uFStWqLGxUb169VJgYKDc3Nws9TIzM7VlyxZlZWWpZ8+eyszM\n1NChQy3ls2bNUkNDgw4cOKCDBw/Kw8NDSUlJioqKstTp37+/CgoKlJ6ertWrV0uSPD09tWzZsjbr\nLwAAwI/Sqampqam9GwEAAIA/FBYWKjk5Wbdu3ZKDg0N7NwcAAOBfgzWyAAAAAAAAYAgksgAAAAAA\nAGAIfLUQAAAAAAAAhsCMLAAAAAAAABgCiSwAAAAAAAAYAoksAAAAAAAAGAKJLAAAAAAAABgCiSwA\nAAAAAAAYAoksAAAAAAAAGAKJLAAAAAAAABjCfwBeby8gDQjn+AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OYKf_EBcnow",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "67c1f721-8957-4385-c071-e6e4f11344b4"
      },
      "source": [
        "y_predict = np.round(model.predict(X_test_norm))\n",
        "y_predict = [i[0] for i in y_predict.tolist()]\n",
        "sum(y_predict == y_test)/len(y_test)"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7548387096774194"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYgBdDiRdA5J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "774f0d26-98b9-4258-fdfc-17ccd04078c5"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "target_names = ['negative', 'positive']\n",
        "C = confusion_matrix(y_test,y_predict) \n",
        "C = C / C.astype(np.float).sum(axis=1)*100\n",
        "sns.heatmap(C, annot=True, fmt=\".2f\",cmap=\"GnBu\",xticklabels=target_names, yticklabels=target_names)\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.show()"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAESCAYAAAAL5+VQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAw0wt1TjZqrQAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmc2KoxrQveg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "436d792a-6b57-4e30-db92-d4f708aede9c"
      },
      "source": [
        "y_predict = np.round(model.predict(X_blind_norm))\n",
        "y_predict = [i[0] for i in y_predict.tolist()]\n",
        "sum(y_predict == y_blind)/len(y_blind)"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7122302158273381"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzyIwwebdLIj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "9071cb51-c38e-4994-fa0f-2a2930cbee87"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "target_names = ['negative', 'positive']\n",
        "C = confusion_matrix(y_blind,y_predict) \n",
        "C = C / C.astype(np.float).sum(axis=1)*100\n",
        "sns.heatmap(C, annot=True, fmt=\".2f\",cmap=\"GnBu\",xticklabels=target_names, yticklabels=target_names)\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.show()"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAESCAYAAAAL5+VQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9o3-KtffeAj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "917b3492-a6fe-403b-90e7-6a5b59dc6393"
      },
      "source": [
        "train_acc[124]"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7858333587646484"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEf2sylQnSgJ",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning for imbalanced problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s8Ei8P5nOiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "model_ibp = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(16, activation='relu', input_shape=(18,)),\n",
        "  tf.keras.layers.Dense(8, activation='relu'),\n",
        "  tf.keras.layers.Dense(4, activation='relu'),\n",
        "  tf.keras.layers.Dense(2, activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')    \n",
        "])\n",
        "model_ibp.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zVmN2dupNv_",
        "colab_type": "code",
        "outputId": "a2ccd404-71d7-4f3c-c29a-a9fb7d6674a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "model_ibp.fit(X_train_norm, y_train, epochs=4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "400/400 [==============================] - 0s 354us/sample - loss: 0.6932 - acc: 0.4625\n",
            "Epoch 2/4\n",
            "400/400 [==============================] - 0s 48us/sample - loss: 0.6923 - acc: 0.5075\n",
            "Epoch 3/4\n",
            "400/400 [==============================] - 0s 42us/sample - loss: 0.6914 - acc: 0.5000\n",
            "Epoch 4/4\n",
            "400/400 [==============================] - 0s 41us/sample - loss: 0.6873 - acc: 0.5475\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5e8cd08ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyi6auIKqIg_",
        "colab_type": "code",
        "outputId": "d8706876-552e-46c5-aa0b-455d9c27a8f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "model_ibp.fit(X_train_norm2, y_train2, epochs=4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "400/400 [==============================] - 0s 61us/sample - loss: 0.6802 - acc: 0.5625\n",
            "Epoch 2/4\n",
            "400/400 [==============================] - 0s 42us/sample - loss: 0.6698 - acc: 0.6150\n",
            "Epoch 3/4\n",
            "400/400 [==============================] - 0s 50us/sample - loss: 0.6608 - acc: 0.6225\n",
            "Epoch 4/4\n",
            "400/400 [==============================] - 0s 49us/sample - loss: 0.6514 - acc: 0.6500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5e8d1acba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBD2y8kOqM2e",
        "colab_type": "code",
        "outputId": "36808a9b-6ee6-4eec-a790-3fe9c2e9fd70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "model_ibp.fit(X_train_norm3, y_train3, epochs=4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "400/400 [==============================] - 0s 54us/sample - loss: 0.6376 - acc: 0.6825\n",
            "Epoch 2/4\n",
            "400/400 [==============================] - 0s 48us/sample - loss: 0.6269 - acc: 0.6825\n",
            "Epoch 3/4\n",
            "400/400 [==============================] - 0s 43us/sample - loss: 0.6171 - acc: 0.7250\n",
            "Epoch 4/4\n",
            "400/400 [==============================] - 0s 50us/sample - loss: 0.6090 - acc: 0.7300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5e8c821518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5caUhBUqT5q",
        "colab_type": "code",
        "outputId": "36d879e3-45e7-425a-c8d1-6d30fb888707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "model_ibp.fit(X_train_norm4, y_train4, epochs=4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "400/400 [==============================] - 0s 73us/sample - loss: 0.6120 - acc: 0.7325\n",
            "Epoch 2/4\n",
            "400/400 [==============================] - 0s 48us/sample - loss: 0.6069 - acc: 0.7525\n",
            "Epoch 3/4\n",
            "400/400 [==============================] - 0s 47us/sample - loss: 0.6024 - acc: 0.7375\n",
            "Epoch 4/4\n",
            "400/400 [==============================] - 0s 45us/sample - loss: 0.5987 - acc: 0.7350\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5e8c846b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4NCbG82qXdj",
        "colab_type": "code",
        "outputId": "d6963ebf-e8c1-4fc9-def9-cc973752fac4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "model_ibp.fit(X_train_norm5, y_train5, epochs=4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "400/400 [==============================] - 0s 55us/sample - loss: 0.6070 - acc: 0.7150\n",
            "Epoch 2/4\n",
            "400/400 [==============================] - 0s 52us/sample - loss: 0.6035 - acc: 0.7500\n",
            "Epoch 3/4\n",
            "400/400 [==============================] - 0s 45us/sample - loss: 0.5960 - acc: 0.7525\n",
            "Epoch 4/4\n",
            "400/400 [==============================] - 0s 52us/sample - loss: 0.5922 - acc: 0.7475\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5e8d20bd68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvNtRPOETQ8P",
        "colab_type": "code",
        "outputId": "fd9c4478-7816-4cfd-c659-3e89a7b05142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "model_ibp.fit(X_train_norm6, y_train6, epochs=4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "400/400 [==============================] - 0s 61us/sample - loss: 0.5765 - acc: 0.7525\n",
            "Epoch 2/4\n",
            "400/400 [==============================] - 0s 48us/sample - loss: 0.5708 - acc: 0.7550\n",
            "Epoch 3/4\n",
            "400/400 [==============================] - 0s 48us/sample - loss: 0.5670 - acc: 0.7600\n",
            "Epoch 4/4\n",
            "400/400 [==============================] - 0s 50us/sample - loss: 0.5599 - acc: 0.7700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5e8cd089b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yas7ScxjqcFa",
        "colab_type": "code",
        "outputId": "6e2fc982-0761-46c1-f576-63f62491ccb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_predict = np.round(model_ibp.predict(X_test_norm))\n",
        "y_predict = [i[0] for i in y_predict.tolist()]\n",
        "sum(y_predict == y_test)/len(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6527777777777778"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkhW04wi8JQW",
        "colab_type": "text"
      },
      "source": [
        "# Save Data to SVM format for Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o8fycN68JQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from sklearn.datasets import dump_svmlight_file\n",
        "#dump_svmlight_file(X_train_norm, y_train, 'training.svm',zero_based=False)\n",
        "#dump_svmlight_file(X_test_norm,y_test,'test.svm',zero_based=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evjavp_N8JQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}