{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Clinical_Analysis_pmmet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "nkhW04wi8JQW"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plenoi/Clinic/blob/master/Clinical_Analysis_pmmet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWHnvx9H8JPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jntbWEiv8JPM",
        "colab_type": "text"
      },
      "source": [
        "# Read all data and set hn as index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCtTLpCL8JPM",
        "colab_type": "code",
        "outputId": "6c1c2cc1-2486-4012-eb65-ee0d5a16c11a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/plenoi/Clinic/master/ultima_all_clean.csv')\n",
        "df = df.set_index('hn')\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>parity</th>\n",
              "      <th>hiv</th>\n",
              "      <th>menopaus</th>\n",
              "      <th>disease</th>\n",
              "      <th>surgery</th>\n",
              "      <th>conization</th>\n",
              "      <th>OPDsize</th>\n",
              "      <th>appearance</th>\n",
              "      <th>stage</th>\n",
              "      <th>pchemo</th>\n",
              "      <th>Wardsize</th>\n",
              "      <th>finalhisto</th>\n",
              "      <th>nodeyiel</th>\n",
              "      <th>RHlvsi</th>\n",
              "      <th>depth</th>\n",
              "      <th>size</th>\n",
              "      <th>utmet</th>\n",
              "      <th>vgmargin</th>\n",
              "      <th>vgmet</th>\n",
              "      <th>pelvicme</th>\n",
              "      <th>pmmet</th>\n",
              "      <th>adnmet</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hn</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2631840</th>\n",
              "      <td>52</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2633481</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2634477</th>\n",
              "      <td>52</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2633633</th>\n",
              "      <td>38</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2630496</th>\n",
              "      <td>55</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         age  parity  hiv  menopaus  ...  vgmet  pelvicme  pmmet  adnmet\n",
              "hn                                   ...                                \n",
              "2631840   52       3  0.0       0.0  ...    0.0       0.0    0.0     0.0\n",
              "2633481   32       2  0.0       0.0  ...    0.0       1.0    0.0     2.0\n",
              "2634477   52       2  0.0       0.0  ...    0.0       0.0    0.0     0.0\n",
              "2633633   38       2  0.0       0.0  ...    0.0       0.0    0.0     2.0\n",
              "2630496   55       3  0.0       1.0  ...    0.0       1.0    0.0     0.0\n",
              "\n",
              "[5 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw4lB2Yj8JPP",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iO_k48uw8JPQ",
        "colab_type": "text"
      },
      "source": [
        "Check number of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1gVBPp48JPQ",
        "colab_type": "code",
        "outputId": "b95dfe3a-7245-4cde-d889-6863c9d35ee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1723, 23)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKStqK_68JPT",
        "colab_type": "text"
      },
      "source": [
        "Check any missing data in each column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q77h1QHI8JPT",
        "colab_type": "code",
        "outputId": "9c4252e1-7190-4ee8-9124-926a858dacfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "df.isnull().sum(axis=0)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age             0\n",
              "parity          0\n",
              "hiv             4\n",
              "menopaus        1\n",
              "disease         0\n",
              "surgery         0\n",
              "conization      5\n",
              "OPDsize        17\n",
              "appearance    101\n",
              "stage          24\n",
              "pchemo          1\n",
              "Wardsize      145\n",
              "finalhisto     10\n",
              "nodeyiel       12\n",
              "RHlvsi        366\n",
              "depth         489\n",
              "size          114\n",
              "utmet          98\n",
              "vgmargin       96\n",
              "vgmet          97\n",
              "pelvicme        1\n",
              "pmmet          94\n",
              "adnmet          7\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlAFFf418JPV",
        "colab_type": "text"
      },
      "source": [
        "Delete column with missing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5YSx3NY8JPW",
        "colab_type": "code",
        "outputId": "7b565e11-1c60-4829-8901-f4e7d10dd241",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "df_clean_column = df.drop(['appearance','Wardsize','RHlvsi','depth','nodeyiel','vgmargin','pelvicme','adnmet'],axis = 1)\n",
        "df_clean_column.isnull().sum(axis=0)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age             0\n",
              "parity          0\n",
              "hiv             4\n",
              "menopaus        1\n",
              "disease         0\n",
              "surgery         0\n",
              "conization      5\n",
              "OPDsize        17\n",
              "stage          24\n",
              "pchemo          1\n",
              "finalhisto     10\n",
              "size          114\n",
              "utmet          98\n",
              "vgmet          97\n",
              "pmmet          94\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyNnK21u8JPX",
        "colab_type": "text"
      },
      "source": [
        "Delete row with at least 1 missing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03MGmT4a8JPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_clean = df_clean_column.dropna(axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kUDENLG8JPZ",
        "colab_type": "text"
      },
      "source": [
        "Total Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brL_4adg8JPb",
        "colab_type": "code",
        "outputId": "2267847d-e52d-44b2-deef-98adb8afb1f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df_clean.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1555, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHcRmIb78JPc",
        "colab_type": "text"
      },
      "source": [
        "Check number of sample in pelvicme class "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QQbuaAY8JPd",
        "colab_type": "code",
        "outputId": "5e477d1a-888c-4edd-a8bb-8160f79d91a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pel_class = np.unique(df_clean['pmmet'])\n",
        "pel_class"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LctFWu_H8JPf",
        "colab_type": "code",
        "outputId": "fa731b2c-f8ff-43ac-dd0d-f52d4105a31b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pel_value = [sum(df_clean['pmmet']==pel_class[0]),\n",
        "             sum(df_clean['pmmet']==pel_class[1])]\n",
        "pel_value"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1278, 277]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VJ6Ejg48JPk",
        "colab_type": "text"
      },
      "source": [
        "Separate pelviceme dataset into data (X) and label (y)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Gdtafhe8JPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = df_clean['pmmet'].values\n",
        "X = df_clean.drop(['pmmet'],axis = 1).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFhEkk8X8JPm",
        "colab_type": "text"
      },
      "source": [
        "Randomly choose 200 samples of class 1 (positive) as training data and the rest as test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnFz3mek8JPn",
        "colab_type": "code",
        "outputId": "d2592362-1ae5-45cb-af55-b5bc09063728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import random\n",
        "random.seed(1)\n",
        "positive_index = np.where(y==1)[0]\n",
        "negative_index = np.where(y==0)[0]\n",
        "pos_train_index = random.sample(list(positive_index),200)\n",
        "pos_test_index = list(set(positive_index) - set(pos_train_index))\n",
        "\n",
        "print(\"All dataset: \"+str(len(positive_index))+\" \"+str(len(negative_index)))\n",
        "print(\"Positive test dataset: \"+str(len(pos_test_index)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All dataset: 277 1278\n",
            "Positive test dataset: 77\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb3---1e8JPo",
        "colab_type": "text"
      },
      "source": [
        "Randomly separate negative dataset into 5 parts to create 5 training datasets consisted of 200 samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpCeL8Jz8JPp",
        "colab_type": "code",
        "outputId": "fd5750e5-44de-43df-9487-8e2adbedd956",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "neg_train_index1 = random.sample(list(negative_index),200)\n",
        "neg_tmp_index = list(set(negative_index) - set(neg_train_index1))\n",
        "\n",
        "neg_train_index2 = random.sample(list(neg_tmp_index),200)\n",
        "neg_tmp_index = list(set(neg_tmp_index) - set(neg_train_index2))\n",
        "\n",
        "neg_train_index3 = random.sample(list(neg_tmp_index),200)\n",
        "neg_tmp_index = list(set(neg_tmp_index) - set(neg_train_index3))\n",
        "\n",
        "neg_train_index4 = random.sample(list(neg_tmp_index),200)\n",
        "neg_tmp_index = list(set(neg_tmp_index) - set(neg_train_index4))\n",
        "\n",
        "neg_train_index5 = random.sample(list(neg_tmp_index),200)\n",
        "neg_tmp_index = list(set(neg_tmp_index) - set(neg_train_index5))\n",
        "\n",
        "neg_train_index6 = random.sample(list(neg_tmp_index),200)\n",
        "neg_tmp_index = list(set(neg_tmp_index) - set(neg_train_index6))\n",
        "\n",
        "neg_test_index = neg_tmp_index\n",
        "print(\"Negative test dataset: \"+str(len(neg_test_index)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Negative test dataset: 78\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCzFlfc58JPq",
        "colab_type": "text"
      },
      "source": [
        "Create 5 training dataset and 1 test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDcP1OdWSYZM",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LR5IIJ58JPr",
        "colab_type": "code",
        "outputId": "9b267249-f05a-4cb2-e6bc-840595d186a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train1 = np.concatenate((X[neg_train_index1,:],X[pos_train_index,:]),axis=0)\n",
        "y_train1 = np.concatenate((y[neg_train_index1],y[pos_train_index]),axis=0)\n",
        "\n",
        "X_train2 = np.concatenate((X[neg_train_index2,:],X[pos_train_index,:]),axis=0)\n",
        "y_train2 = np.concatenate((y[neg_train_index2],y[pos_train_index]),axis=0)\n",
        "\n",
        "X_train3 = np.concatenate((X[neg_train_index3,:],X[pos_train_index,:]),axis=0)\n",
        "y_train3 = np.concatenate((y[neg_train_index3],y[pos_train_index]),axis=0)\n",
        "\n",
        "X_train4 = np.concatenate((X[neg_train_index4,:],X[pos_train_index,:]),axis=0)\n",
        "y_train4 = np.concatenate((y[neg_train_index4],y[pos_train_index]),axis=0)\n",
        "\n",
        "X_train5 = np.concatenate((X[neg_train_index5,:],X[pos_train_index,:]),axis=0)\n",
        "y_train5 = np.concatenate((y[neg_train_index5],y[pos_train_index]),axis=0)\n",
        "\n",
        "X_train6 = np.concatenate((X[neg_train_index6,:],X[pos_train_index,:]),axis=0)\n",
        "y_train6 = np.concatenate((y[neg_train_index6],y[pos_train_index]),axis=0)\n",
        "\n",
        "X_train6.shape, y_train6.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((400, 14), (400,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5hQ5N618JPt",
        "colab_type": "code",
        "outputId": "6af34a50-b15b-4fb8-8318-63b08d45d9be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_test = np.concatenate((X[neg_test_index,:],X[pos_test_index,:]),axis=0)\n",
        "y_test = np.concatenate((y[neg_test_index],y[pos_test_index]),axis=0)\n",
        "X_test.shape, y_test.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((155, 14), (155,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgKGb8HK8JPv",
        "colab_type": "text"
      },
      "source": [
        "Data normalization to range of (0 to 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmkv5Op98JPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "alltrain = np.concatenate((X_train1,X_train2,X_train3,X_train4,X_train5,X_train6),axis=0)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaler.fit(alltrain)\n",
        "X_train_norm1 = scaler.transform(X_train1)\n",
        "X_train_norm2 = scaler.transform(X_train2)\n",
        "X_train_norm3 = scaler.transform(X_train3)\n",
        "X_train_norm4 = scaler.transform(X_train4)\n",
        "X_train_norm5 = scaler.transform(X_train5)\n",
        "X_train_norm6 = scaler.transform(X_train6)\n",
        "X_test_norm = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIohrB1DCyQS",
        "colab_type": "text"
      },
      "source": [
        "# Load Blind Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeW-r6UEC03N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "97fadc74-81b5-4082-c4e0-c6da7aec6d19"
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/plenoi/Clinic/master/Blind.csv')\n",
        "df = df.set_index('hn')\n",
        "df_clean_column = df.drop(['appearance','Wardsize','RHlvsi','depth','nodeyiel','vgmargin','pelvicme','adnmet'],axis = 1)\n",
        "df_clean = df_clean_column.dropna(axis = 0)\n",
        "y_blind = df_clean['pmmet'].values\n",
        "X_blind = df_clean.drop(['pmmet'],axis = 1).values\n",
        "\n",
        "pel_class = np.unique(df_clean['pmmet'])\n",
        "pel_value = [sum(df_clean['pmmet']==pel_class[0]),\n",
        "             sum(df_clean['pmmet']==pel_class[1])]\n",
        "pel_value\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[96, 43]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_TJBQK9C2Cv",
        "colab_type": "text"
      },
      "source": [
        "Data normalization using same scale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1IiFfFLC1Fw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_blind_norm = scaler.transform(X_blind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpFA7Afx8JPy",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEC9r99f8JP0",
        "colab_type": "text"
      },
      "source": [
        "10-Folds Cross Validation Training Accuracy with Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtagYOLw8JP0",
        "colab_type": "code",
        "outputId": "26184273-c5f5-42d3-a278-f2b8de29d460",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {'C': [1, 2, 4, 8, 16]}\n",
        "clf = GridSearchCV(LogisticRegression(random_state=0, solver='liblinear'),params, cv=10)\n",
        "clf.fit(X_train_norm, y_train)\n",
        "print(\"Best params : \" + str(clf.best_params_))\n",
        "print(\"10CV accuracy : \"+str(clf.best_score_*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best params : {'C': 2}\n",
            "10CV accuracy : 71.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TzTClfO8JP3",
        "colab_type": "text"
      },
      "source": [
        "Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "HhOEcwrg8JP3",
        "colab_type": "code",
        "outputId": "5747ffcd-6a65-4721-e737-b9458eeda6ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_predict = clf.predict(X_test_norm)\n",
        "print(\"Test accuracy : \"+str(sum(y_test == y_predict)/len(y_test)*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy : 72.90322580645162\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1j5UvWw8JP5",
        "colab_type": "code",
        "outputId": "36a761db-f155-460c-95c5-e4503cc0fd16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "target_names = ['negative', 'positive']\n",
        "C = confusion_matrix(y_test,y_predict) \n",
        "#C = C / C.astype(np.float).sum(axis=1)*100\n",
        "sns.heatmap(C, annot=True, fmt=\"d\",cmap=\"GnBu\",xticklabels=target_names, yticklabels=target_names)\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAEKCAYAAAA/2c+EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icsS2l7yS3Ke",
        "colab_type": "text"
      },
      "source": [
        "The precision is the ratio tp / (tp + fp). The precision is the ability of the classifier not to label as positive a sample that is negative.\n",
        "\n",
        "The recall is the ratio tp / (tp + fn). The recall is the ability of the classifier to find all the positive samples.\n",
        "\n",
        "The F score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n",
        "\n",
        "The support is the number of occurrences of each class in y_true."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUQXjLDe8JP6",
        "colab_type": "code",
        "outputId": "176247ad-3c37-4427-a87c-fef2dc779f81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "print(classification_report(y_test, y_predict, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.80      0.62      0.70        78\n",
            "    positive       0.68      0.84      0.76        77\n",
            "\n",
            "    accuracy                           0.73       155\n",
            "   macro avg       0.74      0.73      0.73       155\n",
            "weighted avg       0.74      0.73      0.73       155\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqDI1JlmZat7",
        "colab_type": "text"
      },
      "source": [
        "#SGD Classifier\n",
        "10-Folds Cross Validation Training Accuracy with Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGvBrm6SZbGw",
        "colab_type": "code",
        "outputId": "21531334-5e19-456a-cb57-0c8c79856eff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {\n",
        "    'loss' : ['hinge', 'log','modified_huber','squared_hinge', 'perceptron'],\n",
        "    'penalty' : ['l2', 'l1', 'none', 'elasticnet'],\n",
        "    'alpha' : [0.0001, 0.001, 0.01, 0.1],\n",
        "    'eta0' : [0.001, 0.01, 0.1],\n",
        "    'l1_ratio' : [0.1, 0.2, 0.3,0.4,0.5,0.6,0.7,0.8, 0.9]\n",
        "}\n",
        "clf = GridSearchCV(SGDClassifier(random_state=0,max_iter=1000,learning_rate='constant', eta0 = 0.0001),params, cv=10)\n",
        "clf.fit(X_train_norm, y_train)\n",
        "print(\"Best params : \" + str(clf.best_params_))\n",
        "print(\"10CV accuracy : \"+str(clf.best_score_))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best params : {'alpha': 0.0001, 'eta0': 0.1, 'l1_ratio': 0.8, 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
            "10CV accuracy : 0.74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtJH6iOFDBDG",
        "colab_type": "text"
      },
      "source": [
        "Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlx7BzobDC4o",
        "colab_type": "code",
        "outputId": "b740379e-6bb8-4671-88f0-5400a2808ac3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_predict = clf.predict(X_test_norm)\n",
        "print(\"Test accuracy : \"+str(sum(y_test == y_predict)/len(y_test)*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy : 72.90322580645162\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "virBSM-lDUs2",
        "colab_type": "code",
        "outputId": "47b57532-badc-4f65-c274-ff106a61dd1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "target_names = ['negative', 'positive']\n",
        "C = confusion_matrix(y_test,y_predict) \n",
        "#C = C / C.astype(np.float).sum(axis=1)*100\n",
        "sns.heatmap(C, annot=True, fmt=\"d\",cmap=\"GnBu\",xticklabels=target_names, yticklabels=target_names)\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAEKCAYAAAA/2c+EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHhtJREFUeJzt3XucVXW9//HXew8aCHKJo4AogrfI\nChAIMTmmYGZm3rO8hemJ1Lxllp70mKmntDqVHtMATSk1b+XR/OUtBM0r3lBUQEVFMQRFEFPGED6/\nP9YaHZGZvffMXnuvzbyfPtZj1l577/X9bmf4zHc+6/v9LEUEZmaWP4Vad8DMzNbOAdrMLKccoM3M\ncsoB2swspxygzcxyygHazCynHKDNzHLKAdrMLKccoM3McqpTrTvQktGT7/MSR/uIs/dorHUXLIe+\n0H+s2nuOLrucVXLMWTHtjHa3VwqPoM3Mciq3I2gzs6pSVQbFZXGANjMDaGiodQ8+wgHazAw8gjYz\nyy3l75KcA7SZGUDBI2gzs3xyisPMLKec4jAzy6kGB2gzs3zyCNrMLKecgzYzyymPoM3MciqH0+zy\n9yvDzKwWCg2lb0VI6inpeklzJM2WtIOkj0u6Q9Kz6ddeRbtUkQ9mZlbvpNK34s4Hbo2IwcBQYDZw\nKjA1IrYGpqaPW+UAbWYGSYqj1K0VknoAOwGXAkTEvyJiGbA3MCV92RRgn6JdatcHMjNbV6hQ8iZp\ngqSHm20Tmp1pEPAacJmkxyRdIqkr0CciFqaveRXoU6xLvkhoZgZlTbOLiEnApBae7gQMB46LiAcl\nnc8a6YyICElF7+DiEbSZGVQyB70AWBARD6aPrycJ2Isk9UuaUj9gcbETOUCbmUFSsL/UrRUR8Srw\nsqRPpIfGAU8DNwHj02PjgRuLdckpDjMzqPRKwuOAKyWtDzwPfJNkQHytpCOB+cCBxU7iAG1mBhVd\nSRgRM4GRa3lqXDnncYA2M4NcriR0gDYzAxdLMjPLrRKWcFebA7SZGTjFYWaWWy43amaWU85Bm5nl\nkxygzczyKYfx2QHazAygoSF/EdoB2swMpzjMzHIrh/HZAdrMDDyCNjPLLQdoM7OcymF8doA2MwMo\neBaHmVk+OcVhZpZTOYzPDtBmZgCFHEZoB2gzM5ziMDPLrYLrQZuZ5VMOB9AO0GZmAPII2swsnzyC\nNjPLKV8kNDPLqRzGZwdoMzOAQsE3jTUzy6UcXiN0gM6jguCyfYbw2jv/4uTb5jBik+4cv/1AOhUK\nzHn9n/zk7udYFbXupVXL0sVv8Ptzp/DW0uWA2HHPMeyy/1gWzFvA1b+6indXvEvvPr0Zf9o36dK1\nS627W7c8i8NK8rVP9+PFZSvoun4DAs74/NYc+9enePnNRr41YjP22GZj/jJ3ca27aVVSaGhgv6P2\nZ7NtBtD4TiPnHfVTBo/4JFf94gr2PWo/th66Dfffch9Tr7mDPY/Yq9bdrVt5zEHnL+nSwW3UdX0+\nt1kvbpq7CIAenTuxcnXw8puNAMx4ZRm7DOxdyy5alfXo3YPNthkAQOcNOtN3QF+Wvb6MxQsWsdWQ\nrQEYPGIwM//+WC27WfcklbxVS+YBWlIXSZ/Iup11xXdHD+LCGfOJNIWxrPE9GiQG/1tXAMYO6s3G\n3davYQ+tlpa8uoQFz73MwE8OpN/mm/DEvY8D8Ohdj7J08dIa966+SaVv1ZJpgJb0FWAmcGv6eJik\nm7Jss57tOKAXSxtXMvf1tz90/L/unMuJOwzi0r0/wzsrV7Ha+ecO6d0VjVzyo4nsf8xX6dK1C4f8\n4DD+fuPdnPftn/DuO400rOeMZXsUCoWSt2rJ+jt6JjAKmA4QETMlDWrpxZImABMABh36Azbeae+M\nu5cvQ/psyL8P6MXnNhvO+g0Fuq7fwJk7b82Z05/lqL88CcCo/j3YrIcvBHU0q95bxeQfTWLkrqMY\nttN2APQd0Jdjf348AIteXsRTDzxZyy7WvTzmoLMO0Csj4s01cjYtjv8iYhIwCWD05Ps63Djx4ode\n4uKHXgJgeL/uHDxkE86c/iy9Oq/H0saVrFcQhw3tz+UzF9S4p1ZNEcGVP/8DfQf0ZdxXd33/+FtL\nl7Nhr+6sXr2a2664hTF77VTDXta/Ss7ikPQi8BawCngvIkZK+jhwDTAQeBE4MCJazUtlHaCfknQw\n0CBpa+B44L6M21znHDJkE8YM6IUk/jz7VR75x/Jad8mq6Pkn5zHjjgfZZIv+/PRb/w3AXkfuzeJX\nFnP3jXcBMGzMMEbvvkMtu1n3MhhB7xIRrzd7fCowNSLOlXRq+viUVvsUkd1AVdIGwGnAbumh24Bz\nIqKx2Hs74gjaijt7j6I/OtYBfaH/2HaH12Hn31VyzJl5wudbbS8dQY9sHqAlzQV2joiFkvoB0yOi\n1QkUWY+gB0fEaSRB2swstypcsD+A2yUFMDFN3/aJiIXp868CfYqdJOsA/T+S+gLXA9dEhK9imFku\nlZODbj6hITUpDcJNxkTEK5I2Bu6QNKf5+yMi0uDdqkwDdETskgboA4GJkrqTBOpzsmzXzKxc5SxA\naT6hoYXnX0m/LpZ0A8lstkWS+jVLcRRdDpz5hL6IeDUiLgCOIpkTfUbWbZqZlatSC1UkdZW0YdM+\nyTW4J4GbgPHpy8YDNxbrU6YjaEmfBL4G7A8sIZli8r0s2zQza4sKLuHuA9yQnq8TcFVE3CrpIeBa\nSUcC80kyC63KOgf9O5Kg/MWI+EfGbZmZtVml5kFHxPPA0LUcXwKMK+dcWeegPTHTzOpChWdxVEQm\nAVrStRFxoKRZfHjloEguYA7Jol0zs7Yq5HCtd1Yj6BPSr3tmdH4zs4rKYXzOZhZHs8nYx0TE/OYb\ncEwWbZqZtYcKKnmrlqyn2X1hLce+lHGbZmZly2PB/qxy0EeTjJS3kPREs6c2BO7Nok0zs/bIY4oj\nqxz0VcAtwE9JKjY1eSsi3sioTTOzNis05O8OgJkE6Ih4E3gTOAggXY/eGegmqVtEvJRFu2ZmbZXH\nEXTmt7yS9CzwAnAXSZHqW7Js08ysLfKYg856TH8OMBp4JiIGkayieSDjNs3MytYRA/TKdHljQVIh\nIqYBIzNu08ysbAWVvlVL1rU4lknqBtwNXClpMfB2kfeYmVVdHi8SZt2jvYEVwHeBW4F5wFcybtPM\nrGyVKjdaSVkXS2o+Wp6SZVtmZu1RzdxyqbKuB/0WHy6WBMn0u4eB76Vl+czMaq6aS7hLlXUO+tfA\nApKFKwK+DmwJPEpSK3rnjNs3MytJDgfQmQfovSKieeHqSZJmRsQpkn6YcdtmZiXLY4oj64uE70g6\nUFIh3Q4EGtPnit7R1sysWhoKKnmrlqwD9CHAYSR3r12U7h8qqQtwbMZtm5mVTIqSt2rJehbH87Q8\nre6eLNs2MytHDjMcLQdoSTfQShoiIvYrdnJJ2wAXA30i4tOShpDkpc9pS2fNzLJSqOLIuFStjaAv\nrMD5JwPfByYCRMQTkq4iqdFhZpYbORxAtxygI2Jq076k9YEBEfFcmeffICJmrHF19L0yz2FmlrmG\nQv5G0EUvEkr6MjALuCN9PCxNf5TidUlbkqZKJB0ALGz9LWZm1VevS73PArYHpgFExExJW5V4/u8A\nk4DBkl4hqQt9SFs6amaWpXrLQTdZGRHL1khTlPpJXgEuIwnuHweWA+NJgr6ZWW7UVQ66mdnpApOC\npEHA8ZRedP9GYBnJ0u5/tK2LZmbZq9cR9LHAGcBq4AbgNuC0Es+/aUTs3sa+mZlVTV3Ng26Slgw9\nRdKPk4exoozz3yfpMxExq809NDOrgoZ6HEFLGg5cCmyUPl4EfCsiHi3h/GOAwyW9ALxLkuaJiBjS\n9i6bmVVeNZdwl6qUFMdlwInp/QSRtHN6bGhrb0p9qe1dMzOrnhyWgy4pQK9uCs4AETFd0upSTh4R\n89vcMzOzKqqrEXRaNwNguqTfAH8kmV73NeDOKvTNzKxq6m0E/Zs1HjfPG+fvV42ZWTsoh2GttVoc\n/17NjpiZ1VKla3FIaiC5/+orEbFnuo7kaqA38AhwWET8q7VzlFQPWtIXgU8BnZuORcRP2tpxM7O8\nyWAe9AnAbKB7+vg84FcRcbWk3wJHkpRjblEpxZIuIlmefRLQBTgUKLUWh5lZXSgoSt6KkbQp8GXg\nkvSxgLHA9elLpgD7FO1TCf0eExEHA0si4r9ICic5QJvZOkVlbCX4NfADkhXYkKQ1lkVEU7nlBUD/\nYicpJUA3rRxslNSX5Kavm5TWRzOz+lBOuVFJEyQ93Gyb8MF5tCewOCIeaW+fSslB3yKpJ/ALYCaw\nimR4bma2zijnImFETCIppbw2OwJ7SdqD5Lpdd+B8oKekTukoelOSap+tKjqCjogzI2JZRFwHDAI+\nA/yptI9hZlYfCkTJW2si4j8jYtOIGAh8HbgzIg4hKbt8QPqy8STVPov0qQwRsSIi3iCpamdmts6o\nwh1VTgFOkvQcSU760mJvKGma3VrkcM2NmVnbZbHUOyKmA9PT/eeBUeW8v60BOn9LbszM2qGulnqn\nN4ZdWyAWyfA8U9OP+FTWTVgd6rXr+bXuguXQimlj232OuiqWBFzYxufMzOpOXRXsj4ip1eyImVkt\nlTVjokramoM2M1un1FuKw8ysw8jhNcLSA7Skj0XEu1l2xsysVkopglRtpVSzGyVpFvBs+niopP/N\nvGdmZlVU4WJJFVFKXvwCYE9gCUBEPA7skmWnzMyqraEQJW/VUkqKoxAR8/Xh9Y2rMuqPmVlN1GsO\n+mVJo4BIb+FyHPBMtt0yM6uuPOagSwnQR5OkOQYAi4C/pcfMzNYZdTmCjojFJCXzzMzWWXU5gpY0\nmbXU5IiICWt5uZlZXarLAE2S0mjSGdgXeDmb7piZ1UZdLvWOiGuaP5b0B+CezHpkZlYD68pS70FA\nn0p3xMyslupyBC1pKR/koAvAG8CpWXbKzKza6m4ErWR1ylA+uPvs6ojI36cwM2unuhtBR0RI+mtE\nfLpaHTIzq4U8zuIo5ZfGTEnbZd4TM7MaKihK3qqltXsSdoqI94DtgIckzQPeJllwExExvEp9NDPL\nnHK4lLC1FMcMYDiwV5X6YmZWM4W13iO7tloL0AKIiHlV6ouZWc3U2wh6I0kntfRkRPwyg/6YmdVE\nDuNzqwG6AehGPvttZlZRDTmcxdFagF4YEWdVrSdmZjWUx2l2RXPQZmYdQR4DXmsBelzVemFmVmN1\ntdQ7It6oZkfMzGqp7pZ6m5l1FIUczrNzgDYzA+QAbWaWT/kLzw7QZmYAKIchOo95cTOzqpNK31o/\njzpLmiHpcUlPSfpxenyQpAclPSfpGknrF+uTA7SZGVBAJW9FvAuMjYihwDBgd0mjgfOAX0XEVsBS\n4MjifTIzMwpSyVtrIvHP9OF66RbAWOD69PgUYJ+ifWr7xzEzW3eUk+KQNEHSw822CR8+lxokzQQW\nA3cA84BlaY19gAVA/2J98kVCMzPKu0gYEZOASa08vwoYJqkncAMwuC19coA2MyObetARsUzSNGAH\noGezO1Vtygc3426RUxxmZiQj6FL/a/U80kbpyBlJXYAvALOBacAB6cvGAzcW65NH0GZmQEPlhtD9\ngCmSGkgGwddGxM2SngaulnQO8BhwabETOUCbmVG5lYQR8QTJzbbXPP48MKqcczlAm5nhWhxmZrmV\nv/DsAG1mBngEbWaWW/kLzw7QZmZARWdxVIwDtJkZ+Sw36gBtZkY2KwnbywHazIx8jqC91Dtnzjjt\nbHYe80X22+vr7x+78ILfcsA+B3Pgvofw7f84jsWLX6thD61WenT9GFedeQAzpxzDY5cfzfbbbvr+\ncyd8dTQrpp1B7+5datjD+lapgv2V5ACdM3vv+2UunnT+h44dfsShXP9/V3HtDVey0+fHMPGiS2rU\nO6ulXxy3O7fPmMew8Rcx6j8mMmd+8ot60426M+6zW/LSq8tq3MP6VqlaHJXkAJ0zI0YOp3uP7h86\n1q1bt/f3G1esyOV8TctW964fY8yQAVz+18cAWPneat58+10Afvad3Tht4t+IWnZwHVCpgv2VlGkO\nWkkkOQTYIiLOkjQA6BsRM7Jsd130v7++iL/c9Fe6devGJZdfXOvuWJUN7NuT15e9w6RT9uIzW/bh\nsWcWcvKFtzF2xCD+8fpbzJq3qNZdrHt5HK1m3aeLSOqgHpQ+fgv4TUsvbn6XgksnX55x1+rLcSce\nw+133syX99ydq6+8rtbdsSrr1FBg2Db9mHzTI+wwYTLvNK7k9PGf5weH/DtnXTa91t1bJ0gqeauW\nrAP09hHxHaARICKWAi3eyTYiJkXEyIgYeeS3Ds+4a/Vpjz1352933FnrbliVvfLacl55bTkPzU5q\nvN9w12yGbdOPzfv2ZMYl32bOH4+n/0bduX/SBPr06lrj3tYrlbFVR9bT7FamNVEDkkLWwOqM21zn\nzH/xJTYfOACAaXfexaAtBta2Q1Z1i5a+zYLFy9l6s948+/ISdh4+iJnPLGSP7/3h/dfM+ePx7Pjt\nySxZvqKGPa1febyyk3WAvoDkflwbS/pvkrsJnJ5xm3XtlJNP5+EZj7Bs2TK+sMueHH3st7jn7vt4\n8YX5FAoF+m3Sl9N/dGqtu2k1cNIFt3DZafuyfqcGXly4lAnn3VTrLq1TpPxloRWR7bVfSYOBcSS/\noKZGxOxS3te46k1flLaP6LXr+cVfZB3OimlntHsAPHPJgyXHnGG9t6/KgDvrWRwXAFdHRIsXBs3M\n8qAjriR8BDhd0jxJv5A0MuP2zMzaJodLCTMN0BExJSL2AD4LzAXOk/Rslm2ambVF/uZwVK9Y0lbA\nYGBzktuPm5nlTP5SHFnnoH8G7AvMA64Bzo4IFwwws9yp5hLuUmU9gp4H7BARr2fcjplZO3WQAC1p\ncETMAR4CBqQ1ON4XEY9m0a6ZWVvlcRZHViPok4AJwP+s5bkAxmbUrplZm+QvPGcUoCNiQrr7pYho\nbP6cpM5ZtGlm1i45zEFnPQ/6vhKPmZnVVB4L9meVg+4L9Ae6SNqOD/566A5skEWbZmbt0ZFy0F8E\nDgc2BX7Z7PhbwA8zatPMrM3yeKeirHLQU4ApkvaPiD9l0YaZWWV1kAAt6dCIuAIYKOmkNZ+PiF+u\n5W1mZjWTv/CcXYqj6ZYO3Vp9lZlZTnSYHHRETEy//jiL85uZVVoec9CZTrOT9DNJ3SWtJ2mqpNck\nHZplm2ZmbVGpaXaSNpM0TdLTkp6SdEJ6/OOS7pD0bPq1V7E+ZT0PereIWA7sCbxIUtXu+xm3aWbW\nBhUrOPoe8L2I2BYYDXxH0rbAqSR3ldoamJo+blXWAbophfJl4LqIeDPj9szM2qRS9fojYmFTvaGI\neIukxHJ/YG9gSvqyKcA+xfqUdTW7myXNAVYAR6d39W4s8h4zsxqofA5a0kBgO+BBoE9ELEyfehXo\nU+z9Wd9R5VTgc8DIiFgJvE3yW8TMLFfKyUFLmiDp4WbbhI+cT+oG/Ak4MU31vi+Su3UXvUlt1gX7\n1wMOBXZKr5DeBfw2yzbNzNqinFkcETEJmNTKudYjCc5XRsSf08OLJPWLiIWS+gGLi7WTdQ76YmAE\ncFG6DU+PmZnlSgVncQi4FJi9xqK8m4Dx6f544MZifco6B/3ZiBja7PGdkh7PuE0zs7JVcKHKjsBh\nwCxJM9NjPwTOBa6VdCQwHziw2ImyDtCrJG0ZEfMAJG0BrMq4TTOz8lUoPkfEPa2cbVw558o6QH8f\nmCbp+fTxQOCbGbdpZla2PC71zjoHfS8wEVgNvJHu359xm2ZmZeswBfub+T2wHDg7fXww8Afgqxm3\na2ZWljzW4sg6QH86Xe7YZJqkpzNu08ysbB0xxfGopNFNDyRtDzyccZtmZmWrWCWOCsp6BD0CuE/S\nS+njAcBcSbNIFtMMybh9M7PSdMAUx+4Zn9/MrCLymOLINEBHxPwsz29mVimFjhagzczqRv7iswO0\nmRl0wBSHmVm9yGOAznqanZmZtZFH0GZmdMyVhGZmdcGzOMzM8sojaDOzfMrjRUIHaDMzcjkN2gHa\nzAw8gjYzyy/noM3M8smzOMzM8sojaDOzfMpfeHaANjMDfJHQzCy3HKDNzHIqj7U4FBG17oMVIWlC\nREyqdT8sX/xzse5zudH6MKHWHbBc8s/FOs4B2swspxygzcxyygG6PjjPaGvjn4t1nC8SmpnllEfQ\nZmY55QBdZyT1lHRMs8ebSLq+ln2y6pJ0lKRvpPuHS9qk2XOXSNq2dr2zSnKKo85IGgjcHBGfrnFX\nLAckTQdOjoiHa90XqzyPoCtM0kBJsyVNlvSUpNsldZG0paRbJT0i6e+SBqev31LSA5JmSTpH0j/T\n490kTZX0aPrc3mkT5wJbSpop6edpe0+m73lA0qea9WW6pJGSukr6naQZkh5rdi6rsvT7NUfSlenP\nyfWSNpA0Lv3ezEq/Vx9LX3+upKclPSHpF+mxMyWdLOkAYCRwZfrz0KXZ9/woST9v1u7hki5M9w9N\nfxZmSpooqaEW/y+sBBHhrYIbMBB4DxiWPr4WOBSYCmydHtseuDPdvxk4KN0/Cvhnut8J6J7u/xvw\nHEnBrYHAk2u092S6/13gx+l+P2Buuv8T4NB0vyfwDNC11v+vOuKWfr8C2DF9/DvgdOBlYJv02O+B\nE4HewFw++Eu3Z/r1TJJRM8B0YGSz808nCdobAc81O34LMAb4JPAXYL30+EXAN2r9/8Xb2jePoLPx\nQkTMTPcfIflH+TngOkkzgYkkARRgB+C6dP+qZucQ8BNJTwB/A/oDfYq0ey1wQLp/INCUm94NODVt\nezrQGRhQ9qeySnk5Iu5N968AxpH8zDyTHpsC7AS8CTQCl0raD3in1AYi4jXgeUmjJfUGBgP3pm2N\nAB5Kfx7GAVtU4DNZBlwsKRvvNttfRRJYl0XEsDLOcQjJKGhERKyU9CJJYG1RRLwiaYmkIcDXSEbk\nkAT7/SNibhntW3bWvPCzjGS0/OEXRbwnaRRJED0AOBYYW0Y7V5P8op4D3BARoaQi0JSI+M829dyq\nyiPo6lgOvCDpqwBKDE2fewDYP93/erP39AAWp8F5F2Dz9PhbwIattHUN8AOgR0Q8kR67DTgu/ceJ\npO3a+4GsXQZI2iHdPxh4GBgoaav02GHAXZK6kXwf/0qSvhr60VO1+vNwA7A3cBBJsIYk1XaApI0B\nJH1c0uYtvN9qzAG6eg4BjpT0OPAUyT8cSHKNJ6WpjK1I/qwFuBIYKWkW8A2SURARsQS4V9KTzS8C\nNXM9SaC/ttmxs4H1gCckPZU+ttqZC3xH0mygF/Ar4JskKbBZwGrgtySB9+b0Z+Me4KS1nOty4LdN\nFwmbPxERS4HZwOYRMSM99jRJzvv29Lx38EG6zXLG0+xqTNIGwIr0z8+vk1ww9CyLdZSnSVo5nIOu\nvRHAhWn6YRlwRI37Y2Y54RG0mVlOOQdtZpZTDtBmZjnlAG1mllMO0LZWklalU7eelHRdOtukrefa\nWdLN6f5ekk5t5bUfqtZXRhtnSjq51OOtnOeflWjXrBIcoK0lKyJiWDod7F98sCoReH+xTdk/PxFx\nU0Sc28pLegJlB2izdZEDtJXi78BWaSW2uZJ+DzwJbCZpN0n3p1X3rktXvyFp97Rq26PAfk0nWqOq\nWh9JN0h6PN0+xxrV+tLXfV/SQ2lFtx83O9dpkp6RdA/wiXI+kKT/U1JZ8ClJE9Z47lfp8amSNkqP\nrbUaoVmWHKCtVZI6AV8CZqWHtgYuiohPAW+TrErbNSKGkyxZPklSZ2Ay8BWSed59Wzj9BcBdETEU\nGE6ywvJUYF46ev++pN3SNkcBw4ARknaSNIJkxeQwYA/gs2V+tCMiYgRJ5bfj04JCAF2Bh9PPdxfw\no/T4JOC49D0nk1SBM8uUF6pYS7qk1c4gGUFfCmwCzI+IB9Ljo4FtSZaeA6wP3E9SOe2FiHgWQNIV\nwIdGqamxJMvYiYhVwJuSeq3xmt3S7bH0cTeSgL0hSQGgd9I2birz8x0vad90f7P0nEtIlllfkx6/\nAvhz+ldBUzXCpvd/rMz2zMrmAG0tWbFm9b00OL3d/BBwR0QctMbryqnaV4yAn0bExDXaOLHNJ5R2\nBnYFdoiId5TclaSlSoFB8pdmudUIzdrNKQ5rjweAHZuqsCm5c8s2JIWdBkraMn3dQS28fypwdPre\nBkk9+Gh1ttuAI5rltvunldjuBvZRcheRDUnSKaXqASxNg/Ngkr8EmhT4oKb2wcA9EdFaNUKzzDhA\nW5ulReEPB/6YVka7HxgcEY0kKY3/l14kXNzCKU4AdkkruD0CbLtmtb6IuJ3kRgb3p6+7HtgwIh4l\nSUU8TnK3kIda6erpkhY0bcCtQKe0mty5JL9omrwNjFJyG7GxwFnp8ZaqEZplxrU4zMxyyiNoM7Oc\ncoA2M8spB2gzs5xygDYzyykHaDOznHKANjPLKQdoM7OccoA2M8up/w9sFIl3lNf/BwAAAABJRU5E\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yZrHa4ZDWkX",
        "colab_type": "text"
      },
      "source": [
        "The precision is the ratio tp / (tp + fp). The precision is the ability of the classifier not to label as positive a sample that is negative.\n",
        "\n",
        "The recall is the ratio tp / (tp + fn). The recall is the ability of the classifier to find all the positive samples.\n",
        "\n",
        "The F score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n",
        "\n",
        "The support is the number of occurrences of each class in y_true."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sanoTnPqDW6G",
        "colab_type": "code",
        "outputId": "e330973d-cb99-4998-8814-95498ab3a623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(classification_report(y_test, y_predict, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.79      0.63      0.70        78\n",
            "    positive       0.69      0.83      0.75        77\n",
            "\n",
            "    accuracy                           0.73       155\n",
            "   macro avg       0.74      0.73      0.73       155\n",
            "weighted avg       0.74      0.73      0.73       155\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83TUQPbu8JP8",
        "colab_type": "text"
      },
      "source": [
        "# Support Vector Machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB-9M8468JP9",
        "colab_type": "text"
      },
      "source": [
        "10-Folds Cross Validation Training Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVtRIhLf8JP9",
        "colab_type": "code",
        "outputId": "72bd405f-3033-4af5-efdf-060facb56d3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "params = {\n",
        "    'C' : [1,2,4,8,16,32], # High C = Overfitting\n",
        "    'gamma' : [0.03125, 0.0625, 0.125, 0.25, 0.5, 1, 2, 4, 8, 16, 32] # High gamma = Overfitting\n",
        "}\n",
        "clf = GridSearchCV(SVC(),params, cv=10)\n",
        "clf.fit(X_train_norm, y_train)\n",
        "print(\"Best params : \" + str(clf.best_params_))\n",
        "print(\"10CV accuracy : \"+str(clf.best_score_*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best params : {'C': 16, 'gamma': 0.125}\n",
            "10CV accuracy : 74.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsybCHdK8JP_",
        "colab_type": "text"
      },
      "source": [
        "Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ0XGMYp8JP_",
        "colab_type": "code",
        "outputId": "a869883c-c930-41f8-e8a0-54906cb7d20b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_predict = clf.predict(X_test_norm)\n",
        "target_names = ['negative', 'positive']\n",
        "sum(y_test == y_predict)/len(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7225806451612903"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C--oj8zn8JQB",
        "colab_type": "code",
        "outputId": "618302e5-bde5-4ea1-88b4-2a4fe22217fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "target_names = ['negative', 'positive']\n",
        "C = confusion_matrix(y_test,y_predict) \n",
        "C = C / C.astype(np.float).sum(axis=1)*100\n",
        "sns.heatmap(C, annot=True, fmt=\".2f\",cmap=\"GnBu\",xticklabels=target_names, yticklabels=target_names)\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAEKCAYAAAA/2c+EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naBGRmFKAFhGJKQVoEZGYUoAWEYmp/wfu9fT+86blFgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8NLTtRz8JQC",
        "colab_type": "code",
        "outputId": "23a8bb4d-b821-4aad-e6cb-4e8f2bcace77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(classification_report(y_test, y_predict, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.78      0.63      0.70        78\n",
            "    positive       0.68      0.82      0.75        77\n",
            "\n",
            "    accuracy                           0.72       155\n",
            "   macro avg       0.73      0.72      0.72       155\n",
            "weighted avg       0.73      0.72      0.72       155\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDlmx5yk8JQF",
        "colab_type": "text"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtXkXkoc8JQF",
        "colab_type": "text"
      },
      "source": [
        "10-Folds Cross Validation Training Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hBNax_o8JQG",
        "colab_type": "code",
        "outputId": "87b5bb59-97a4-4db3-dab0-fe291dc9f63b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "params = {\n",
        "    'n_estimators' : [10,50,100,200,500],\n",
        "    'min_samples_leaf' : [1,2,4,8,16,32],\n",
        "    'max_features' : ['sqrt',0.5,0.8],\n",
        "    'criterion' : ['gini','entropy']\n",
        "}\n",
        "clf = GridSearchCV(RandomForestClassifier(random_state=0),params, cv = 10)\n",
        "clf.fit(X_train_norm, y_train)\n",
        "print(\"Best params : \" + str(clf.best_params_))\n",
        "print(\"10CV accuracy : \"+str(clf.best_score_*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best params : {'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 8, 'n_estimators': 100}\n",
            "10CV accuracy : 75.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysu3HGiI8JQJ",
        "colab_type": "text"
      },
      "source": [
        "Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhvwH76Z8JQK",
        "colab_type": "code",
        "outputId": "6e9cf8c3-1576-45d9-cd6f-a39c47317329",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_predict = clf.predict(X_test_norm)\n",
        "target_names = ['negative', 'positive']\n",
        "sum(y_test == y_predict)/len(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7096774193548387"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY-ZyplN8JQL",
        "colab_type": "code",
        "outputId": "07d29e68-5883-407c-d529-340efccbc503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "target_names = ['negative', 'positive']\n",
        "C = confusion_matrix(y_test,y_predict) \n",
        "C = C / C.astype(np.float).sum(axis=1)*100\n",
        "sns.heatmap(C, annot=True, fmt=\".2f\",cmap=\"GnBu\",xticklabels=target_names, yticklabels=target_names)\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAEKCAYAAAA/2c+EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzqS4AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwGUP-YJ8JQN",
        "colab_type": "code",
        "outputId": "28f7382e-7fe0-42ce-f92c-01edde49f8c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(classification_report(y_test, y_predict, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.77      0.60      0.68        78\n",
            "    positive       0.67      0.82      0.74        77\n",
            "\n",
            "    accuracy                           0.71       155\n",
            "   macro avg       0.72      0.71      0.71       155\n",
            "weighted avg       0.72      0.71      0.71       155\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP7YysqP8JQO",
        "colab_type": "text"
      },
      "source": [
        "# XGBoost\n",
        "\n",
        "10-Folds Cross Validation Training Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "8k6aTTLX8JQP",
        "colab_type": "code",
        "outputId": "6508eb7c-91ae-4e66-d4ae-393dcf041be1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#!pip install xgboost  // If you dont have XGBoost\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "params = {\n",
        "    'n_estimators' : [10,50,100,200],\n",
        "    'max_depth' : [2, 4, 8],\n",
        "     'gamma' : [0.03125, 0.0625, 0.125, 0.25, 0.5, 1, 2, 4, 8, 16, 32], # High gamma as much as possible\n",
        "    'learning_rate' : [0.001, 0.01, 0.1],\n",
        "    'minchildweight' : [1,2,4,8,16,32],  # High as much as possible\n",
        "    'subsample' : [0.5,0.8, 1],\n",
        "    'colsample_bytree' : [0.5, 0.8, 1]\n",
        "    #'reg_alpha','reg_lamnda' : [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]  # Only for linear\n",
        "    \n",
        "}\n",
        "clf = GridSearchCV(xgb.XGBClassifier(random_state=0, objective='binary:logistic',n_jobs=-1),params, cv = 10)\n",
        "clf.fit(X_train_norm1, y_train)\n",
        "print(\"Best params : \" + str(clf.best_params_))\n",
        "print(\"10CV accuracy : \"+str(clf.best_score_*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best params : {'colsample_bytree': 0.5, 'gamma': 0.5, 'learning_rate': 0.1, 'max_depth': 2, 'minchildweight': 1, 'n_estimators': 100, 'subsample': 0.8}\n",
            "10CV accuracy : 76.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCjZSw3y8JQR",
        "colab_type": "text"
      },
      "source": [
        "Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBzk9b858JQR",
        "colab_type": "code",
        "outputId": "ef64d40b-484d-4a01-a906-3ac35556eed4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_predict1 = clf.predict(X_test_norm)\n",
        "target_names = ['negative', 'positive']\n",
        "sum(y_test == y_predict1)/len(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7161290322580646"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "udIynozx8JQT",
        "colab_type": "code",
        "outputId": "834a2d42-3d17-4465-dd2d-186e8e0638ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "target_names = ['negative', 'positive']\n",
        "C = confusion_matrix(y_test,y_predict1) \n",
        "C = C / C.astype(np.float).sum(axis=1)*100\n",
        "sns.heatmap(C, annot=True, fmt=\".2f\",cmap=\"GnBu\",xticklabels=target_names, yticklabels=target_names)\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAEKCAYAAAA/2c+EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLKccoM3McsoB2swsp/4ffRJECUtxxwAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6GyVYcE8JQV",
        "colab_type": "code",
        "outputId": "fa7b45ea-b0a6-4c5e-fdb4-6fd83668a610",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(classification_report(y_test, y_predict1, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.76      0.64      0.69        78\n",
            "    positive       0.69      0.79      0.73        77\n",
            "\n",
            "    accuracy                           0.72       155\n",
            "   macro avg       0.72      0.72      0.71       155\n",
            "weighted avg       0.72      0.72      0.71       155\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGawdzqGHuvG",
        "colab_type": "text"
      },
      "source": [
        "# XGBoost Model 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXvhAQsZHt8Z",
        "colab_type": "code",
        "outputId": "d788acfc-c878-47f0-9832-a31a94c3024e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#!pip install xgboost  // If you dont have XGBoost\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "params = {\n",
        "    'n_estimators' : [10,50,100,200],\n",
        "    'max_depth' : [2, 4, 8],\n",
        "     'gamma' : [0.03125, 0.0625, 0.125, 0.25, 0.5, 1, 2, 4, 8, 16, 32], # High gamma as much as possible\n",
        "    'learning_rate' : [0.001, 0.01, 0.1],\n",
        "    'minchildweight' : [1,2,4,8,16,32],  # High as much as possible\n",
        "    'subsample' : [0.5,0.8, 1],\n",
        "    'colsample_bytree' : [0.5, 0.8, 1]\n",
        "    #'reg_alpha','reg_lamnda' : [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]  # Only for linear\n",
        "    \n",
        "}\n",
        "clf = GridSearchCV(xgb.XGBClassifier(random_state=0, objective='binary:logistic'),params, cv = 10)\n",
        "clf.fit(X_train_norm2, y_train2)\n",
        "print(\"Best params : \" + str(clf.best_params_))\n",
        "print(\"10CV accuracy : \"+str(clf.best_score_*100))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best params : {'colsample_bytree': 0.5, 'gamma': 0.0625, 'learning_rate': 0.1, 'max_depth': 8, 'minchildweight': 1, 'n_estimators': 50, 'subsample': 0.5}\n",
            "10CV accuracy : 75.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVvRtxDxdLKG",
        "colab_type": "code",
        "outputId": "c84a5f58-d310-4693-90f8-9fc9f7ccb521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_predict2 = clf.predict(X_test_norm)\n",
        "target_names = ['negative', 'positive']\n",
        "sum(y_test == y_predict2)/len(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7032258064516129"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXF9ri8idLU3",
        "colab_type": "code",
        "outputId": "271dd2d5-2c11-45ce-ac69-a215b95bb004",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "target_names = ['negative', 'positive']\n",
        "C = confusion_matrix(y_test,y_predict2) \n",
        "C = C / C.astype(np.float).sum(axis=1)*100\n",
        "sns.heatmap(C, annot=True, fmt=\".2f\",cmap=\"GnBu\",xticklabels=target_names, yticklabels=target_names)\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAEKCAYAAAA/2c+EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFhGJqf8PoHiip6N/MvcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpj-c6NowXWk",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQSupzLuyrcJ",
        "colab_type": "text"
      },
      "source": [
        "Install Tensorflow 2.0, if you dont have it, pls uncomment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apfoJf7KwjCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install tensorflow==2.0.0-beta1\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFIQ4e9By39Q",
        "colab_type": "text"
      },
      "source": [
        "Create Deep Learning Model \n",
        "Using Sequential = Feed-Forward Model\n",
        "1. The first hidden layer contains 16 hidden nodes connected to input layers with 18 nodes corresponding to number of features\n",
        "2. Other layers is chosen based on 2^1, 2^2, 2^3 concept with 'relu' activation function\n",
        "3. Output layer is sigmoid because it can output value which is close 0 and 1 (Binary Class)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYWq1QcjxVVn",
        "colab_type": "code",
        "outputId": "19165452-bbb8-4030-8178-ac7e13317bd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(16, activation='relu', input_shape=(14,)),\n",
        "  tf.keras.layers.Dense(8, activation='relu'),\n",
        "  tf.keras.layers.Dense(4, activation='relu'),\n",
        "  tf.keras.layers.Dense(2, activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')    \n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 16)                240       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 36        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 10        \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 3         \n",
            "=================================================================\n",
            "Total params: 425\n",
            "Trainable params: 425\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1HIk3Vbz8O5",
        "colab_type": "text"
      },
      "source": [
        "Set up Optimizer to 'adam' with is argubly the best one now, the loss function is set to binary_crossentropy (Binary Classification)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmgpa6cVz_G0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "bac4e296-2fa6-46e8-9e38-7a0fadebe72a"
      },
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufIG52Wj0dwp",
        "colab_type": "text"
      },
      "source": [
        "Train model around 20 epochs with batchsize 20\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYQMmo_A0swm",
        "colab_type": "code",
        "outputId": "040a02fa-7d7a-4922-eb0a-047282bdbdbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "model.fit(X_train_norm, y_train, epochs=3, batch_size=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "400/400 [==============================] - 0s 74us/sample - loss: 0.6211 - acc: 0.7150\n",
            "Epoch 2/3\n",
            "400/400 [==============================] - 0s 66us/sample - loss: 0.6179 - acc: 0.7150\n",
            "Epoch 3/3\n",
            "400/400 [==============================] - 0s 66us/sample - loss: 0.6118 - acc: 0.7300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5e8c3b0710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8i0vv5X1Qbt",
        "colab_type": "text"
      },
      "source": [
        "The model train accuracy is stable aroud 75, so stop train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSFwXFbj1RRC",
        "colab_type": "text"
      },
      "source": [
        "Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q36U3-5H1Qw8",
        "colab_type": "code",
        "outputId": "44b7254f-8b0b-4b24-c2e3-aa657fce9cc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_predict = np.round(model.predict(X_test_norm))\n",
        "y_predict = [i[0] for i in y_predict.tolist()]\n",
        "sum(y_predict == y_test)/len(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6709677419354839"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJxV7N9r3dwG",
        "colab_type": "code",
        "outputId": "1c7310cd-032c-4125-d9eb-64670e1c54cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "target_names = ['negative', 'positive']\n",
        "C = confusion_matrix(y_test,y_predict) \n",
        "C = C / C.astype(np.float).sum(axis=1)*100\n",
        "sns.heatmap(C, annot=True, fmt=\".2f\",cmap=\"GnBu\",xticklabels=target_names, yticklabels=target_names)\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAEKCAYAAAA/2c+EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\neCCrAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym5c3nO53rql",
        "colab_type": "code",
        "outputId": "e8a3ed95-2ba8-48ab-fe9c-2c931f1067bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(classification_report(y_test, y_predict, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.65      0.47      0.55        68\n",
            "    positive       0.62      0.78      0.69        76\n",
            "\n",
            "    accuracy                           0.63       144\n",
            "   macro avg       0.64      0.62      0.62       144\n",
            "weighted avg       0.64      0.63      0.62       144\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uom0bA6w9gur",
        "colab_type": "text"
      },
      "source": [
        "#Deep Learning with Early Stop to Prevent Overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTVxdiKh9w72",
        "colab_type": "text"
      },
      "source": [
        "Split validation data from training data and also create new training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dVtYji49nTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train_over, X_val_norm, y_train_over, y_val = train_test_split(X_train_norm, y_train, test_size=0.20, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XihTMWtN-tr-",
        "colab_type": "text"
      },
      "source": [
        "Create model again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yewRErEJ-wtm",
        "colab_type": "code",
        "outputId": "45ddb0e7-dac7-478b-beb7-c4ec28c8543c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(16, activation='relu', input_shape=(18,)),\n",
        "  tf.keras.layers.Dense(8, activation='relu'),\n",
        "  tf.keras.layers.Dense(4, activation='relu'),\n",
        "  tf.keras.layers.Dense(2, activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')    \n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_5 (Dense)              (None, 16)                304       \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 4)                 36        \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 2)                 10        \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 3         \n",
            "=================================================================\n",
            "Total params: 489\n",
            "Trainable params: 489\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdtWWJgN-zZF",
        "colab_type": "text"
      },
      "source": [
        "Create optimizer again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_Lnu8PI-_DP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRcpU0YB_Avn",
        "colab_type": "text"
      },
      "source": [
        "Create early stop function to prevent overfitting\n",
        "if the accuracy of validation data does not increase for 5 epoch (patience = 5), use the latest best validation accuracy model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpCuc6rY_EN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "overfit_prevent = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10KdH0jV_reE",
        "colab_type": "text"
      },
      "source": [
        "Train model with early stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8jnO-td_vgr",
        "colab_type": "code",
        "outputId": "e12a900b-c55b-4727-be9e-7dcd5770a79d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train_over, y_train_over, epochs= 100, batch_size = 20, validation_data= (X_val_norm, y_val), callbacks=[overfit_prevent])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 320 samples, validate on 80 samples\n",
            "Epoch 1/100\n",
            "320/320 [==============================] - 0s 555us/sample - loss: 0.7039 - acc: 0.4656 - val_loss: 0.6961 - val_acc: 0.4875\n",
            "Epoch 2/100\n",
            "320/320 [==============================] - 0s 86us/sample - loss: 0.6940 - acc: 0.4906 - val_loss: 0.6935 - val_acc: 0.4875\n",
            "Epoch 3/100\n",
            "320/320 [==============================] - 0s 81us/sample - loss: 0.6890 - acc: 0.5562 - val_loss: 0.6911 - val_acc: 0.4875\n",
            "Epoch 4/100\n",
            "320/320 [==============================] - 0s 85us/sample - loss: 0.6853 - acc: 0.6094 - val_loss: 0.6867 - val_acc: 0.5375\n",
            "Epoch 5/100\n",
            "320/320 [==============================] - 0s 101us/sample - loss: 0.6798 - acc: 0.6156 - val_loss: 0.6807 - val_acc: 0.5375\n",
            "Epoch 6/100\n",
            "320/320 [==============================] - 0s 96us/sample - loss: 0.6737 - acc: 0.6219 - val_loss: 0.6712 - val_acc: 0.5625\n",
            "Epoch 7/100\n",
            "320/320 [==============================] - 0s 98us/sample - loss: 0.6669 - acc: 0.6594 - val_loss: 0.6630 - val_acc: 0.5875\n",
            "Epoch 8/100\n",
            "320/320 [==============================] - 0s 82us/sample - loss: 0.6602 - acc: 0.6438 - val_loss: 0.6519 - val_acc: 0.6000\n",
            "Epoch 9/100\n",
            "320/320 [==============================] - 0s 92us/sample - loss: 0.6537 - acc: 0.6531 - val_loss: 0.6437 - val_acc: 0.6375\n",
            "Epoch 10/100\n",
            "320/320 [==============================] - 0s 77us/sample - loss: 0.6465 - acc: 0.6938 - val_loss: 0.6377 - val_acc: 0.6250\n",
            "Epoch 11/100\n",
            "320/320 [==============================] - 0s 83us/sample - loss: 0.6404 - acc: 0.6906 - val_loss: 0.6270 - val_acc: 0.6125\n",
            "Epoch 12/100\n",
            "320/320 [==============================] - 0s 91us/sample - loss: 0.6318 - acc: 0.7063 - val_loss: 0.6195 - val_acc: 0.6250\n",
            "Epoch 13/100\n",
            "320/320 [==============================] - 0s 77us/sample - loss: 0.6266 - acc: 0.7031 - val_loss: 0.6147 - val_acc: 0.6500\n",
            "Epoch 14/100\n",
            "320/320 [==============================] - 0s 80us/sample - loss: 0.6199 - acc: 0.7156 - val_loss: 0.6077 - val_acc: 0.6750\n",
            "Epoch 15/100\n",
            "320/320 [==============================] - 0s 86us/sample - loss: 0.6145 - acc: 0.7219 - val_loss: 0.6007 - val_acc: 0.6750\n",
            "Epoch 16/100\n",
            "320/320 [==============================] - 0s 81us/sample - loss: 0.6095 - acc: 0.7312 - val_loss: 0.5975 - val_acc: 0.6875\n",
            "Epoch 17/100\n",
            "320/320 [==============================] - 0s 85us/sample - loss: 0.6051 - acc: 0.7344 - val_loss: 0.5947 - val_acc: 0.6875\n",
            "Epoch 18/100\n",
            "320/320 [==============================] - 0s 85us/sample - loss: 0.5988 - acc: 0.7250 - val_loss: 0.5904 - val_acc: 0.6875\n",
            "Epoch 19/100\n",
            "320/320 [==============================] - 0s 91us/sample - loss: 0.5950 - acc: 0.7281 - val_loss: 0.5857 - val_acc: 0.6875\n",
            "Epoch 20/100\n",
            "320/320 [==============================] - 0s 86us/sample - loss: 0.5914 - acc: 0.7312 - val_loss: 0.5827 - val_acc: 0.6750\n",
            "Epoch 21/100\n",
            "320/320 [==============================] - 0s 85us/sample - loss: 0.5864 - acc: 0.7312 - val_loss: 0.5765 - val_acc: 0.6875\n",
            "Epoch 22/100\n",
            "320/320 [==============================] - 0s 87us/sample - loss: 0.5817 - acc: 0.7312 - val_loss: 0.5752 - val_acc: 0.6875\n",
            "Epoch 23/100\n",
            "320/320 [==============================] - 0s 80us/sample - loss: 0.5769 - acc: 0.7406 - val_loss: 0.5712 - val_acc: 0.7000\n",
            "Epoch 24/100\n",
            "320/320 [==============================] - 0s 89us/sample - loss: 0.5740 - acc: 0.7437 - val_loss: 0.5686 - val_acc: 0.7250\n",
            "Epoch 25/100\n",
            "320/320 [==============================] - 0s 81us/sample - loss: 0.5693 - acc: 0.7594 - val_loss: 0.5673 - val_acc: 0.7250\n",
            "Epoch 26/100\n",
            "320/320 [==============================] - 0s 80us/sample - loss: 0.5648 - acc: 0.7594 - val_loss: 0.5625 - val_acc: 0.7250\n",
            "Epoch 27/100\n",
            "320/320 [==============================] - 0s 84us/sample - loss: 0.5604 - acc: 0.7688 - val_loss: 0.5612 - val_acc: 0.7250\n",
            "Epoch 28/100\n",
            "320/320 [==============================] - 0s 86us/sample - loss: 0.5563 - acc: 0.7750 - val_loss: 0.5592 - val_acc: 0.7375\n",
            "Epoch 29/100\n",
            "320/320 [==============================] - 0s 91us/sample - loss: 0.5521 - acc: 0.7719 - val_loss: 0.5572 - val_acc: 0.7375\n",
            "Epoch 30/100\n",
            "320/320 [==============================] - 0s 83us/sample - loss: 0.5483 - acc: 0.7812 - val_loss: 0.5580 - val_acc: 0.7250\n",
            "Epoch 31/100\n",
            "320/320 [==============================] - 0s 78us/sample - loss: 0.5436 - acc: 0.7969 - val_loss: 0.5530 - val_acc: 0.7375\n",
            "Epoch 32/100\n",
            "320/320 [==============================] - 0s 81us/sample - loss: 0.5401 - acc: 0.7875 - val_loss: 0.5508 - val_acc: 0.7250\n",
            "Epoch 33/100\n",
            "320/320 [==============================] - 0s 75us/sample - loss: 0.5378 - acc: 0.7875 - val_loss: 0.5510 - val_acc: 0.7500\n",
            "Epoch 34/100\n",
            "320/320 [==============================] - 0s 74us/sample - loss: 0.5312 - acc: 0.8062 - val_loss: 0.5549 - val_acc: 0.7625\n",
            "Epoch 35/100\n",
            "320/320 [==============================] - 0s 79us/sample - loss: 0.5277 - acc: 0.8031 - val_loss: 0.5472 - val_acc: 0.7625\n",
            "Epoch 36/100\n",
            "320/320 [==============================] - 0s 82us/sample - loss: 0.5224 - acc: 0.8062 - val_loss: 0.5458 - val_acc: 0.7625\n",
            "Epoch 37/100\n",
            "320/320 [==============================] - 0s 95us/sample - loss: 0.5181 - acc: 0.8094 - val_loss: 0.5453 - val_acc: 0.7625\n",
            "Epoch 38/100\n",
            "320/320 [==============================] - 0s 87us/sample - loss: 0.5141 - acc: 0.8094 - val_loss: 0.5446 - val_acc: 0.7625\n",
            "Epoch 39/100\n",
            "320/320 [==============================] - 0s 85us/sample - loss: 0.5091 - acc: 0.8094 - val_loss: 0.5436 - val_acc: 0.7625\n",
            "Epoch 40/100\n",
            "320/320 [==============================] - 0s 83us/sample - loss: 0.5055 - acc: 0.8062 - val_loss: 0.5435 - val_acc: 0.7750\n",
            "Epoch 41/100\n",
            "320/320 [==============================] - 0s 84us/sample - loss: 0.5024 - acc: 0.8125 - val_loss: 0.5430 - val_acc: 0.7625\n",
            "Epoch 42/100\n",
            "320/320 [==============================] - 0s 82us/sample - loss: 0.4969 - acc: 0.8125 - val_loss: 0.5423 - val_acc: 0.7750\n",
            "Epoch 43/100\n",
            "320/320 [==============================] - 0s 82us/sample - loss: 0.4924 - acc: 0.8156 - val_loss: 0.5410 - val_acc: 0.7500\n",
            "Epoch 44/100\n",
            "320/320 [==============================] - 0s 82us/sample - loss: 0.4899 - acc: 0.8125 - val_loss: 0.5411 - val_acc: 0.7625\n",
            "Epoch 45/100\n",
            "320/320 [==============================] - 0s 78us/sample - loss: 0.4860 - acc: 0.8281 - val_loss: 0.5389 - val_acc: 0.7625\n",
            "Epoch 46/100\n",
            "320/320 [==============================] - 0s 84us/sample - loss: 0.4876 - acc: 0.7937 - val_loss: 0.5398 - val_acc: 0.7500\n",
            "Epoch 47/100\n",
            "320/320 [==============================] - 0s 84us/sample - loss: 0.4811 - acc: 0.8313 - val_loss: 0.5392 - val_acc: 0.7500\n",
            "Epoch 48/100\n",
            "320/320 [==============================] - 0s 72us/sample - loss: 0.4747 - acc: 0.8219 - val_loss: 0.5384 - val_acc: 0.7500\n",
            "Epoch 49/100\n",
            "320/320 [==============================] - 0s 75us/sample - loss: 0.4684 - acc: 0.8375 - val_loss: 0.5385 - val_acc: 0.7500\n",
            "Epoch 50/100\n",
            "320/320 [==============================] - 0s 82us/sample - loss: 0.4650 - acc: 0.8313 - val_loss: 0.5401 - val_acc: 0.7500\n",
            "Epoch 51/100\n",
            "320/320 [==============================] - 0s 96us/sample - loss: 0.4617 - acc: 0.8281 - val_loss: 0.5393 - val_acc: 0.7500\n",
            "Epoch 52/100\n",
            "320/320 [==============================] - 0s 75us/sample - loss: 0.4567 - acc: 0.8469 - val_loss: 0.5416 - val_acc: 0.7375\n",
            "Epoch 53/100\n",
            "320/320 [==============================] - 0s 79us/sample - loss: 0.4542 - acc: 0.8438 - val_loss: 0.5413 - val_acc: 0.7500\n",
            "Epoch 54/100\n",
            "320/320 [==============================] - 0s 81us/sample - loss: 0.4508 - acc: 0.8594 - val_loss: 0.5413 - val_acc: 0.7375\n",
            "Epoch 55/100\n",
            "320/320 [==============================] - 0s 83us/sample - loss: 0.4477 - acc: 0.8531 - val_loss: 0.5420 - val_acc: 0.7375\n",
            "Epoch 56/100\n",
            "320/320 [==============================] - 0s 82us/sample - loss: 0.4461 - acc: 0.8594 - val_loss: 0.5444 - val_acc: 0.7375\n",
            "Epoch 57/100\n",
            "320/320 [==============================] - 0s 80us/sample - loss: 0.4411 - acc: 0.8562 - val_loss: 0.5429 - val_acc: 0.7375\n",
            "Epoch 58/100\n",
            "320/320 [==============================] - 0s 250us/sample - loss: 0.4368 - acc: 0.8531 - val_loss: 0.5445 - val_acc: 0.7500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5e8e0f08d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOJCRse-CGUg",
        "colab_type": "code",
        "outputId": "3bca1913-54ad-4477-d1c2-cf1ce5867b47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_predict = np.round(model.predict(X_test_norm))\n",
        "y_predict = [i[0] for i in y_predict.tolist()]\n",
        "sum(y_predict == y_test)/len(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7152777777777778"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmTIvtc3GL-J",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning with validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKOWZZb5GK_E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "e61539d6-636b-42cd-e1dc-c3551db20d7b"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(8, activation='relu', input_shape=(14,)),\n",
        "  tf.keras.layers.Dense(4, activation='relu'),\n",
        "  tf.keras.layers.Dense(2, activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')    \n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_57 (Dense)             (None, 8)                 120       \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 4)                 36        \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 2)                 10        \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 1)                 3         \n",
            "=================================================================\n",
            "Total params: 169\n",
            "Trainable params: 169\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obm8KJsLGhvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpzqsFk0Gw2r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5c2e3761-9c2f-49b7-81bc-33a6d3dfc4db"
      },
      "source": [
        "train_acc = list()\n",
        "val_acc = list()\n",
        "for i in range(0,130):\n",
        "  history = model.fit(X_train_norm1, y_train1, epochs= 1, batch_size = 200, validation_data= (X_test_norm, y_test))\n",
        "  tmp_avg = np.mean(history.history['acc'])\n",
        "  tmp_avg_val = np.mean(history.history['val_acc'])\n",
        "  \n",
        "  history = model.fit(X_train_norm2, y_train2, epochs= 1, batch_size = 200, validation_data= (X_test_norm, y_test))\n",
        "  tmp_avg = tmp_avg + np.mean(history.history['acc'])\n",
        "  tmp_avg_val = tmp_avg_val + np.mean(history.history['val_acc'])\n",
        "  \n",
        "  history = model.fit(X_train_norm3, y_train3, epochs= 1, batch_size = 200, validation_data= (X_test_norm, y_test))\n",
        "  tmp_avg = tmp_avg + np.mean(history.history['acc'])\n",
        "  tmp_avg_val = tmp_avg_val + np.mean(history.history['val_acc'])\n",
        "  \n",
        "  history = model.fit(X_train_norm4, y_train4, epochs= 1, batch_size = 200, validation_data= (X_test_norm, y_test))\n",
        "  tmp_avg = tmp_avg + np.mean(history.history['acc'])\n",
        "  tmp_avg_val = tmp_avg_val + np.mean(history.history['val_acc'])\n",
        "  \n",
        "  history = model.fit(X_train_norm5, y_train5, epochs= 1, batch_size = 200, validation_data= (X_test_norm, y_test))\n",
        "  tmp_avg = tmp_avg + np.mean(history.history['acc'])\n",
        "  tmp_avg_val = tmp_avg_val + np.mean(history.history['val_acc'])\n",
        "  \n",
        "  history = model.fit(X_train_norm6, y_train6, epochs= 1, batch_size = 200, validation_data= (X_test_norm, y_test))\n",
        "  tmp_avg = tmp_avg + np.mean(history.history['acc'])\n",
        "  tmp_avg_val = tmp_avg_val + np.mean(history.history['val_acc'])\n",
        "  \n",
        "  train_acc.append(tmp_avg/6)\n",
        "  val_acc.append(tmp_avg_val/6)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 1ms/sample - loss: 0.6928 - acc: 0.5150 - val_loss: 0.6928 - val_acc: 0.6065\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.6929 - acc: 0.5725 - val_loss: 0.6927 - val_acc: 0.6065\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.6928 - acc: 0.5625 - val_loss: 0.6927 - val_acc: 0.6129\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.6926 - acc: 0.5825 - val_loss: 0.6926 - val_acc: 0.6194\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6925 - acc: 0.5850 - val_loss: 0.6926 - val_acc: 0.6000\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.6928 - acc: 0.5850 - val_loss: 0.6926 - val_acc: 0.5935\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6925 - acc: 0.5875 - val_loss: 0.6925 - val_acc: 0.5935\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6926 - acc: 0.5975 - val_loss: 0.6924 - val_acc: 0.5935\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.6924 - acc: 0.5975 - val_loss: 0.6924 - val_acc: 0.5935\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6922 - acc: 0.6100 - val_loss: 0.6923 - val_acc: 0.5935\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6920 - acc: 0.5925 - val_loss: 0.6922 - val_acc: 0.6000\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.6924 - acc: 0.6075 - val_loss: 0.6921 - val_acc: 0.6000\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6921 - acc: 0.6025 - val_loss: 0.6920 - val_acc: 0.6000\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.6922 - acc: 0.6000 - val_loss: 0.6919 - val_acc: 0.6000\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.6918 - acc: 0.6025 - val_loss: 0.6918 - val_acc: 0.6000\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6915 - acc: 0.6150 - val_loss: 0.6917 - val_acc: 0.6065\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.6913 - acc: 0.6050 - val_loss: 0.6916 - val_acc: 0.6065\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.6918 - acc: 0.6175 - val_loss: 0.6915 - val_acc: 0.6065\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 35us/sample - loss: 0.6914 - acc: 0.6025 - val_loss: 0.6914 - val_acc: 0.6065\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6915 - acc: 0.5975 - val_loss: 0.6912 - val_acc: 0.6065\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.6910 - acc: 0.6125 - val_loss: 0.6911 - val_acc: 0.6129\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.6905 - acc: 0.6200 - val_loss: 0.6909 - val_acc: 0.6129\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6903 - acc: 0.6125 - val_loss: 0.6907 - val_acc: 0.6065\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.6910 - acc: 0.6325 - val_loss: 0.6906 - val_acc: 0.6129\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.6905 - acc: 0.6250 - val_loss: 0.6903 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6906 - acc: 0.6150 - val_loss: 0.6901 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.6898 - acc: 0.6300 - val_loss: 0.6899 - val_acc: 0.6323\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.6891 - acc: 0.6350 - val_loss: 0.6897 - val_acc: 0.6387\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 34us/sample - loss: 0.6888 - acc: 0.6325 - val_loss: 0.6894 - val_acc: 0.6323\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.6898 - acc: 0.6350 - val_loss: 0.6892 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.6891 - acc: 0.6275 - val_loss: 0.6889 - val_acc: 0.6323\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.6893 - acc: 0.6275 - val_loss: 0.6887 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6880 - acc: 0.6375 - val_loss: 0.6884 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.6873 - acc: 0.6450 - val_loss: 0.6881 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6868 - acc: 0.6375 - val_loss: 0.6878 - val_acc: 0.6323\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.6881 - acc: 0.6325 - val_loss: 0.6875 - val_acc: 0.6194\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.6873 - acc: 0.6400 - val_loss: 0.6872 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.6876 - acc: 0.6225 - val_loss: 0.6869 - val_acc: 0.6129\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.6858 - acc: 0.6350 - val_loss: 0.6866 - val_acc: 0.6129\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6850 - acc: 0.6500 - val_loss: 0.6863 - val_acc: 0.6000\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.6845 - acc: 0.6450 - val_loss: 0.6860 - val_acc: 0.6065\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6863 - acc: 0.6400 - val_loss: 0.6856 - val_acc: 0.6129\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.6852 - acc: 0.6425 - val_loss: 0.6853 - val_acc: 0.6129\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 52us/sample - loss: 0.6856 - acc: 0.6400 - val_loss: 0.6849 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 36us/sample - loss: 0.6832 - acc: 0.6700 - val_loss: 0.6846 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.6824 - acc: 0.6700 - val_loss: 0.6842 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.6816 - acc: 0.6525 - val_loss: 0.6838 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.6841 - acc: 0.6500 - val_loss: 0.6835 - val_acc: 0.6194\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.6827 - acc: 0.6525 - val_loss: 0.6831 - val_acc: 0.6194\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.6832 - acc: 0.6400 - val_loss: 0.6827 - val_acc: 0.6194\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.6802 - acc: 0.6675 - val_loss: 0.6823 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.6793 - acc: 0.6675 - val_loss: 0.6818 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 35us/sample - loss: 0.6784 - acc: 0.6600 - val_loss: 0.6814 - val_acc: 0.6323\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.6816 - acc: 0.6425 - val_loss: 0.6809 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.6798 - acc: 0.6650 - val_loss: 0.6805 - val_acc: 0.6194\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.6803 - acc: 0.6600 - val_loss: 0.6800 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6766 - acc: 0.6750 - val_loss: 0.6795 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6757 - acc: 0.6650 - val_loss: 0.6790 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6747 - acc: 0.6600 - val_loss: 0.6785 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.6786 - acc: 0.6475 - val_loss: 0.6779 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.6764 - acc: 0.6650 - val_loss: 0.6774 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6771 - acc: 0.6525 - val_loss: 0.6768 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6727 - acc: 0.6825 - val_loss: 0.6763 - val_acc: 0.6323\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.6718 - acc: 0.6650 - val_loss: 0.6757 - val_acc: 0.6323\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 34us/sample - loss: 0.6705 - acc: 0.6600 - val_loss: 0.6751 - val_acc: 0.6323\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6753 - acc: 0.6425 - val_loss: 0.6745 - val_acc: 0.6323\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6724 - acc: 0.6675 - val_loss: 0.6739 - val_acc: 0.6258\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6734 - acc: 0.6500 - val_loss: 0.6733 - val_acc: 0.6323\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6681 - acc: 0.6875 - val_loss: 0.6726 - val_acc: 0.6323\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.6673 - acc: 0.6700 - val_loss: 0.6719 - val_acc: 0.6387\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6658 - acc: 0.6675 - val_loss: 0.6713 - val_acc: 0.6452\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6716 - acc: 0.6425 - val_loss: 0.6706 - val_acc: 0.6452\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6680 - acc: 0.6750 - val_loss: 0.6699 - val_acc: 0.6452\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6692 - acc: 0.6550 - val_loss: 0.6692 - val_acc: 0.6452\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.6630 - acc: 0.6900 - val_loss: 0.6684 - val_acc: 0.6452\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6625 - acc: 0.6725 - val_loss: 0.6677 - val_acc: 0.6387\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6607 - acc: 0.6725 - val_loss: 0.6670 - val_acc: 0.6387\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6674 - acc: 0.6475 - val_loss: 0.6662 - val_acc: 0.6323\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.6631 - acc: 0.6750 - val_loss: 0.6655 - val_acc: 0.6323\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.6646 - acc: 0.6650 - val_loss: 0.6647 - val_acc: 0.6323\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6576 - acc: 0.6900 - val_loss: 0.6639 - val_acc: 0.6323\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.6571 - acc: 0.6750 - val_loss: 0.6631 - val_acc: 0.6387\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6553 - acc: 0.6750 - val_loss: 0.6623 - val_acc: 0.6387\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6629 - acc: 0.6425 - val_loss: 0.6616 - val_acc: 0.6387\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.6579 - acc: 0.6850 - val_loss: 0.6608 - val_acc: 0.6452\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.6597 - acc: 0.6650 - val_loss: 0.6599 - val_acc: 0.6452\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.6519 - acc: 0.6900 - val_loss: 0.6591 - val_acc: 0.6452\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 44us/sample - loss: 0.6514 - acc: 0.6775 - val_loss: 0.6582 - val_acc: 0.6452\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6496 - acc: 0.6775 - val_loss: 0.6574 - val_acc: 0.6452\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.6582 - acc: 0.6500 - val_loss: 0.6565 - val_acc: 0.6452\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 36us/sample - loss: 0.6524 - acc: 0.6750 - val_loss: 0.6557 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6543 - acc: 0.6675 - val_loss: 0.6548 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6459 - acc: 0.6900 - val_loss: 0.6540 - val_acc: 0.6452\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 39us/sample - loss: 0.6455 - acc: 0.6750 - val_loss: 0.6531 - val_acc: 0.6452\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 36us/sample - loss: 0.6438 - acc: 0.6775 - val_loss: 0.6522 - val_acc: 0.6452\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.6532 - acc: 0.6575 - val_loss: 0.6513 - val_acc: 0.6452\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.6467 - acc: 0.6775 - val_loss: 0.6504 - val_acc: 0.6452\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.6489 - acc: 0.6675 - val_loss: 0.6496 - val_acc: 0.6516\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.6398 - acc: 0.6950 - val_loss: 0.6487 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 38us/sample - loss: 0.6394 - acc: 0.6825 - val_loss: 0.6478 - val_acc: 0.6774\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 34us/sample - loss: 0.6378 - acc: 0.6800 - val_loss: 0.6469 - val_acc: 0.6645\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.6481 - acc: 0.6650 - val_loss: 0.6460 - val_acc: 0.6710\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 38us/sample - loss: 0.6407 - acc: 0.6800 - val_loss: 0.6452 - val_acc: 0.6710\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.6432 - acc: 0.6775 - val_loss: 0.6443 - val_acc: 0.6710\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6335 - acc: 0.6950 - val_loss: 0.6434 - val_acc: 0.6710\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.6332 - acc: 0.6850 - val_loss: 0.6426 - val_acc: 0.6710\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6318 - acc: 0.6800 - val_loss: 0.6418 - val_acc: 0.6710\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.6429 - acc: 0.6725 - val_loss: 0.6410 - val_acc: 0.6839\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.6347 - acc: 0.6825 - val_loss: 0.6401 - val_acc: 0.6839\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.6375 - acc: 0.6825 - val_loss: 0.6392 - val_acc: 0.6839\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.6272 - acc: 0.6975 - val_loss: 0.6384 - val_acc: 0.6839\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 36us/sample - loss: 0.6271 - acc: 0.6950 - val_loss: 0.6376 - val_acc: 0.6903\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6260 - acc: 0.6900 - val_loss: 0.6368 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.6375 - acc: 0.6725 - val_loss: 0.6360 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.6288 - acc: 0.6875 - val_loss: 0.6353 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6319 - acc: 0.6875 - val_loss: 0.6344 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6208 - acc: 0.7075 - val_loss: 0.6336 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.6209 - acc: 0.6975 - val_loss: 0.6328 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6202 - acc: 0.6900 - val_loss: 0.6320 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.6322 - acc: 0.6875 - val_loss: 0.6312 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.6228 - acc: 0.7025 - val_loss: 0.6304 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.6262 - acc: 0.7000 - val_loss: 0.6296 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.6143 - acc: 0.7275 - val_loss: 0.6287 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.6146 - acc: 0.7100 - val_loss: 0.6279 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.6144 - acc: 0.7050 - val_loss: 0.6271 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 49us/sample - loss: 0.6270 - acc: 0.6950 - val_loss: 0.6264 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.6168 - acc: 0.7100 - val_loss: 0.6257 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.6207 - acc: 0.7075 - val_loss: 0.6248 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6079 - acc: 0.7275 - val_loss: 0.6240 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6087 - acc: 0.7175 - val_loss: 0.6233 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6090 - acc: 0.7100 - val_loss: 0.6225 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.6218 - acc: 0.7050 - val_loss: 0.6218 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 34us/sample - loss: 0.6110 - acc: 0.7100 - val_loss: 0.6212 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.6152 - acc: 0.7150 - val_loss: 0.6204 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.6017 - acc: 0.7325 - val_loss: 0.6197 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.6029 - acc: 0.7175 - val_loss: 0.6189 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6039 - acc: 0.7075 - val_loss: 0.6181 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.6168 - acc: 0.7075 - val_loss: 0.6174 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.6054 - acc: 0.7125 - val_loss: 0.6169 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.6098 - acc: 0.7150 - val_loss: 0.6162 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5954 - acc: 0.7375 - val_loss: 0.6155 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5974 - acc: 0.7225 - val_loss: 0.6147 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.5990 - acc: 0.7175 - val_loss: 0.6140 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.6118 - acc: 0.7125 - val_loss: 0.6135 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5997 - acc: 0.7225 - val_loss: 0.6129 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.6045 - acc: 0.7250 - val_loss: 0.6123 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5895 - acc: 0.7450 - val_loss: 0.6115 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5920 - acc: 0.7325 - val_loss: 0.6106 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.5939 - acc: 0.7250 - val_loss: 0.6098 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.6070 - acc: 0.7125 - val_loss: 0.6091 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5941 - acc: 0.7250 - val_loss: 0.6085 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5993 - acc: 0.7250 - val_loss: 0.6076 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5838 - acc: 0.7450 - val_loss: 0.6068 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5867 - acc: 0.7350 - val_loss: 0.6061 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5893 - acc: 0.7350 - val_loss: 0.6057 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.6024 - acc: 0.7175 - val_loss: 0.6050 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 36us/sample - loss: 0.5890 - acc: 0.7250 - val_loss: 0.6045 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.5945 - acc: 0.7350 - val_loss: 0.6041 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 35us/sample - loss: 0.5785 - acc: 0.7625 - val_loss: 0.6037 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 35us/sample - loss: 0.5818 - acc: 0.7450 - val_loss: 0.6032 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.5846 - acc: 0.7450 - val_loss: 0.6029 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.5982 - acc: 0.7350 - val_loss: 0.6025 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.5843 - acc: 0.7425 - val_loss: 0.6021 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5902 - acc: 0.7475 - val_loss: 0.6017 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 35us/sample - loss: 0.5734 - acc: 0.7675 - val_loss: 0.6013 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5773 - acc: 0.7525 - val_loss: 0.6008 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5800 - acc: 0.7500 - val_loss: 0.6003 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 40us/sample - loss: 0.5943 - acc: 0.7325 - val_loss: 0.5999 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.5794 - acc: 0.7375 - val_loss: 0.5996 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5857 - acc: 0.7450 - val_loss: 0.5992 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5685 - acc: 0.7725 - val_loss: 0.5988 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 35us/sample - loss: 0.5728 - acc: 0.7475 - val_loss: 0.5982 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5759 - acc: 0.7525 - val_loss: 0.5978 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5906 - acc: 0.7325 - val_loss: 0.5971 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.5750 - acc: 0.7375 - val_loss: 0.5968 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5813 - acc: 0.7475 - val_loss: 0.5963 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5639 - acc: 0.7700 - val_loss: 0.5958 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5683 - acc: 0.7525 - val_loss: 0.5951 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5722 - acc: 0.7525 - val_loss: 0.5946 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.5870 - acc: 0.7375 - val_loss: 0.5941 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5708 - acc: 0.7425 - val_loss: 0.5936 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 37us/sample - loss: 0.5771 - acc: 0.7525 - val_loss: 0.5933 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.5594 - acc: 0.7725 - val_loss: 0.5927 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5639 - acc: 0.7500 - val_loss: 0.5921 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5688 - acc: 0.7575 - val_loss: 0.5916 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 34us/sample - loss: 0.5836 - acc: 0.7450 - val_loss: 0.5911 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 36us/sample - loss: 0.5669 - acc: 0.7525 - val_loss: 0.5907 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5730 - acc: 0.7650 - val_loss: 0.5901 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5552 - acc: 0.7825 - val_loss: 0.5894 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5595 - acc: 0.7600 - val_loss: 0.5888 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5658 - acc: 0.7625 - val_loss: 0.5885 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5804 - acc: 0.7500 - val_loss: 0.5881 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.5632 - acc: 0.7550 - val_loss: 0.5878 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5690 - acc: 0.7625 - val_loss: 0.5875 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5510 - acc: 0.7775 - val_loss: 0.5870 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.5555 - acc: 0.7575 - val_loss: 0.5866 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5629 - acc: 0.7600 - val_loss: 0.5862 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5776 - acc: 0.7500 - val_loss: 0.5858 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5595 - acc: 0.7575 - val_loss: 0.5855 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5653 - acc: 0.7675 - val_loss: 0.5852 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5472 - acc: 0.7850 - val_loss: 0.5849 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 34us/sample - loss: 0.5517 - acc: 0.7650 - val_loss: 0.5846 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5603 - acc: 0.7575 - val_loss: 0.5844 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5749 - acc: 0.7525 - val_loss: 0.5839 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.5562 - acc: 0.7600 - val_loss: 0.5837 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 43us/sample - loss: 0.5619 - acc: 0.7675 - val_loss: 0.5834 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5436 - acc: 0.7875 - val_loss: 0.5833 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5480 - acc: 0.7675 - val_loss: 0.5832 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5578 - acc: 0.7550 - val_loss: 0.5831 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5723 - acc: 0.7500 - val_loss: 0.5828 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5528 - acc: 0.7575 - val_loss: 0.5825 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5588 - acc: 0.7625 - val_loss: 0.5824 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5400 - acc: 0.7875 - val_loss: 0.5822 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5442 - acc: 0.7675 - val_loss: 0.5819 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5553 - acc: 0.7525 - val_loss: 0.5817 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5698 - acc: 0.7500 - val_loss: 0.5814 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 38us/sample - loss: 0.5494 - acc: 0.7600 - val_loss: 0.5812 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5555 - acc: 0.7650 - val_loss: 0.5811 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.5366 - acc: 0.7850 - val_loss: 0.5810 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5408 - acc: 0.7725 - val_loss: 0.5807 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5529 - acc: 0.7500 - val_loss: 0.5804 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5674 - acc: 0.7525 - val_loss: 0.5798 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5463 - acc: 0.7600 - val_loss: 0.5796 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5525 - acc: 0.7675 - val_loss: 0.5794 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5335 - acc: 0.7875 - val_loss: 0.5791 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5374 - acc: 0.7725 - val_loss: 0.5789 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5509 - acc: 0.7500 - val_loss: 0.5790 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5652 - acc: 0.7550 - val_loss: 0.5787 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5435 - acc: 0.7600 - val_loss: 0.5785 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5498 - acc: 0.7725 - val_loss: 0.5781 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5303 - acc: 0.7875 - val_loss: 0.5780 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5344 - acc: 0.7750 - val_loss: 0.5778 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5487 - acc: 0.7525 - val_loss: 0.5775 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5629 - acc: 0.7600 - val_loss: 0.5773 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5410 - acc: 0.7625 - val_loss: 0.5772 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5472 - acc: 0.7725 - val_loss: 0.5771 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5274 - acc: 0.7875 - val_loss: 0.5769 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5315 - acc: 0.7775 - val_loss: 0.5768 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5467 - acc: 0.7550 - val_loss: 0.5765 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5611 - acc: 0.7525 - val_loss: 0.5762 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.5388 - acc: 0.7600 - val_loss: 0.5762 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5448 - acc: 0.7750 - val_loss: 0.5762 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5247 - acc: 0.7900 - val_loss: 0.5761 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5289 - acc: 0.7775 - val_loss: 0.5759 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5452 - acc: 0.7475 - val_loss: 0.5758 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5592 - acc: 0.7575 - val_loss: 0.5754 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5367 - acc: 0.7675 - val_loss: 0.5752 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.5424 - acc: 0.7725 - val_loss: 0.5751 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.5223 - acc: 0.7900 - val_loss: 0.5752 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.5265 - acc: 0.7775 - val_loss: 0.5752 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5437 - acc: 0.7450 - val_loss: 0.5751 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5573 - acc: 0.7575 - val_loss: 0.5747 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5346 - acc: 0.7725 - val_loss: 0.5746 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5403 - acc: 0.7675 - val_loss: 0.5746 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5201 - acc: 0.7900 - val_loss: 0.5747 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5240 - acc: 0.7800 - val_loss: 0.5747 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5421 - acc: 0.7475 - val_loss: 0.5746 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.5557 - acc: 0.7600 - val_loss: 0.5743 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 38us/sample - loss: 0.5328 - acc: 0.7750 - val_loss: 0.5744 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.5383 - acc: 0.7700 - val_loss: 0.5743 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.5178 - acc: 0.7850 - val_loss: 0.5743 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5219 - acc: 0.7800 - val_loss: 0.5742 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.5408 - acc: 0.7500 - val_loss: 0.5740 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.5541 - acc: 0.7575 - val_loss: 0.5739 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5310 - acc: 0.7725 - val_loss: 0.5740 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.5363 - acc: 0.7675 - val_loss: 0.5740 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.5158 - acc: 0.7850 - val_loss: 0.5741 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 40us/sample - loss: 0.5196 - acc: 0.7800 - val_loss: 0.5741 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 43us/sample - loss: 0.5394 - acc: 0.7500 - val_loss: 0.5740 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.5527 - acc: 0.7550 - val_loss: 0.5739 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 46us/sample - loss: 0.5291 - acc: 0.7750 - val_loss: 0.5738 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5344 - acc: 0.7675 - val_loss: 0.5737 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5139 - acc: 0.7850 - val_loss: 0.5737 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 36us/sample - loss: 0.5174 - acc: 0.7800 - val_loss: 0.5738 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5382 - acc: 0.7500 - val_loss: 0.5738 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 36us/sample - loss: 0.5514 - acc: 0.7550 - val_loss: 0.5736 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.5276 - acc: 0.7750 - val_loss: 0.5735 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 36us/sample - loss: 0.5327 - acc: 0.7700 - val_loss: 0.5734 - val_acc: 0.7548\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 35us/sample - loss: 0.5120 - acc: 0.7875 - val_loss: 0.5734 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5155 - acc: 0.7825 - val_loss: 0.5734 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5371 - acc: 0.7550 - val_loss: 0.5733 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.5502 - acc: 0.7575 - val_loss: 0.5731 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5262 - acc: 0.7750 - val_loss: 0.5731 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.5311 - acc: 0.7725 - val_loss: 0.5732 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.5101 - acc: 0.7900 - val_loss: 0.5732 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5133 - acc: 0.7875 - val_loss: 0.5732 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5359 - acc: 0.7550 - val_loss: 0.5731 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5488 - acc: 0.7575 - val_loss: 0.5730 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5247 - acc: 0.7800 - val_loss: 0.5730 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5296 - acc: 0.7700 - val_loss: 0.5730 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 41us/sample - loss: 0.5083 - acc: 0.7900 - val_loss: 0.5730 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5113 - acc: 0.7900 - val_loss: 0.5731 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 36us/sample - loss: 0.5347 - acc: 0.7550 - val_loss: 0.5730 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5475 - acc: 0.7575 - val_loss: 0.5729 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.5229 - acc: 0.7800 - val_loss: 0.5728 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 37us/sample - loss: 0.5281 - acc: 0.7725 - val_loss: 0.5728 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5064 - acc: 0.7900 - val_loss: 0.5728 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.5094 - acc: 0.7875 - val_loss: 0.5727 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5336 - acc: 0.7550 - val_loss: 0.5726 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 39us/sample - loss: 0.5461 - acc: 0.7575 - val_loss: 0.5726 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 34us/sample - loss: 0.5214 - acc: 0.7825 - val_loss: 0.5726 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5264 - acc: 0.7700 - val_loss: 0.5725 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.5047 - acc: 0.7875 - val_loss: 0.5725 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5075 - acc: 0.7900 - val_loss: 0.5724 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5324 - acc: 0.7550 - val_loss: 0.5725 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5447 - acc: 0.7525 - val_loss: 0.5724 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5198 - acc: 0.7800 - val_loss: 0.5724 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.5247 - acc: 0.7700 - val_loss: 0.5723 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5025 - acc: 0.7875 - val_loss: 0.5723 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5056 - acc: 0.7925 - val_loss: 0.5724 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 34us/sample - loss: 0.5312 - acc: 0.7625 - val_loss: 0.5724 - val_acc: 0.7484\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.5432 - acc: 0.7625 - val_loss: 0.5723 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5180 - acc: 0.7825 - val_loss: 0.5722 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 35us/sample - loss: 0.5230 - acc: 0.7775 - val_loss: 0.5721 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5004 - acc: 0.7925 - val_loss: 0.5721 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 37us/sample - loss: 0.5034 - acc: 0.7875 - val_loss: 0.5722 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5300 - acc: 0.7600 - val_loss: 0.5722 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5411 - acc: 0.7600 - val_loss: 0.5722 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5160 - acc: 0.7825 - val_loss: 0.5722 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5214 - acc: 0.7775 - val_loss: 0.5723 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4978 - acc: 0.7950 - val_loss: 0.5722 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5015 - acc: 0.7900 - val_loss: 0.5721 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5286 - acc: 0.7600 - val_loss: 0.5720 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5391 - acc: 0.7625 - val_loss: 0.5719 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5140 - acc: 0.7825 - val_loss: 0.5718 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5198 - acc: 0.7825 - val_loss: 0.5717 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4964 - acc: 0.8000 - val_loss: 0.5717 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5000 - acc: 0.7900 - val_loss: 0.5717 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5276 - acc: 0.7600 - val_loss: 0.5717 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5376 - acc: 0.7625 - val_loss: 0.5717 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5125 - acc: 0.7875 - val_loss: 0.5716 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.5181 - acc: 0.7850 - val_loss: 0.5715 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4949 - acc: 0.8025 - val_loss: 0.5714 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4984 - acc: 0.7950 - val_loss: 0.5713 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5265 - acc: 0.7650 - val_loss: 0.5713 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5360 - acc: 0.7650 - val_loss: 0.5712 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5109 - acc: 0.7900 - val_loss: 0.5712 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5165 - acc: 0.7825 - val_loss: 0.5711 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4935 - acc: 0.7975 - val_loss: 0.5709 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4968 - acc: 0.7925 - val_loss: 0.5709 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5254 - acc: 0.7675 - val_loss: 0.5709 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5342 - acc: 0.7650 - val_loss: 0.5709 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5095 - acc: 0.7925 - val_loss: 0.5708 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5150 - acc: 0.7825 - val_loss: 0.5708 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4920 - acc: 0.8000 - val_loss: 0.5708 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.4953 - acc: 0.7925 - val_loss: 0.5708 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5245 - acc: 0.7675 - val_loss: 0.5708 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5331 - acc: 0.7650 - val_loss: 0.5707 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.5078 - acc: 0.7900 - val_loss: 0.5706 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5133 - acc: 0.7850 - val_loss: 0.5705 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4908 - acc: 0.8025 - val_loss: 0.5705 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4940 - acc: 0.7925 - val_loss: 0.5706 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 40us/sample - loss: 0.5236 - acc: 0.7675 - val_loss: 0.5707 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.5317 - acc: 0.7650 - val_loss: 0.5707 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.5066 - acc: 0.7900 - val_loss: 0.5707 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 35us/sample - loss: 0.5120 - acc: 0.7825 - val_loss: 0.5706 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 36us/sample - loss: 0.4894 - acc: 0.8025 - val_loss: 0.5706 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4927 - acc: 0.7950 - val_loss: 0.5707 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5225 - acc: 0.7675 - val_loss: 0.5707 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.5308 - acc: 0.7650 - val_loss: 0.5706 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5053 - acc: 0.7900 - val_loss: 0.5705 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5108 - acc: 0.7850 - val_loss: 0.5705 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4880 - acc: 0.8025 - val_loss: 0.5705 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4913 - acc: 0.7950 - val_loss: 0.5706 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5217 - acc: 0.7675 - val_loss: 0.5707 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 36us/sample - loss: 0.5295 - acc: 0.7650 - val_loss: 0.5706 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.5039 - acc: 0.7875 - val_loss: 0.5706 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 36us/sample - loss: 0.5096 - acc: 0.7825 - val_loss: 0.5704 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.4869 - acc: 0.8025 - val_loss: 0.5704 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.4901 - acc: 0.7950 - val_loss: 0.5705 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5208 - acc: 0.7675 - val_loss: 0.5705 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5282 - acc: 0.7650 - val_loss: 0.5704 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5030 - acc: 0.7875 - val_loss: 0.5704 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.5085 - acc: 0.7825 - val_loss: 0.5704 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.4856 - acc: 0.8025 - val_loss: 0.5704 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4890 - acc: 0.7950 - val_loss: 0.5706 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5200 - acc: 0.7625 - val_loss: 0.5707 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5274 - acc: 0.7650 - val_loss: 0.5705 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5018 - acc: 0.7850 - val_loss: 0.5704 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5074 - acc: 0.7800 - val_loss: 0.5703 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4845 - acc: 0.7975 - val_loss: 0.5703 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.4877 - acc: 0.7900 - val_loss: 0.5704 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5190 - acc: 0.7650 - val_loss: 0.5704 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5260 - acc: 0.7625 - val_loss: 0.5703 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5003 - acc: 0.7825 - val_loss: 0.5702 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.5062 - acc: 0.7775 - val_loss: 0.5701 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4834 - acc: 0.7950 - val_loss: 0.5701 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4866 - acc: 0.7875 - val_loss: 0.5701 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5183 - acc: 0.7650 - val_loss: 0.5700 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5249 - acc: 0.7675 - val_loss: 0.5700 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.4993 - acc: 0.7825 - val_loss: 0.5700 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5053 - acc: 0.7775 - val_loss: 0.5699 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4822 - acc: 0.7950 - val_loss: 0.5700 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.4854 - acc: 0.7875 - val_loss: 0.5701 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5177 - acc: 0.7600 - val_loss: 0.5702 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 36us/sample - loss: 0.5243 - acc: 0.7700 - val_loss: 0.5701 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.4982 - acc: 0.7825 - val_loss: 0.5700 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.5043 - acc: 0.7775 - val_loss: 0.5700 - val_acc: 0.7419\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 49us/sample - loss: 0.4813 - acc: 0.7975 - val_loss: 0.5700 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.4842 - acc: 0.7900 - val_loss: 0.5700 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5170 - acc: 0.7625 - val_loss: 0.5700 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5230 - acc: 0.7675 - val_loss: 0.5699 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4971 - acc: 0.7850 - val_loss: 0.5699 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 36us/sample - loss: 0.5035 - acc: 0.7800 - val_loss: 0.5698 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4802 - acc: 0.7975 - val_loss: 0.5698 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4831 - acc: 0.7900 - val_loss: 0.5699 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5164 - acc: 0.7600 - val_loss: 0.5700 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5224 - acc: 0.7700 - val_loss: 0.5698 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 34us/sample - loss: 0.4960 - acc: 0.7850 - val_loss: 0.5698 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5025 - acc: 0.7800 - val_loss: 0.5697 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.4792 - acc: 0.7975 - val_loss: 0.5698 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4821 - acc: 0.7925 - val_loss: 0.5699 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5157 - acc: 0.7600 - val_loss: 0.5699 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5214 - acc: 0.7650 - val_loss: 0.5698 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.4951 - acc: 0.7875 - val_loss: 0.5697 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.5016 - acc: 0.7825 - val_loss: 0.5697 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 43us/sample - loss: 0.4781 - acc: 0.8000 - val_loss: 0.5697 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.4812 - acc: 0.7950 - val_loss: 0.5699 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.5152 - acc: 0.7600 - val_loss: 0.5698 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 34us/sample - loss: 0.5206 - acc: 0.7675 - val_loss: 0.5696 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 39us/sample - loss: 0.4941 - acc: 0.7900 - val_loss: 0.5696 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5009 - acc: 0.7825 - val_loss: 0.5696 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4774 - acc: 0.8025 - val_loss: 0.5697 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.4802 - acc: 0.7950 - val_loss: 0.5699 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5144 - acc: 0.7625 - val_loss: 0.5698 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 34us/sample - loss: 0.5195 - acc: 0.7650 - val_loss: 0.5697 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4930 - acc: 0.7900 - val_loss: 0.5696 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4999 - acc: 0.7800 - val_loss: 0.5695 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4760 - acc: 0.8025 - val_loss: 0.5696 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.4792 - acc: 0.7975 - val_loss: 0.5697 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5137 - acc: 0.7625 - val_loss: 0.5697 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 34us/sample - loss: 0.5183 - acc: 0.7700 - val_loss: 0.5697 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.4918 - acc: 0.7900 - val_loss: 0.5697 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 34us/sample - loss: 0.4989 - acc: 0.7850 - val_loss: 0.5697 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.4750 - acc: 0.8050 - val_loss: 0.5697 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.4780 - acc: 0.7975 - val_loss: 0.5699 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 34us/sample - loss: 0.5132 - acc: 0.7675 - val_loss: 0.5699 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5172 - acc: 0.7725 - val_loss: 0.5699 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.4910 - acc: 0.7925 - val_loss: 0.5698 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4983 - acc: 0.7850 - val_loss: 0.5698 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4740 - acc: 0.8050 - val_loss: 0.5699 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4768 - acc: 0.7975 - val_loss: 0.5700 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.5124 - acc: 0.7650 - val_loss: 0.5699 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5165 - acc: 0.7725 - val_loss: 0.5698 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.4897 - acc: 0.7950 - val_loss: 0.5697 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4972 - acc: 0.7850 - val_loss: 0.5697 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.4729 - acc: 0.8075 - val_loss: 0.5698 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4753 - acc: 0.7975 - val_loss: 0.5700 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5113 - acc: 0.7650 - val_loss: 0.5701 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5151 - acc: 0.7725 - val_loss: 0.5700 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 35us/sample - loss: 0.4883 - acc: 0.7925 - val_loss: 0.5699 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.4954 - acc: 0.7850 - val_loss: 0.5700 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.4712 - acc: 0.8100 - val_loss: 0.5701 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4735 - acc: 0.7975 - val_loss: 0.5704 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5100 - acc: 0.7675 - val_loss: 0.5705 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5142 - acc: 0.7750 - val_loss: 0.5706 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.4872 - acc: 0.7925 - val_loss: 0.5706 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 34us/sample - loss: 0.4939 - acc: 0.7875 - val_loss: 0.5708 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4700 - acc: 0.8100 - val_loss: 0.5710 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4719 - acc: 0.8000 - val_loss: 0.5712 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5092 - acc: 0.7675 - val_loss: 0.5712 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.5130 - acc: 0.7750 - val_loss: 0.5712 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4859 - acc: 0.7925 - val_loss: 0.5713 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4926 - acc: 0.7875 - val_loss: 0.5715 - val_acc: 0.7355\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4688 - acc: 0.8125 - val_loss: 0.5716 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.4704 - acc: 0.8000 - val_loss: 0.5720 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5082 - acc: 0.7700 - val_loss: 0.5721 - val_acc: 0.7290\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.5123 - acc: 0.7725 - val_loss: 0.5721 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.4849 - acc: 0.7875 - val_loss: 0.5722 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4916 - acc: 0.7825 - val_loss: 0.5722 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.4679 - acc: 0.8100 - val_loss: 0.5724 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4692 - acc: 0.7975 - val_loss: 0.5725 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.5076 - acc: 0.7675 - val_loss: 0.5725 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.5116 - acc: 0.7750 - val_loss: 0.5725 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4842 - acc: 0.7900 - val_loss: 0.5727 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 42us/sample - loss: 0.4908 - acc: 0.7900 - val_loss: 0.5728 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4669 - acc: 0.8100 - val_loss: 0.5730 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.4680 - acc: 0.8025 - val_loss: 0.5732 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5075 - acc: 0.7650 - val_loss: 0.5732 - val_acc: 0.7226\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5110 - acc: 0.7750 - val_loss: 0.5731 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.4833 - acc: 0.7925 - val_loss: 0.5732 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 42us/sample - loss: 0.4899 - acc: 0.7875 - val_loss: 0.5733 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4661 - acc: 0.8100 - val_loss: 0.5734 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 34us/sample - loss: 0.4669 - acc: 0.8050 - val_loss: 0.5736 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5065 - acc: 0.7675 - val_loss: 0.5737 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5099 - acc: 0.7725 - val_loss: 0.5738 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.4827 - acc: 0.7925 - val_loss: 0.5738 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.4888 - acc: 0.7900 - val_loss: 0.5738 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4652 - acc: 0.8150 - val_loss: 0.5738 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.4657 - acc: 0.8075 - val_loss: 0.5739 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5057 - acc: 0.7700 - val_loss: 0.5740 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5089 - acc: 0.7725 - val_loss: 0.5741 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4820 - acc: 0.7950 - val_loss: 0.5742 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4878 - acc: 0.7925 - val_loss: 0.5744 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4646 - acc: 0.8150 - val_loss: 0.5745 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.4647 - acc: 0.8075 - val_loss: 0.5748 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5054 - acc: 0.7700 - val_loss: 0.5747 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.5084 - acc: 0.7725 - val_loss: 0.5748 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.4813 - acc: 0.7950 - val_loss: 0.5748 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4870 - acc: 0.7925 - val_loss: 0.5749 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.4637 - acc: 0.8150 - val_loss: 0.5749 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4636 - acc: 0.8075 - val_loss: 0.5751 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 35us/sample - loss: 0.5048 - acc: 0.7725 - val_loss: 0.5750 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 38us/sample - loss: 0.5075 - acc: 0.7725 - val_loss: 0.5751 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4806 - acc: 0.7975 - val_loss: 0.5751 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4861 - acc: 0.7900 - val_loss: 0.5752 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4632 - acc: 0.8150 - val_loss: 0.5752 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4626 - acc: 0.8075 - val_loss: 0.5754 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 34us/sample - loss: 0.5045 - acc: 0.7725 - val_loss: 0.5754 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5066 - acc: 0.7725 - val_loss: 0.5753 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.4800 - acc: 0.7975 - val_loss: 0.5753 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.4854 - acc: 0.7900 - val_loss: 0.5753 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.4625 - acc: 0.8150 - val_loss: 0.5754 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4619 - acc: 0.8075 - val_loss: 0.5757 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.5044 - acc: 0.7750 - val_loss: 0.5758 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5064 - acc: 0.7700 - val_loss: 0.5757 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.4791 - acc: 0.8000 - val_loss: 0.5757 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.4845 - acc: 0.7925 - val_loss: 0.5757 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4617 - acc: 0.8150 - val_loss: 0.5758 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.4609 - acc: 0.8075 - val_loss: 0.5760 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5036 - acc: 0.7725 - val_loss: 0.5760 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5052 - acc: 0.7700 - val_loss: 0.5760 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4788 - acc: 0.7975 - val_loss: 0.5761 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 41us/sample - loss: 0.4839 - acc: 0.7900 - val_loss: 0.5761 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.4609 - acc: 0.8150 - val_loss: 0.5762 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 35us/sample - loss: 0.4602 - acc: 0.8100 - val_loss: 0.5765 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5036 - acc: 0.7750 - val_loss: 0.5765 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5049 - acc: 0.7725 - val_loss: 0.5763 - val_acc: 0.7161\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 36us/sample - loss: 0.4779 - acc: 0.7975 - val_loss: 0.5763 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4831 - acc: 0.7925 - val_loss: 0.5761 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4603 - acc: 0.8150 - val_loss: 0.5761 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4593 - acc: 0.8100 - val_loss: 0.5763 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.5028 - acc: 0.7750 - val_loss: 0.5764 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5039 - acc: 0.7775 - val_loss: 0.5765 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4774 - acc: 0.8000 - val_loss: 0.5764 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4824 - acc: 0.7925 - val_loss: 0.5764 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4596 - acc: 0.8150 - val_loss: 0.5764 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.4585 - acc: 0.8100 - val_loss: 0.5766 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.5028 - acc: 0.7725 - val_loss: 0.5766 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.5033 - acc: 0.7775 - val_loss: 0.5765 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4767 - acc: 0.8000 - val_loss: 0.5767 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4817 - acc: 0.7925 - val_loss: 0.5768 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.4592 - acc: 0.8175 - val_loss: 0.5769 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.4576 - acc: 0.8125 - val_loss: 0.5772 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5025 - acc: 0.7725 - val_loss: 0.5773 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 34us/sample - loss: 0.5028 - acc: 0.7750 - val_loss: 0.5773 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4761 - acc: 0.8025 - val_loss: 0.5773 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4810 - acc: 0.7950 - val_loss: 0.5773 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.4585 - acc: 0.8200 - val_loss: 0.5772 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4570 - acc: 0.8125 - val_loss: 0.5774 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.5022 - acc: 0.7700 - val_loss: 0.5774 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5021 - acc: 0.7775 - val_loss: 0.5774 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.4756 - acc: 0.8000 - val_loss: 0.5775 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4803 - acc: 0.7950 - val_loss: 0.5774 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4579 - acc: 0.8200 - val_loss: 0.5775 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4562 - acc: 0.8125 - val_loss: 0.5777 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5016 - acc: 0.7725 - val_loss: 0.5778 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.5013 - acc: 0.7775 - val_loss: 0.5778 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4751 - acc: 0.8025 - val_loss: 0.5778 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4797 - acc: 0.7950 - val_loss: 0.5778 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4572 - acc: 0.8200 - val_loss: 0.5779 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4555 - acc: 0.8125 - val_loss: 0.5781 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.5015 - acc: 0.7700 - val_loss: 0.5781 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.5006 - acc: 0.7775 - val_loss: 0.5782 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4745 - acc: 0.8000 - val_loss: 0.5781 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4791 - acc: 0.7925 - val_loss: 0.5780 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.4567 - acc: 0.8200 - val_loss: 0.5780 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.4549 - acc: 0.8125 - val_loss: 0.5783 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5011 - acc: 0.7700 - val_loss: 0.5783 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.4998 - acc: 0.7775 - val_loss: 0.5782 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4740 - acc: 0.8025 - val_loss: 0.5782 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.4783 - acc: 0.7900 - val_loss: 0.5783 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4561 - acc: 0.8175 - val_loss: 0.5783 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.4542 - acc: 0.8100 - val_loss: 0.5785 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.5010 - acc: 0.7700 - val_loss: 0.5785 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4997 - acc: 0.7750 - val_loss: 0.5787 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4738 - acc: 0.8000 - val_loss: 0.5788 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.4780 - acc: 0.7900 - val_loss: 0.5788 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4556 - acc: 0.8175 - val_loss: 0.5787 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.4534 - acc: 0.8075 - val_loss: 0.5788 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.5004 - acc: 0.7650 - val_loss: 0.5786 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.4983 - acc: 0.7750 - val_loss: 0.5787 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4732 - acc: 0.8000 - val_loss: 0.5788 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4772 - acc: 0.7925 - val_loss: 0.5788 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4551 - acc: 0.8150 - val_loss: 0.5788 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4528 - acc: 0.8075 - val_loss: 0.5790 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.4999 - acc: 0.7650 - val_loss: 0.5790 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4975 - acc: 0.7800 - val_loss: 0.5791 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4726 - acc: 0.8000 - val_loss: 0.5792 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4763 - acc: 0.7925 - val_loss: 0.5792 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4546 - acc: 0.8150 - val_loss: 0.5791 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4521 - acc: 0.8100 - val_loss: 0.5792 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4995 - acc: 0.7675 - val_loss: 0.5793 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4966 - acc: 0.7800 - val_loss: 0.5794 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4724 - acc: 0.8000 - val_loss: 0.5796 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4757 - acc: 0.7925 - val_loss: 0.5796 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4540 - acc: 0.8150 - val_loss: 0.5793 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4513 - acc: 0.8075 - val_loss: 0.5795 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4991 - acc: 0.7675 - val_loss: 0.5795 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4959 - acc: 0.7800 - val_loss: 0.5797 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4718 - acc: 0.8025 - val_loss: 0.5797 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4750 - acc: 0.7950 - val_loss: 0.5796 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.4534 - acc: 0.8125 - val_loss: 0.5794 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4507 - acc: 0.8100 - val_loss: 0.5795 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.4988 - acc: 0.7650 - val_loss: 0.5796 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4954 - acc: 0.7825 - val_loss: 0.5799 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.4716 - acc: 0.8025 - val_loss: 0.5800 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4745 - acc: 0.7975 - val_loss: 0.5800 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4529 - acc: 0.8125 - val_loss: 0.5796 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.4500 - acc: 0.8100 - val_loss: 0.5797 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4987 - acc: 0.7650 - val_loss: 0.5796 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4948 - acc: 0.7775 - val_loss: 0.5795 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4709 - acc: 0.8025 - val_loss: 0.5795 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4738 - acc: 0.7950 - val_loss: 0.5796 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.4523 - acc: 0.8125 - val_loss: 0.5794 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.4493 - acc: 0.8100 - val_loss: 0.5796 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4980 - acc: 0.7675 - val_loss: 0.5797 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.4939 - acc: 0.7825 - val_loss: 0.5799 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.4705 - acc: 0.8025 - val_loss: 0.5798 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4731 - acc: 0.7925 - val_loss: 0.5799 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.4515 - acc: 0.8125 - val_loss: 0.5797 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4487 - acc: 0.8100 - val_loss: 0.5798 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.4977 - acc: 0.7650 - val_loss: 0.5797 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.4934 - acc: 0.7825 - val_loss: 0.5799 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.4702 - acc: 0.8025 - val_loss: 0.5799 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4727 - acc: 0.7975 - val_loss: 0.5799 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.4512 - acc: 0.8125 - val_loss: 0.5797 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.4484 - acc: 0.8100 - val_loss: 0.5799 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4976 - acc: 0.7650 - val_loss: 0.5799 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4930 - acc: 0.7800 - val_loss: 0.5803 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4697 - acc: 0.8025 - val_loss: 0.5805 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.4721 - acc: 0.7975 - val_loss: 0.5804 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4505 - acc: 0.8125 - val_loss: 0.5803 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.4475 - acc: 0.8100 - val_loss: 0.5803 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4974 - acc: 0.7650 - val_loss: 0.5805 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4921 - acc: 0.7800 - val_loss: 0.5807 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 18us/sample - loss: 0.4693 - acc: 0.8025 - val_loss: 0.5808 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.4717 - acc: 0.7975 - val_loss: 0.5806 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 21us/sample - loss: 0.4498 - acc: 0.8150 - val_loss: 0.5804 - val_acc: 0.6903\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4469 - acc: 0.8100 - val_loss: 0.5806 - val_acc: 0.6903\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4971 - acc: 0.7675 - val_loss: 0.5805 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4916 - acc: 0.7800 - val_loss: 0.5806 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4687 - acc: 0.8000 - val_loss: 0.5807 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4709 - acc: 0.7925 - val_loss: 0.5806 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 43us/sample - loss: 0.4493 - acc: 0.8125 - val_loss: 0.5806 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4466 - acc: 0.8100 - val_loss: 0.5808 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4969 - acc: 0.7625 - val_loss: 0.5809 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4913 - acc: 0.7800 - val_loss: 0.5812 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4685 - acc: 0.8025 - val_loss: 0.5813 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.4706 - acc: 0.7975 - val_loss: 0.5811 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4489 - acc: 0.8125 - val_loss: 0.5808 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4465 - acc: 0.8100 - val_loss: 0.5807 - val_acc: 0.7097\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.4967 - acc: 0.7625 - val_loss: 0.5808 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.4908 - acc: 0.7800 - val_loss: 0.5811 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4681 - acc: 0.8000 - val_loss: 0.5813 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.4701 - acc: 0.7975 - val_loss: 0.5812 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.4485 - acc: 0.8125 - val_loss: 0.5808 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.4456 - acc: 0.8100 - val_loss: 0.5808 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4964 - acc: 0.7650 - val_loss: 0.5808 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4903 - acc: 0.7800 - val_loss: 0.5811 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.4674 - acc: 0.8000 - val_loss: 0.5814 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4696 - acc: 0.7950 - val_loss: 0.5814 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4479 - acc: 0.8125 - val_loss: 0.5812 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4450 - acc: 0.8100 - val_loss: 0.5812 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4959 - acc: 0.7675 - val_loss: 0.5813 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4898 - acc: 0.7800 - val_loss: 0.5815 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4676 - acc: 0.8000 - val_loss: 0.5817 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 54us/sample - loss: 0.4692 - acc: 0.7950 - val_loss: 0.5815 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4474 - acc: 0.8125 - val_loss: 0.5812 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4448 - acc: 0.8100 - val_loss: 0.5812 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4958 - acc: 0.7675 - val_loss: 0.5813 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4892 - acc: 0.7825 - val_loss: 0.5816 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4670 - acc: 0.8000 - val_loss: 0.5819 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4688 - acc: 0.7950 - val_loss: 0.5817 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4471 - acc: 0.8125 - val_loss: 0.5815 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4443 - acc: 0.8125 - val_loss: 0.5815 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4953 - acc: 0.7725 - val_loss: 0.5816 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4888 - acc: 0.7825 - val_loss: 0.5819 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 32us/sample - loss: 0.4665 - acc: 0.8000 - val_loss: 0.5820 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4682 - acc: 0.7925 - val_loss: 0.5820 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4465 - acc: 0.8150 - val_loss: 0.5818 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4439 - acc: 0.8125 - val_loss: 0.5819 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4951 - acc: 0.7725 - val_loss: 0.5819 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4883 - acc: 0.7825 - val_loss: 0.5822 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4660 - acc: 0.8025 - val_loss: 0.5823 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4678 - acc: 0.7950 - val_loss: 0.5822 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4462 - acc: 0.8125 - val_loss: 0.5821 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4434 - acc: 0.8125 - val_loss: 0.5821 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4946 - acc: 0.7725 - val_loss: 0.5822 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4880 - acc: 0.7825 - val_loss: 0.5823 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4655 - acc: 0.8025 - val_loss: 0.5824 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4673 - acc: 0.7925 - val_loss: 0.5823 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.4458 - acc: 0.8150 - val_loss: 0.5822 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4431 - acc: 0.8125 - val_loss: 0.5823 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.4946 - acc: 0.7700 - val_loss: 0.5825 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4873 - acc: 0.7850 - val_loss: 0.5828 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4653 - acc: 0.8025 - val_loss: 0.5828 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4670 - acc: 0.7975 - val_loss: 0.5827 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 37us/sample - loss: 0.4453 - acc: 0.8150 - val_loss: 0.5825 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4426 - acc: 0.8125 - val_loss: 0.5826 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4942 - acc: 0.7700 - val_loss: 0.5826 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4870 - acc: 0.7825 - val_loss: 0.5829 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4649 - acc: 0.8025 - val_loss: 0.5830 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.4667 - acc: 0.7975 - val_loss: 0.5829 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.4450 - acc: 0.8175 - val_loss: 0.5828 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4423 - acc: 0.8125 - val_loss: 0.5829 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.4938 - acc: 0.7700 - val_loss: 0.5828 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.4866 - acc: 0.7825 - val_loss: 0.5830 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4645 - acc: 0.8025 - val_loss: 0.5830 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4663 - acc: 0.7950 - val_loss: 0.5830 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4444 - acc: 0.8150 - val_loss: 0.5829 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4419 - acc: 0.8125 - val_loss: 0.5830 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4936 - acc: 0.7700 - val_loss: 0.5831 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4863 - acc: 0.7850 - val_loss: 0.5834 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4644 - acc: 0.8025 - val_loss: 0.5833 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4660 - acc: 0.7950 - val_loss: 0.5832 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4441 - acc: 0.8175 - val_loss: 0.5831 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.4414 - acc: 0.8125 - val_loss: 0.5832 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4935 - acc: 0.7700 - val_loss: 0.5832 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.4861 - acc: 0.7825 - val_loss: 0.5836 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 29us/sample - loss: 0.4640 - acc: 0.8025 - val_loss: 0.5835 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4656 - acc: 0.7950 - val_loss: 0.5836 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 30us/sample - loss: 0.4439 - acc: 0.8175 - val_loss: 0.5835 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.4410 - acc: 0.8150 - val_loss: 0.5837 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4933 - acc: 0.7700 - val_loss: 0.5838 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4854 - acc: 0.7875 - val_loss: 0.5842 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4644 - acc: 0.8025 - val_loss: 0.5844 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4657 - acc: 0.8000 - val_loss: 0.5840 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4435 - acc: 0.8200 - val_loss: 0.5836 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4406 - acc: 0.8150 - val_loss: 0.5836 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4933 - acc: 0.7725 - val_loss: 0.5836 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4854 - acc: 0.7825 - val_loss: 0.5838 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 33us/sample - loss: 0.4632 - acc: 0.8050 - val_loss: 0.5840 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.4651 - acc: 0.7950 - val_loss: 0.5842 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4433 - acc: 0.8175 - val_loss: 0.5843 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4402 - acc: 0.8125 - val_loss: 0.5845 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4929 - acc: 0.7700 - val_loss: 0.5844 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 35us/sample - loss: 0.4843 - acc: 0.7875 - val_loss: 0.5845 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4631 - acc: 0.8025 - val_loss: 0.5845 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4648 - acc: 0.8000 - val_loss: 0.5843 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4428 - acc: 0.8175 - val_loss: 0.5840 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.4397 - acc: 0.8150 - val_loss: 0.5842 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4928 - acc: 0.7725 - val_loss: 0.5843 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4841 - acc: 0.7875 - val_loss: 0.5848 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4629 - acc: 0.8050 - val_loss: 0.5850 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4644 - acc: 0.7975 - val_loss: 0.5849 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4425 - acc: 0.8200 - val_loss: 0.5846 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.4395 - acc: 0.8150 - val_loss: 0.5844 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4927 - acc: 0.7725 - val_loss: 0.5844 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4838 - acc: 0.7850 - val_loss: 0.5848 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4624 - acc: 0.8050 - val_loss: 0.5850 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.4641 - acc: 0.7975 - val_loss: 0.5850 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4421 - acc: 0.8150 - val_loss: 0.5850 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4387 - acc: 0.8125 - val_loss: 0.5853 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4922 - acc: 0.7725 - val_loss: 0.5852 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4833 - acc: 0.7875 - val_loss: 0.5852 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 22us/sample - loss: 0.4620 - acc: 0.8050 - val_loss: 0.5852 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4635 - acc: 0.8000 - val_loss: 0.5852 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4415 - acc: 0.8150 - val_loss: 0.5852 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4382 - acc: 0.8150 - val_loss: 0.5855 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4921 - acc: 0.7725 - val_loss: 0.5854 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4830 - acc: 0.7850 - val_loss: 0.5858 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4618 - acc: 0.8050 - val_loss: 0.5858 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4632 - acc: 0.7975 - val_loss: 0.5858 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 27us/sample - loss: 0.4413 - acc: 0.8150 - val_loss: 0.5855 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4378 - acc: 0.8150 - val_loss: 0.5856 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 31us/sample - loss: 0.4921 - acc: 0.7725 - val_loss: 0.5857 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 20us/sample - loss: 0.4826 - acc: 0.7875 - val_loss: 0.5858 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4614 - acc: 0.8050 - val_loss: 0.5860 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 28us/sample - loss: 0.4627 - acc: 0.8000 - val_loss: 0.5861 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4410 - acc: 0.8175 - val_loss: 0.5858 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 19us/sample - loss: 0.4374 - acc: 0.8175 - val_loss: 0.5859 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4916 - acc: 0.7675 - val_loss: 0.5859 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4823 - acc: 0.7875 - val_loss: 0.5861 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4610 - acc: 0.8025 - val_loss: 0.5863 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4624 - acc: 0.7975 - val_loss: 0.5865 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 24us/sample - loss: 0.4406 - acc: 0.8150 - val_loss: 0.5863 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 26us/sample - loss: 0.4369 - acc: 0.8150 - val_loss: 0.5862 - val_acc: 0.6968\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 23us/sample - loss: 0.4916 - acc: 0.7675 - val_loss: 0.5862 - val_acc: 0.7032\n",
            "Train on 400 samples, validate on 155 samples\n",
            "400/400 [==============================] - 0s 25us/sample - loss: 0.4819 - acc: 0.7875 - val_loss: 0.5865 - val_acc: 0.6968\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdeH91eRHhIq",
        "colab_type": "text"
      },
      "source": [
        "# Model History"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDuZ9cMcIkDZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "outputId": "95d77e3f-7777-4552-b808-117036a5c7e4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "plt.figure(num=None, figsize=(16, 8), dpi=90, facecolor='w', edgecolor='k')\n",
        "plt.plot()\n",
        "plt.plot(train_acc)\n",
        "plt.plot(val_acc)\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABLIAAAJyCAYAAADQJOl3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmc2KoxrQveg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bedb7e3f-818b-46d5-af0f-9d7bf17525d3"
      },
      "source": [
        "y_predict = np.round(model.predict(X_blind_norm))\n",
        "y_predict = [i[0] for i in y_predict.tolist()]\n",
        "sum(y_predict == y_blind)/len(y_blind)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7050359712230215"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEf2sylQnSgJ",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning for imbalanced problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s8Ei8P5nOiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "model_ibp = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(16, activation='relu', input_shape=(18,)),\n",
        "  tf.keras.layers.Dense(8, activation='relu'),\n",
        "  tf.keras.layers.Dense(4, activation='relu'),\n",
        "  tf.keras.layers.Dense(2, activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')    \n",
        "])\n",
        "model_ibp.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zVmN2dupNv_",
        "colab_type": "code",
        "outputId": "a2ccd404-71d7-4f3c-c29a-a9fb7d6674a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "model_ibp.fit(X_train_norm, y_train, epochs=4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "400/400 [==============================] - 0s 354us/sample - loss: 0.6932 - acc: 0.4625\n",
            "Epoch 2/4\n",
            "400/400 [==============================] - 0s 48us/sample - loss: 0.6923 - acc: 0.5075\n",
            "Epoch 3/4\n",
            "400/400 [==============================] - 0s 42us/sample - loss: 0.6914 - acc: 0.5000\n",
            "Epoch 4/4\n",
            "400/400 [==============================] - 0s 41us/sample - loss: 0.6873 - acc: 0.5475\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5e8cd08ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyi6auIKqIg_",
        "colab_type": "code",
        "outputId": "d8706876-552e-46c5-aa0b-455d9c27a8f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "model_ibp.fit(X_train_norm2, y_train2, epochs=4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "400/400 [==============================] - 0s 61us/sample - loss: 0.6802 - acc: 0.5625\n",
            "Epoch 2/4\n",
            "400/400 [==============================] - 0s 42us/sample - loss: 0.6698 - acc: 0.6150\n",
            "Epoch 3/4\n",
            "400/400 [==============================] - 0s 50us/sample - loss: 0.6608 - acc: 0.6225\n",
            "Epoch 4/4\n",
            "400/400 [==============================] - 0s 49us/sample - loss: 0.6514 - acc: 0.6500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5e8d1acba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBD2y8kOqM2e",
        "colab_type": "code",
        "outputId": "36808a9b-6ee6-4eec-a790-3fe9c2e9fd70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "model_ibp.fit(X_train_norm3, y_train3, epochs=4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "400/400 [==============================] - 0s 54us/sample - loss: 0.6376 - acc: 0.6825\n",
            "Epoch 2/4\n",
            "400/400 [==============================] - 0s 48us/sample - loss: 0.6269 - acc: 0.6825\n",
            "Epoch 3/4\n",
            "400/400 [==============================] - 0s 43us/sample - loss: 0.6171 - acc: 0.7250\n",
            "Epoch 4/4\n",
            "400/400 [==============================] - 0s 50us/sample - loss: 0.6090 - acc: 0.7300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5e8c821518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5caUhBUqT5q",
        "colab_type": "code",
        "outputId": "36d879e3-45e7-425a-c8d1-6d30fb888707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "model_ibp.fit(X_train_norm4, y_train4, epochs=4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "400/400 [==============================] - 0s 73us/sample - loss: 0.6120 - acc: 0.7325\n",
            "Epoch 2/4\n",
            "400/400 [==============================] - 0s 48us/sample - loss: 0.6069 - acc: 0.7525\n",
            "Epoch 3/4\n",
            "400/400 [==============================] - 0s 47us/sample - loss: 0.6024 - acc: 0.7375\n",
            "Epoch 4/4\n",
            "400/400 [==============================] - 0s 45us/sample - loss: 0.5987 - acc: 0.7350\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5e8c846b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4NCbG82qXdj",
        "colab_type": "code",
        "outputId": "d6963ebf-e8c1-4fc9-def9-cc973752fac4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "model_ibp.fit(X_train_norm5, y_train5, epochs=4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "400/400 [==============================] - 0s 55us/sample - loss: 0.6070 - acc: 0.7150\n",
            "Epoch 2/4\n",
            "400/400 [==============================] - 0s 52us/sample - loss: 0.6035 - acc: 0.7500\n",
            "Epoch 3/4\n",
            "400/400 [==============================] - 0s 45us/sample - loss: 0.5960 - acc: 0.7525\n",
            "Epoch 4/4\n",
            "400/400 [==============================] - 0s 52us/sample - loss: 0.5922 - acc: 0.7475\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5e8d20bd68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvNtRPOETQ8P",
        "colab_type": "code",
        "outputId": "fd9c4478-7816-4cfd-c659-3e89a7b05142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "model_ibp.fit(X_train_norm6, y_train6, epochs=4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "400/400 [==============================] - 0s 61us/sample - loss: 0.5765 - acc: 0.7525\n",
            "Epoch 2/4\n",
            "400/400 [==============================] - 0s 48us/sample - loss: 0.5708 - acc: 0.7550\n",
            "Epoch 3/4\n",
            "400/400 [==============================] - 0s 48us/sample - loss: 0.5670 - acc: 0.7600\n",
            "Epoch 4/4\n",
            "400/400 [==============================] - 0s 50us/sample - loss: 0.5599 - acc: 0.7700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5e8cd089b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yas7ScxjqcFa",
        "colab_type": "code",
        "outputId": "6e2fc982-0761-46c1-f576-63f62491ccb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_predict = np.round(model_ibp.predict(X_test_norm))\n",
        "y_predict = [i[0] for i in y_predict.tolist()]\n",
        "sum(y_predict == y_test)/len(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6527777777777778"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkhW04wi8JQW",
        "colab_type": "text"
      },
      "source": [
        "# Save Data to SVM format for Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o8fycN68JQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from sklearn.datasets import dump_svmlight_file\n",
        "#dump_svmlight_file(X_train_norm, y_train, 'training.svm',zero_based=False)\n",
        "#dump_svmlight_file(X_test_norm,y_test,'test.svm',zero_based=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evjavp_N8JQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}